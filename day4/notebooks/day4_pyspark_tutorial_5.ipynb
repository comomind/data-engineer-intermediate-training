{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4일차 5교시 집계 연산\n",
    "\n",
    "### 목차\n",
    "* 1. 집계 함수 예제\n",
    "* 2. Group By 예제\n",
    "* 3. SparkContext vs. SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 구매 이력 데이터를 사용해 파티션을 휠씬 적은 수로 분할할 수 있도록 리파티셔닝\n",
    "+ 빠르게 접근할 수 있도록 캐싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data Engineer Intermediate Day4\") \\\n",
    "    .config(\"spark.dataengineer.intermediate.day4\", \"tutorial-5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 구매 이력 데이터 \"\"\"\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"data/retail-data/all/*.csv\") \\\n",
    "    .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 집계 함수\n",
    "+ org.apache.spark.sql.functions 패키지 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "df.select(count(\"StockCode\")).show() # 명시적으로 컬럼을 지정한 경우 해당 컬럼이 널 인 경우 해당 로우는 제외\n",
    "df.select(count(\"*\")).show()         # 명시적인 컬럼 지정이 아니므로 전체 로우를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "1454\n",
      "+------------------+\n",
      "|count(Description)|\n",
      "+------------------+\n",
      "|            540455|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print(df.where(\"Description is null\").count()) # 1,454\n",
    "df.select(count(\"Description\")).show() # 540,455 + 1,454 = 541,909"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 countDistinct\n",
    "+ 고유레코드 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 approx_count_distinct\n",
    "+ 근사치로 구하지만 연산 속도가 빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            4079|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show() # 0.1은 최대 추정 오류율\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.01)).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 first와 last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show(1) # null도 감안하려면 True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 min과 max\n",
    "+ 문자열도 동작이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n",
      "+--------------------+-----------------+\n",
      "|    min(Description)| max(Description)|\n",
      "+--------------------+-----------------+\n",
      "| 4 PURPLE FLOCK D...|wrongly sold sets|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show(1)\n",
    "df.select(min(\"Description\"), max(\"Description\")).show(1) # 문자열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show(1) # 고유값을 합산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 avg\n",
    "+ avg, mean 함수로 평균을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+-----------------+\n",
      "|(total_purchases / total_transcations)|   avg_purchases|mean_transcations|\n",
      "+--------------------------------------+----------------+-----------------+\n",
      "|                      9.55224954743324|9.55224954743324| 9.55224954743324|\n",
      "+--------------------------------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transcations\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_transcations\"),    \n",
    ").selectExpr(\n",
    "    \"total_purchases / total_transcations\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_transcations\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 분산과 표준편차\n",
    "+ 표본표준분산 및 편차: variance, stddev\n",
    "+ 모표준분산 및 편차 : var_pop, stddev_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)|var_samp(Quantity)|stddev_samp(Quantity)| var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "|47559.391409298696|   218.08115785023404|47559.391409298696|   218.08115785023404|47559.303646609005|  218.08095663447784|\n",
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance, stddev\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "\n",
    "df.select(variance(\"Quantity\"), stddev(\"Quantity\"),      \n",
    "          var_samp(\"Quantity\"), stddev_samp(\"Quantity\"), # 위와 동일\n",
    "          var_pop(\"Quantity\"), stddev_pop(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+------------------+---------------------+-----------------+--------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)|var_samp(Quantity)|stddev_samp(Quantity)|var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+---------------------+------------------+---------------------+-----------------+--------------------+\n",
      "|               NaN|                  NaN|               NaN|                  NaN|              0.0|                 0.0|\n",
      "+------------------+---------------------+------------------+---------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df.select(\"*\").take(1)).select(variance(\"Quantity\"), stddev(\"Quantity\"),      \n",
    "          var_samp(\"Quantity\"), stddev_samp(\"Quantity\"), # 위와 동일\n",
    "          var_pop(\"Quantity\"), stddev_pop(\"Quantity\")).show() # 1일 때는 NaN이 나옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 비대칭도와 첨도\n",
    "+ 비대칭도와 첨도 : https://www.youtube.com/watch?time_continue=2&v=g9VOhfy2WWY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052783|119768.05495533274|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![왜도](image/func1.png)\n",
    "\n",
    "#### 비대칭도, 왜도 (skewness)\n",
    "> 왜도는 데이터가 대칭이 아닌 정도입니다. 왜도 값(0, 양수 또는 음수)이 데이터 형상에 대한 정보를 나타냅니다.\n",
    "데이터가 대칭에 가까울수록 왜도 값이 0에 근접합니다. 그러나 왜도 부족만으로 정규성을 의미하지는 않습니다.\n",
    "\n",
    "#### 첨도(kurtosis)\n",
    "> 첨도는 분포의 꼬리가 정규 분포와 어떻게 다른지 나타냅니다. 완전히 정규 분포를 따르는 데이터의 첨도 값은 0입니다.\n",
    "분포의 첨도 값이 양수이면 분포의 꼬리가 정규 분포보다 두껍다는 것을 나타냅니다\n",
    "분포의 첨도 값이 음수이면 분포의 꼬리가 정규 분포보다 얇다는 것을 나타냅니다. \n",
    "\n",
    "#### [skewness](https://github.com/apache/spark/blob/5a7403623d0525c23ab8ae575e9d1383e3e10635/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/CentralMomentAgg.scala#L231)\n",
    "```scala\n",
    "org.apache.spark.sql.catalyst.expressions.aggregate.CentralMomentAgg\n",
    "\n",
    "def skewness(columnName: String): Column = skewness(Column(columnName))\n",
    "def skewness(e: Column): Column = withAggregateFunction { Skewness(e.expr) }\n",
    "def kurtosis(e: Column): Column = withAggregateFunction { Kurtosis(e.expr) }\n",
    "def kurtosis(columnName: String): Column = kurtosis(Column(columnName))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.11 공분산과 상관관계\n",
    "+ 표본공분산(cover_samp), 모공분산(cover_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|     4.912186085648426E-4|            1052.7260778770628|              1052.728054393167|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![공분산](image/func2.gif)\n",
    "          \n",
    "#### [공분산](https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%B6%84%EC%82%B0) (covariance)\n",
    "> 공분산(共分散, 영어: covariance)은 2개의 확률변수의 상관정도를 나타내는 값이다.(1개의 변수의 이산정도를 나타내는 분산과는 별개임) 만약 2개의 변수중 하나의 값이 상승하는 경향을 보일 때, 다른 값도 상승하는 경향의 상관관계에 있다면, 공분산의 값은 양수가 될 것이다. 반대로 2개의 변수중 하나의 값이 상승하는 경향을 보일 때, 다른 값이 하강하는 경향을 보인다면 공분산의 값은 음수가 된다. <br>\n",
    "<br>\n",
    "![function](image/func3.png)\n",
    "\n",
    "<br>\n",
    "단, 100점만점인 두과목의 점수 공분산은 별로 상관성이 부족하지만 100점만점이기 때문에 큰 값이 나오고\n",
    "10점짜리 두과목의 점수 공분산은 상관성이 아주 높을지만 10점만점이기 때문에 작은값이 나온다\n",
    "\n",
    "![function](image/func4.png)\n",
    "\n",
    "#### [상관관계](https://ko.wikipedia.org/wiki/%EC%83%81%EA%B4%80_%EB%B6%84%EC%84%9D) (correlation)\n",
    "> 상관 분석(Correlation analysis)은 확률론과 통계학에서 두 변수간에 어떤 선형적 관계를 갖고 있는 지를 분석하는 방법이다. \n",
    "![function](image/func5.png)\n",
    "\n",
    "#### 피어슨 상관 계수\n",
    "> 피어슨 상관 계수란 두 변수 X 와 Y 간의 선형 상관 관계를 계량화한 수치다 . 피어슨 상관 계수는 코시-슈바르츠 부등식에 의해 +1과 -1 사이의 값을 가지며, +1은 완벽한 양의 선형 상관 관계, 0은 선형 상관 관계 없음, -1은 완벽한 음의 선형 상관 관계를 의미한다.\n",
    "\n",
    "#### Perason's r = X와 Y가 함께 변하는 정도 / X와 Y가 각각 변하는 정도\n",
    "##### r 값은 X 와 Y 가 완전히 동일하면 +1, 전혀 다르면 0, 반대방향으로 완전히 동일 하면 –1 을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.12 복합 데이터 타입의 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|collect_list(Country)|collect_set(Country)|\n",
      "+---------------------+--------------------+\n",
      "| [United Kingdom, ...|[Portugal, Italy,...|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set, size\n",
    "\n",
    "df.select(collect_list(\"Country\"), collect_set(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------------+\n",
      "|size(collect_list(Country))|size(collect_set(Country))|\n",
      "+---------------------------+--------------------------+\n",
      "|                     541909|                        38|\n",
      "+---------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(size(collect_list(\"Country\")), size(collect_set(\"Country\"))).show() # 각 컬럼의 복합데이터 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Country)|\n",
      "+-----------------------+\n",
      "|                     38|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"Country\")).show() # 중복없이 카운트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 복합 데이터 타입의 집계 함수(collect_set, collect_list)의 실용 사례?\n",
    "> 데이터의 Cardinality 가 충분히 많지 않은 경우에 하나의 컬럼에 담아 처리하고 싶을 때 활용할 수 있습니다.\n",
    "\n",
    "#### [collect_set](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala#L125)\n",
    "```scala\n",
    "org.apache.spark.sql.catalyst.expressions.aggregate.collect.scala\n",
    "case class CollectSet(\n",
    "    child: Expression,\n",
    "    mutableAggBufferOffset: Int = 0,\n",
    "    inputAggBufferOffset: Int = 0) extends Collect[mutable.HashSet[Any]] {\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Group By\n",
    "+ 하나 이상의 컬럼을 그룹화하여 RelationalGroupedDataset 반환\n",
    "+ 집계 연산을 수행하는 두 번째 단계에서는 DataFrame이 반환됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 표현식을 이용한 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----+---------------+\n",
      "|InvoiceNo|CustomerId|guan|count(Quantity)|\n",
      "+---------+----------+----+---------------+\n",
      "|   536846|     14573|  76|             76|\n",
      "|   537026|     12395|  12|             12|\n",
      "|   537883|     14437|   5|              5|\n",
      "|   538068|     17978|  12|             12|\n",
      "|   538279|     14952|   7|              7|\n",
      "+---------+----------+----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df \\\n",
    "    .groupBy(\"InvoiceNo\", \"CustomerId\") \\\n",
    "    .agg(\n",
    "        count(\"Quantity\").alias(\"guan\"),\n",
    "        expr(\"count(Quantity)\")\n",
    "    ) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 맵을 이용한 그룹화\n",
    "+ 파이선의 딕셔너리 데이터 타입을 활용하여 집계함수의 표현이 가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 맵을 이용한 그룹화 (agg(key->value))\n",
    "> 맵을 이용하여 컬럼 단위로 적용할 함수를 전달하는 방식입니다\n",
    "\n",
    "#### [RelationalGroupedDataset.agg](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L168)\n",
    "```scala\n",
    "org.apache.spark.sql.RelationalGroupedDataset\n",
    "\n",
    "def agg(exprs: Map[String, String]): DataFrame = {\n",
    "    toDF(exprs.map { case (colName, expr) =>\n",
    "        strToExpr(expr)(df(colName).expr)\n",
    "    }.toSeq)\n",
    "}\n",
    "```\n",
    "##### 1. 위의 함수 호출 시에 value (key) 형식으로 expression 을 만들어주게 됩니다.\n",
    "##### 2. map 이 mutable.HashMap  을 넘기면  Compile Error 가 발생함에 유의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|InvoiceNo|stddev_pop(Quantity)|\n",
      "+---------+--------------------+\n",
      "|   536596|  1.1180339887498947|\n",
      "|   536938|  20.698023172885524|\n",
      "|   537252|                 0.0|\n",
      "|   537691|   5.597097462078001|\n",
      "|   538041|                 0.0|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 런타임 시에 맵으로 전달된 함수를 표현식으로 사용할 수 있습니다.\n",
    "df.groupBy(\"InvoiceNo\") \\\n",
    "    .agg(\n",
    "        {\"Quantity\":\"avg\", \"Quantity\":\"stddev_pop\"}\n",
    "    ) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 요약\n",
    "+ User Defined Aggregation Function, UDAF\n",
    "+ UDAF를 생성하려면 기본 클래스인 UserDefinedAggregateFunction을 상속\n",
    "+ UDAF는 현재 스칼라와 자바로만 사용할 수 있음(ver 2.3)\n",
    "```\n",
    "inputSchema: UDAF 입력 파라미터의 스키마를 StructType으로 정의 \n",
    "bufferSchema: UDAF 중간 결과의 스키마를 StructType으로 정의\n",
    "dataType: 반환될 값의 DataType을 정의\n",
    "deterministic: UDAF가 동일한 입력값에 대해 항상 동일한 결과를 반환하는지 불리언값으로 정의\n",
    "initialize: 집계용 버퍼의 값을 초기화하는 로직을 정의\n",
    "update: 입력받은 로우를 기바느로 내부 버퍼를 업데이트하는 로직을 정의\n",
    "merge: 두 개의 집계용 버퍼를 병합하는 로직을 정의\n",
    "evaluate: 집계의 최종 결과를 생성하는 로직을 정의\n",
    "```\n",
    "\n",
    "※ Efficient UD(A)Fs with PySpark https://www.inovex.de/blog/efficient-udafs-with-pyspark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext vs. SparkSession (1/2)\n",
    "> SparkContext 는 spark.core 프로젝트이고, SparkSession 은 spark.sql 프로젝트이다\n",
    "아래와 같이 SparkContext 는 Spark 실행에 가장 중심이 되는 객체이고, 병렬화, 브로드캐스팅, 분산 파일 추가 및 종료 등의 스파크 작업을 관장하는 클래스라 볼 수 있습니다\n",
    "\n",
    "#### 1. [SparkContext.{parallelize, broadcast, addFile, listFiles, addJar, listJars, stop, getOrCreate}](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala)\n",
    "```scala\n",
    "\n",
    "'SparkContext : \"Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.\"\n",
    "@note Only one `SparkContext` should be active per JVM. You must `stop()` the active `SparkContext` before creating a new one.\n",
    "@param config a Spark Config object describing the application configuration. Any settings in this config overrides the default configs as well as system properties.\n",
    "\n",
    "\n",
    "'parallelize : \"Distribute a local Scala collection to form an RDD.\"\n",
    "@note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call to parallelize and before the first action on the RDD, the resultant RDD will reflect the modified collection. Pass a copy of the argument to avoid this.\n",
    "@note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions.\n",
    "@param seq Scala collection to distribute\n",
    "@param numSlices number of partitions to divide the collection into\n",
    "def parallelize[T: ClassTag](seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope {\n",
    "  assertNotStopped()\n",
    "  new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())\n",
    "}\n",
    "\n",
    "'broadcast : \"Broadcast a read-only variable to the cluster, returning a [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions. The variable will be sent to each cluster only once.\"\n",
    "@param value value to broadcast to the Spark nodes\n",
    "@return `Broadcast` object, a read-only variable cached on each machine\n",
    "def broadcast[T: ClassTag](value: T): Broadcast[T] = {\n",
    "  assertNotStopped()\n",
    "  require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass), \"Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\")\n",
    "  val bc = env.broadcastManager.newBroadcast[T](value, isLocal)\n",
    "  val callSite = getCallSite\n",
    "  logInfo(\"Created broadcast \" + bc.id + \" from \" + callSite.shortForm)\n",
    "  cleaner.foreach(_.registerBroadcastForCleanup(bc))\n",
    "  bc\n",
    "}\n",
    "\n",
    "'addFile : \" Add a file to be downloaded with this Spark job on every node. If a file is added during execution, it will not be available until the next TaskSet starts.\"\n",
    "@param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, use `SparkFiles.get(fileName)` to find its download location. \n",
    "@note A path can be added only once. Subsequent additions of the same path are ignored.\n",
    "def addFile(path: String): Unit = {\n",
    "  addFile(path, false)\n",
    "}\n",
    "\n",
    "'listFiles : \"Returns a list of file paths that are added to resources.\"\n",
    "def listFiles(): Seq[String] = addedFiles.keySet.toSeq\n",
    "\n",
    "'addJar : \"Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future.  If a jar is added during execution, it will not be available until the next TaskSet starts.\"\n",
    "@param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node.\n",
    "@note A path can be added only once. Subsequent additions of the same path are ignored.\n",
    "def addJar(path: String) { ... }\n",
    "\n",
    "'listJars : \"Returns a list of jar files that are added to resources.\"\n",
    "def listJars(): Seq[String] = addedJars.keySet.toSeq\n",
    "\n",
    "\n",
    "'stop : \"Shut down the SparkContext.\"\n",
    "def stop(): Unit = { ... }\n",
    "\n",
    "'getOrCreate(config) : \"This function may be used to get or instantiate a SparkContext and register it as a singleton object. Because we can only have one active SparkContext per JVM, this is useful when applications may wish to share a SparkContext.\"\n",
    "@param config `SparkConfig` that will be used for initialisation of the `SparkContext`\n",
    "@return current `SparkContext` (or a new one if it wasn't created before the function call)\n",
    "def getOrCreate(config: SparkConf): SparkContext = {\n",
    "  // Synchronize to ensure that multiple create requests don't trigger an exception\n",
    "  // from assertNoOtherContextIsRunning within setActiveContext\n",
    "  SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {\n",
    "    if (activeContext.get() == null) {\n",
    "      setActiveContext(new SparkContext(config))\n",
    "    } else {\n",
    "      if (config.getAll.nonEmpty) { logWarning(\"Using an existing SparkContext; some configuration may not take effect.\") }\n",
    "    }\n",
    "    activeContext.get()\n",
    "  }\n",
    "}\n",
    "\n",
    "'getOrCreate() : \"This function may be used to get or instantiate a SparkContext and register it as a singleton object. Because we can only have one active SparkContext per JVM, this is useful when applications may wish to share a SparkContext. This method allows not passing a SparkConf (useful if just retrieving).\"\n",
    "@return current `SparkContext` (or a new one if wasn't created before the function call)\n",
    "def getOrCreate(): SparkContext = {\n",
    "  SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {\n",
    "    if (activeContext.get() == null) { setActiveContext(new SparkContext()) }\n",
    "    activeContext.get()\n",
    "  }\n",
    "}\n",
    "```\n",
    "> Only one `SparkContext` should be active per JVM. You must `stop()` the active `SparkContext` before creating a new one. <br>\n",
    "a Spark Config object describing the application configuration. Any settings in this config overrides the default configs as well as system properties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext vs. SparkSession (2/2)\n",
    "> SparkSession 경우 sparkContext 와 관련을 가지는 객체와 데이터프레임을 다루는 함수들로 구성되어 있습니다.\n",
    "\n",
    "#### 2. [SparkSession](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala)\n",
    "\n",
    "#### 2-1 SparkSession.{version, sqlContext, udf}\n",
    "#### 2-2 SparkSession.{newSession, emptyDataFrame, emptyDataset, createDataFrame}\n",
    "#### 2-3 SparkSession.{range, table, sql, read, time, stop, close}\n",
    "#### 2-4 SparkSession.builder.{master, appName, config, getOrCreate}\n",
    "\n",
    "```scala\n",
    "'SparkSession : \"The entry point to programming Spark with the Dataset and DataFrame API.  In environments that this has been created upfront (e.g. REPL, notebooks), use the builder to get an existing session:\"\n",
    "SparkSession.builder().getOrCreate()\n",
    "\"The builder can also be used to create a new session:\"\n",
    "SparkSession.builder\n",
    "  .master(\"local\")\n",
    "  .appName(\"Word Count\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "@param sparkContext The Spark context associated with this Spark session.\n",
    "@param existingSharedState If supplied, use the existing shared state instead of creating a new one.\n",
    "@param parentSessionState If supplied, inherit all session state (i.e. temporary views, SQL config, UDFs etc) from parent.\n",
    "\n",
    "\n",
    "'version : \"The version of Spark on which this application is running.\"\n",
    "def version: String = SPARK_VERSION\n",
    "\n",
    "'sqlContext : \"A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility.\"\n",
    "val sqlContext: SQLContext = new SQLContext(this)\n",
    "\n",
    "'udf : \"A collection of methods for registering user-defined functions (UDF).  The following example registers a Scala closure as UDF:\"\n",
    "sparkSession.udf.register(\"myUDF\", (arg1: Int, arg2: String) => arg2 + arg1)\n",
    "\"The following example registers a UDF in Java:\"\n",
    "sparkSession.udf().register(\"myUDF\",\n",
    "   (Integer arg1, String arg2) -> arg2 + arg1,\n",
    "   DataTypes.StringType);\n",
    "@note The user-defined functions must be deterministic. Due to optimization, duplicate invocations may be eliminated or the function may even be invoked more times than it is present in the query.\n",
    "def udf: UDFRegistration = sessionState.udfRegistration\n",
    "\n",
    "'newSession : \"Start a new session with isolated SQL configurations, temporary tables, registered functions are isolated, but sharing the underlying `SparkContext` and cached data.\"\n",
    "@note Other than the `SparkContext`, all shared state is initialized lazily. This method will force the initialization of the shared state to ensure that parent and child sessions are set up with the same shared state. If the underlying catalog implementation is Hive, this will initialize the metastore, which may take some time.\n",
    "def newSession(): SparkSession = {\n",
    "  new SparkSession(sparkContext, Some(sharedState), parentSessionState = None, extensions)\n",
    "}\n",
    "\n",
    "'emptyDataFrame : \"Returns a `DataFrame` with no rows or columns.\"\n",
    "lazy val emptyDataFrame: DataFrame = {\n",
    "  createDataFrame(sparkContext.emptyRDD[Row].setName(\"empty\"), StructType(Nil))\n",
    "}\n",
    "\n",
    "'emptyDataset : \"Creates a new [[Dataset]] of type T containing zero elements.\"\n",
    "def emptyDataset[T: Encoder]: Dataset[T] = {\n",
    "  val encoder = implicitly[Encoder[T]]\n",
    "  new Dataset(self, LocalRelation(encoder.schema.toAttributes), encoder)\n",
    "}\n",
    "\n",
    "'createDataFrame : \"Creates a `DataFrame` from an RDD of Product (e.g. case classes, tuples).\"\n",
    "def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = {\n",
    "  SparkSession.setActiveSession(this)\n",
    "  val encoder = Encoders.product[A]\n",
    "  Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder))\n",
    "}\n",
    "\n",
    "'range(end) : \"Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements in a range from 0 to `end` (exclusive) with step value 1.\"\n",
    "def range(end: Long): Dataset[java.lang.Long] = range(0, end)\n",
    "\n",
    "'range(start, end) : \"Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements in a range from `start` to `end` (exclusive) with step value 1.\"\n",
    "def range(start: Long, end: Long): Dataset[java.lang.Long] = {\n",
    "  range(start, end, step = 1, numPartitions = sparkContext.defaultParallelism)\n",
    "}\n",
    "\n",
    "'table(tableName) : \"Returns the specified table/view as a `DataFrame`.\"\n",
    "@param tableName is either a qualified or unqualified name that designates a table or view. If a database is specified, it identifies the table/view from the database. Otherwise, it first attempts to find a temporary view with the given name and then match the table/view from the current database. Note that, the global temporary view database is also valid here.\n",
    "def table(tableName: String): DataFrame = {\n",
    "  table(sessionState.sqlParser.parseMultipartIdentifier(tableName))\n",
    "}\n",
    "\n",
    "'sql : \"Executes a SQL query using Spark, returning the result as a `DataFrame`. The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'.\"\n",
    "def sql(sqlText: String): DataFrame = {\n",
    "  val tracker = new QueryPlanningTracker\n",
    "  val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) {\n",
    "    sessionState.sqlParser.parsePlan(sqlText)\n",
    "  }\n",
    "  Dataset.ofRows(self, plan, tracker)\n",
    "}\n",
    "\n",
    "'read : \"Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a `DataFrame`.\"\n",
    "sparkSession.read.parquet(\"/path/to/file.parquet\")\n",
    "sparkSession.read.schema(schema).json(\"/path/to/file.json\")\n",
    "def read: DataFrameReader = new DataFrameReader(self)\n",
    "\n",
    "'time : \"Executes some code block and prints to stdout the time taken to execute the block. This is available in Scala only and is used primarily for interactive testing and debugging.\"\n",
    "def time[T](f: => T): T = {\n",
    "  val start = System.nanoTime()\n",
    "  val ret = f\n",
    "  val end = System.nanoTime()\n",
    "  // scalastyle:off println\n",
    "  println(s\"Time taken: {NANOSECONDS.toMillis(end - start)} ms\")\n",
    "  // scalastyle:on println\n",
    "  ret\n",
    "}\n",
    "\n",
    "'stop : \"Stop the underlying `SparkContext`.\"\n",
    "def stop(): Unit = { sparkContext.stop() }\n",
    "\n",
    "'close : \"Synonym for `stop()`.\"\n",
    "override def close(): Unit = stop()\n",
    "\n",
    "'SparkSession : \"\"\n",
    "\n",
    "object SparkSession extends Logging {\n",
    "    class Builder extends Logging {\n",
    "        'appName : \" Sets a name for the application, which will be shown in the Spark web UI.\"\n",
    "                \"If no application name is set, a randomly generated name will be used.\"\n",
    "        def appName(name: String): Builder = config(\"spark.app.name\", name)\n",
    "\n",
    "        'getOrCreate : \"Gets an existing [[SparkSession]] or, if there is no existing one, creates a new\n",
    "                one based on the options set in this builder.\n",
    "                This method first checks whether there is a valid thread-local SparkSession,\n",
    "                and if yes, return that one. It then checks whether there is a valid global\n",
    "                default SparkSession, and if yes, return that one. If no valid global default\n",
    "                SparkSession exists, the method creates a new SparkSession and assigns the\n",
    "                newly created SparkSession as the global default.\n",
    "                In case an existing SparkSession is returned, the config options specified in\n",
    "                this builder will be applied to the existing SparkSession.\"\n",
    "        def getOrCreate(): SparkSession = synchronized {\n",
    "            assertOnDriver()\n",
    "        'builder : \"Creates a [[SparkSession.Builder]] for constructing a [[SparkSession]].\"\n",
    "        def builder(): Builder = new Builder\n",
    "    }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
