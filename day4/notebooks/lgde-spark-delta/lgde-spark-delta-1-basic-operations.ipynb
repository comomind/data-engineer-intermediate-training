{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 1. Basic Operations on Delta Lakes\n",
    "\n",
    "* 목차\n",
    "  - [1. What is Delta Lake?](#1.-What-is-Delta-Lake?)\n",
    "  - [2. How to start using Delta Lake](#2.-How-to-start-using-Delta-Lake)\n",
    "  - [3. Basic operations](#3.-Basic-operations)\n",
    "  - [4. Unpacking the Transaction Log](#4.-Unpacking-the-Transaction-Log)\n",
    "  - [5. Table Utilities](#5.-Table-Utilities)\n",
    "  - [6. Summary](#6.-Summary)\n",
    "\n",
    "## Delta Lake quickstart\n",
    "\n",
    "> 델타 레이크는 Apache Spark 에 ACID 트랜젝션을 지원해줄 수 있는 오픈소스 스토리지 레이어입니다.\n",
    "\n",
    "\n",
    "```Dockerfile\n",
    "# Install Delta Lake\n",
    "RUN pip install jip\n",
    "RUN jip install \"io.delta:delta-core_2.12:0.7.0\"\n",
    "```\n",
    "\n",
    "* 델타 테이블\n",
    "  - [Delta Table Documentation](https://docs.delta.io/latest/api/python/index.html)\n",
    "  - [spark.databricks.delta Configuration](https://books.japila.pl/delta-lake-internals/DeltaSQLConf/)\n",
    "\n",
    "* 레퍼런스\n",
    "  - [Delta IO on Github](https://github.com/delta-io/delta)\n",
    "  - [Delta on Maven](https://mvnrepository.com/artifact/io.delta/delta-core)\n",
    "  - [Docker Stack Recipe](https://jupyter-docker-stacks.readthedocs.io/it/latest/using/recipes.html)\n",
    "  - [Data Time Travel by Delta Time Machine](https://databricks.com/session_eu20/data-time-travel-by-delta-time-machine-2) - how each concurrent writes are handled\n",
    "  - [Data Engineer Training](https://github.com/psyoblade/data-engineer-training)\n",
    "  - [SparkSQL Magic](https://github.com/cryeo/sparksql-magic)\n",
    "  - [Introducing Delta Time Travel for Large Scale Data Lakes](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)\n",
    "  - [Query an older snapshot of a table (time travel)](https://docs.databricks.com/delta/delta-batch.html?_ga=2.97316378.466340956.1612291545-1203283123.1598416965#query-an-older-snapshot-of-a-table-time-travel)\n",
    "  - [Unpacking the Transaction Log I](https://databricks.com/discover/diving-into-delta-lake-talks/unpacking-transaction-log)\n",
    "  - [Unpacking the Transaction Log I](https://databricks.com/session_eu20/diving-into-delta-lake-unpacking-the-transaction-log)\n",
    "  - [Table Batch Read & Write](https://docs.databricks.com/delta/delta-batch.html)\n",
    "\n",
    "* 플러그인 및 도구\n",
    "  - [Spark SQL on Notebook](https://github.com/jupyter-incubator/sparkmagic)\n",
    "  - [IPython SQL on Notebook](https://github.com/catherinedevlin/ipython-sql)\n",
    "\n",
    "\n",
    "* 질문 사항\n",
    "  - 왜 설치된 상태로 수행할 수 없고, 매번 delta-core 를 다운로드 받아야 하는가?\n",
    "  - 온 프레미스 환경에서 delta lake 사용을 위한 설치환경은 어떻게 구성해야 하는가? (ex_ JupyterHub + Hadoop3 RBF + normal YARN)\n",
    "  - addpyfile 통해서 추가했을 때에 import 는 되었지만, 실제 method 호출 시에 실패했는데 어떤 이유인가?\n",
    "  - 결국 체크포인트까지 만들어내면 (트랜젝션 로그의 과거 파일을 삭제하지 않으면) 스몰파일 문제는 계속 심화되는 것 아닌가? 언제 지우는가?\n",
    "  - 도대체 프로토콜이 언제 왜 사용되는가? 트랜잭션 수행의 순서를 결정짓는 serializability 를 말합니다.\n",
    "  - 분산환경에서 저장된 델타테이블은 하이브와 같은 메타스토어 그리고 중앙집중식 락 관리도 안 하는 것 처럼 보이는데 왜 가능한가? 그 역할은 누가 어떻게 하는가?\n",
    "  - 노트북 환경에서 vacuum 실행 시에 상당히 오랜 시간이 걸리는데 왜 그런가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "# 노트북에서 델타 레이크를 사용하기 위해 설정을 합니다\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://78ef8252e5ca:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff1cc22daf0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Delta Lake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[맨 위로](#CHAPTER-1.-Basic-Operations-on-Delta-Lakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to start using Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Using Delta Lake via local Spark shells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Leveraging GitHub or Maven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Using Databricks Community Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/lgde-spark-delta\n"
     ]
    }
   ],
   "source": [
    "# 임시 경로를 삭제합니다\n",
    "!pwd\n",
    "!rm -rf /home/jovyan/work/tmp\n",
    "!rm -rf /home/jovyan/work/spark-warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[맨 위로](#CHAPTER-1.-Basic-Operations-on-Delta-Lakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic operations\n",
    "\n",
    "> 델타 테이블은 Spark, Hive, Presto (or Trino), Ballista (Rust) and AWS Athena, Azure Synapse, BigQuery, and Dremio 등 다양한 오픈소스 및 상용 솔루션에서 사용되어질 수 있도록 설계 되었으며, SQL, Scala 및 Python 등을 통해 수행될 수 있는 APIs 가 존재합니다.\n",
    "\n",
    "### 3-1. Creating your first Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![write delta](images/figure.1-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing your Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`author` string, `title` string, `pages` int\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- pages: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>author</th><th>title</th><th>pages</th></tr>\n",
       "<tr><td>정휘센</td><td>안녕하세요 정휘센 입니다</td><td>300</td></tr>\n",
       "<tr><td>김싸이언</td><td>안녕하세요 김싸이언 입니다</td><td>200</td></tr>\n",
       "<tr><td>유코드제로</td><td>안녕하세요 유코드제로 입니다</td><td>100</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------------------------+-----+\n",
       "|    author|                       title|pages|\n",
       "+----------+----------------------------+-----+\n",
       "|    정휘센|    안녕하세요 정휘센 입니다|  300|\n",
       "|  김싸이언|  안녕하세요 김싸이언 입니다|  200|\n",
       "|유코드제로|안녕하세요 유코드제로 입니다|  100|\n",
       "+----------+----------------------------+-----+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = [\n",
    "    Row(\"정휘센\", \"안녕하세요 정휘센 입니다\", 300),\n",
    "    Row(\"김싸이언\", \"안녕하세요 김싸이언 입니다\", 200),\n",
    "    Row(\"유코드제로\", \"안녕하세요 유코드제로 입니다\", 100)\n",
    "]\n",
    "schema = \"`author` string, `title` string, `pages` int\"\n",
    "print(schema)\n",
    "df = spark.createDataFrame(rows, schema)\n",
    "df.printSchema()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"tmp/say-hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended = [\n",
    "    Row(\"박수혁\", \"처음 뵙겠습니다\", 400),\n",
    "    Row(\"김영미\", \"하지메마시떼\", 400)\n",
    "]\n",
    "df2 = spark.createDataFrame(appended, schema)\n",
    "df2.write.format(\"delta\").mode(\"append\").save(\"tmp/say-hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading your Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- pages: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>author</th><th>title</th><th>pages</th></tr>\n",
       "<tr><td>유코드제로</td><td>안녕하세요 유코드제로 입니다</td><td>100</td></tr>\n",
       "<tr><td>김싸이언</td><td>안녕하세요 김싸이언 입니다</td><td>200</td></tr>\n",
       "<tr><td>정휘센</td><td>안녕하세요 정휘센 입니다</td><td>300</td></tr>\n",
       "<tr><td>박수혁</td><td>처음 뵙겠습니다</td><td>400</td></tr>\n",
       "<tr><td>김영미</td><td>하지메마시떼</td><td>400</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------------------------+-----+\n",
       "|    author|                       title|pages|\n",
       "+----------+----------------------------+-----+\n",
       "|유코드제로|안녕하세요 유코드제로 입니다|  100|\n",
       "|  김싸이언|  안녕하세요 김싸이언 입니다|  200|\n",
       "|    정휘센|    안녕하세요 정휘센 입니다|  300|\n",
       "|    박수혁|             처음 뵙겠습니다|  400|\n",
       "|    김영미|                하지메마시떼|  400|\n",
       "+----------+----------------------------+-----+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df3 = spark.read.format(\"delta\").load(\"tmp/say-hello\")\n",
    "df3.printSchema()\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>author</th><th>title</th><th>pages</th></tr>\n",
       "<tr><td>유코드제로</td><td>안녕하세요 유코드제로 입니다</td><td>100</td></tr>\n",
       "<tr><td>김싸이언</td><td>안녕하세요 김싸이언 입니다</td><td>200</td></tr>\n",
       "<tr><td>정휘센</td><td>안녕하세요 정휘센 입니다</td><td>300</td></tr>\n",
       "<tr><td>박수혁</td><td>처음 뵙겠습니다</td><td>400</td></tr>\n",
       "<tr><td>김영미</td><td>하지메마시떼</td><td>400</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------------------------+-----+\n",
       "|    author|                       title|pages|\n",
       "+----------+----------------------------+-----+\n",
       "|유코드제로|안녕하세요 유코드제로 입니다|  100|\n",
       "|  김싸이언|  안녕하세요 김싸이언 입니다|  200|\n",
       "|    정휘센|    안녕하세요 정휘센 입니다|  300|\n",
       "|    박수혁|             처음 뵙겠습니다|  400|\n",
       "|    김영미|                하지메마시떼|  400|\n",
       "+----------+----------------------------+-----+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스파크 SQL 상에서는 상대경로를 지정하면 동작하지 않고, 절대경로를 지정해야만 합니다\n",
    "spark.sql(f\"select * from delta.`{work_dir}/tmp/say-hello`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading your metastore defined Delta table\n",
    "\n",
    "> 메타데이터 접근을 위해서는 `saveAsTable` 혹은 `CREATE TABLE` 구문을 활용해야만 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>hello</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+---------+-----------+\n",
       "|database|tableName|isTemporary|\n",
       "+--------+---------+-----------+\n",
       "| default|    hello|      false|\n",
       "+--------+---------+-----------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.write.format(\"delta\").saveAsTable(\"hello\")\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>hello</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>say</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+---------+-----------+\n",
       "|database|tableName|isTemporary|\n",
       "+--------+---------+-----------+\n",
       "| default|    hello|      false|\n",
       "| default|      say|      false|\n",
       "+--------+---------+-----------+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS say (\n",
    "        author string,\n",
    "        title string,\n",
    "        pages integer\n",
    "    )\n",
    "    USING DELTA\n",
    "    LOCATION \"{work_dir}/tmp/say\"\n",
    "\"\"\")\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 대용량 테이블 저장 시에는 파티션 설계를 고려하면 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>hello</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>say</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>say_hello</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+---------+-----------+\n",
       "|database|tableName|isTemporary|\n",
       "+--------+---------+-----------+\n",
       "| default|    hello|      false|\n",
       "| default|      say|      false|\n",
       "| default|say_hello|      false|\n",
       "+--------+---------+-----------+"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS say_hello (\n",
    "        author string,\n",
    "        title string,\n",
    "        pages integer\n",
    "    )\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (pages)\n",
    "    LOCATION \"{work_dir}/tmp/say_hello\"\n",
    "\"\"\")\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metastore ?\n",
    "> 테이블을 정의하기 위한 정보를 저장해 두는 저장소 (ex_ hive metastore)를 말하며, data location, storage format, table schema and properties 등의 정보가 저장됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[맨 위로](#CHAPTER-1.-Basic-Operations-on-Delta-Lakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unpacking the Transaction Log\n",
    "\n",
    "> 델타 레이크의 가장 큰 혜택이 ACID 트랜잭션을 지원하는 것이며 내부적으로 어떻게 동작하는 지 이해할 필요가 있습니다.\n",
    "\n",
    "* Parquet Table vs. Delta Table\n",
    "  - Delta 테이블의 경우 `_delta_log` 하는 Delta transaction log 경로가 존재하며, 이를 통해 ACID 트랜잭션 뿐만 아니라 스케일러블한 메타데이터 처리 및 타임 트래블링이 가능해 집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_delta_log\n",
      "part-00000-035b3a33-0a65-4edc-ac6f-fdaf165d0d50-c000.snappy.parquet\n",
      "part-00000-f05b7216-70cb-45c2-bd80-b2ea8c63397b-c000.snappy.parquet\n",
      "part-00001-9cd90836-3994-4156-a53d-a37643906aec-c000.snappy.parquet\n",
      "part-00001-e7cce94f-bc22-4cfb-83f6-b41dc2f5a122-c000.snappy.parquet\n",
      "part-00002-0492c0ec-c744-4731-9473-c7cd728a8a51-c000.snappy.parquet\n",
      "part-00002-66a1573a-3a10-4cf6-a643-dd7056209ec2-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls \"tmp/say-hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaSayHello = spark.read.format(\"delta\").load(\"tmp/say-hello\")\n",
    "deltaSayHello.write.mode(\"overwrite\").parquet(\"tmp/say-hello-parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-bb19ac2a-dd71-40c9-b7c7-60e1d90fbb0b-c000.snappy.parquet\n",
      "part-00001-bb19ac2a-dd71-40c9-b7c7-60e1d90fbb0b-c000.snappy.parquet\n",
      "part-00002-bb19ac2a-dd71-40c9-b7c7-60e1d90fbb0b-c000.snappy.parquet\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls \"tmp/say-hello-parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. What Is the Delta Lake Transaction Log?\n",
    "\n",
    "> Delta Lake transaction log (혹은 Delta Log) 는 델타 레이크 테이블에서 발생하는 모든 변경사항을 저장하고 있는 순차적인 레코드 입니다.\n",
    "\n",
    "#### Single Source of Truth\n",
    "\n",
    "> 항상 이용자에게 올바른 뷰를 보여주기 위해, 사용자가 수행하는 모든 변경사항을 트래킹하는 센트럴 레포지토리는 하나의 소스만을 바라보고 있습니다. 이는 소스코드 관리도구 깃의 .git 디렉토리와 유사하게 동작합니다.\n",
    "\n",
    "* 작업 처리과정에서 장애 혹은 애플리케이션 오류 등의 이유로 완전히 정리되지 않은 부분 파일(partial files)들이 존재할 수 있으나, 후속 처리 쿼리나 데이터 애플리케이션은 이러한 파일들을 구분할 수 있어야합니다. \n",
    "\n",
    "![partial file](images/figure.1-4.png)\n",
    "\n",
    "* Job1 은 3~4.parquet 파일을 생성하는 작업 과정에서 실패로 인해, 3.parquet 만 생성되었고\n",
    "* Job2 는 동일한 애플리케이션 재실행을 통해 3~4.parquet 이 제대로 생성 되었다\n",
    "* 파일 목록만으로는 최종 데이터를 파악하기 어렵지만, 트랜잭션 로그를 활용하면 정상적인 파일을 확인할 수 있습니다.\n",
    "\n",
    "![transaction log](images/figure.1-5.png)\n",
    "\n",
    "* t1 시점에서는 작업이 실패했기 때문에 트랜젝션 로그에 저장되지 않았고, 데이터 뷰는 t0 시점과 동일하게 2개가 조회 됩니다.\n",
    "* t2 시점에서는 작업이 성공했기 때문에 트랜젝션 로그에 2개의 파일이 노출되어 모든 데이터가 조회됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Implementation of Atomicity on Delta Lake\n",
    "\n",
    "> 델타 레이크가 원자성(atomicity)을 보장할 수 있는 매커니즘의 핵심은 바로 \"트랜잭션 로그\"입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. How Does the Transaction Log Work?\n",
    "\n",
    "#### Breaking Down Transactions Into Atomic Commits\n",
    "\n",
    "> '델타 레이크'는 모든 연산(CRUD)을 아래에 명시된 discrete 한 단계로 분해합니다.\n",
    "\n",
    "##### Update metadata : 테이블의 이름, 스키마 혹은 파티셔닝의 변경 뿐만 아니라 모든 테이블 메타데이터를 변경합니다\n",
    "##### Add file : 트랜젝션 로그에 데이터 파일을 추가합니다\n",
    "##### Remove file : 트랜잭션 로그로부터 파일을 제거합니다\n",
    "##### Set transaction : 구조화된 스트리밍 작업이 지정된 ID로 마이크로 배치를 커밋했음을 기록합니다\n",
    "##### Change protocol : Delta Lake 트랜잭션 로그를 최신 소프트웨어 프로토콜로 전환하여 새로운 기능을 활성화합니다\n",
    "##### Commit info : 커밋에 대한 정보, 작업이 수행된 위치 및 시간을 포함합니다\n",
    "\n",
    "> 이러한 액션들은 트랜잭션 로그에 순차적으로 저장되며, 커밋이라고 불리는 원자적인 단위이기도 합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Delta Transaction Log Protocol\n",
    "\n",
    "> 이 장에서는 어떻게 Delta tarnsaction log 가 ACID 한 속성을 분산 파일 시스템에 저장된 대용량 콜렉션에 적용하는지 기술합니다. 프로토콜은 아래의 목적에 부합하도록 설계되었습니다.\n",
    "\n",
    "##### Serializable ACID Writes : ACID semantic 을 보장하면서 동시에 다수의 저장이 가능합니다\n",
    "##### Snapshot Isolation for Reads : reader 는 다수의 writer 가 동시에 저장하는 중에도 consistent 한 Delta table 의 snapshot 을 읽을 수 있습니다\n",
    "##### Scalability to billions of partitions or files : Delta table 에 대한 질의는 하나의 장비 혹은 병렬로 수행될 수 있습니다\n",
    "##### Self-describing : 모든 Delta table 의 메타데이터는 데이터와 함께 저장됩니다. 이러한 설계가 분리된 metastore 유지하는 부담을 제거할 수 있으며, 파일시스템 도구를 통한 복사만으로 정적인 테이블 복사가 가능하게 됩니다.\n",
    "##### Support for incremental processing : reader 는 Delta log 를 tail 함으로써 주어진 시간 내에 데이터가 추가되었는지를 인지할 수 있고 효과적인 스트리밍 처리가 가능합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Logstore\n",
    "\n",
    "> LogStore 는 Delta transaction log 를 읽고 쓰는데에 필요한 파일시스템을 위한 인터페이스입니다. 대부분의 파일 스토리지 시스템은 원자성을 보장하지 않기 때문에 LogStore API 를 통해 서 이용할 수 있습니다.\n",
    "\n",
    "##### 어떤 파일도 전체가 보이거나 아예 보이지 않도록 하며, partial files 를 생성해내지 않습니다\n",
    "##### 단 하나의 writer 만이 최종 경로에 파일을 생성할 수 있기 때문에, 수 많은 writers 들이 그 자신의 파일들을 병렬로 쓰는 것은 가능합니다. \n",
    "##### Logstore 는 ACID consistent 파일 목록을 제공합니다\n",
    "\n",
    "<br>\n",
    "\n",
    "#### The Delta Lake Transaction Log at the File Level\n",
    "\n",
    "> Delta table 이 생성될 때, `_delta_log` 라는 하위 디렉토리에 트랜잭션 로그가 자동적으로 생성됩니다. 테이블에 대한 변경이 발생할 때에 순차적으로 변경사항에 대하여 트랜잭션 로그에 원자적 커밋들이 저장됩니다. 매 커밋들은 000000.json 파일로 시작하는 JSON 파일 형태로 저장되며, 이어지는 변경 사항들은 숫자가 늘어나면서 000001.json 과 같이 저장됩니다. 이 숫자가 테이블의 `새로운 버전`을 나타냅니다.\n",
    "\n",
    "![delta on disk](images/figure.1-6.png)\n",
    "\n",
    "##### Implementing Atomicity : 하나의 온전한 트랜잭션은 하나의 json 파일로 구성되며, 이러한 동작 하나 하나가 커밋의 단위로 관리됩니다\n",
    "\n",
    "![automicity in delta](images/figure.1-7.png)\n",
    "\n",
    "> 그림에서와 같이 1.parquet, 2.parquet 파일은 Delta Lake 테이블의 일부가 더 이상 아니지만, 트랜잭션 로그에는 여전히 저장되어지는데, 어떤 트랜잭션도 취소 혹은 다시 되돌릴 수 있기 때문입니다. 이러한 이유로 Delta Lake 는 원자적 커밋을 통해 'time travel' 하거나 'audit' 이 가능하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- add: struct (nullable = true)\n",
      " |    |-- dataChange: boolean (nullable = true)\n",
      " |    |-- modificationTime: long (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |-- commitInfo: struct (nullable = true)\n",
      " |    |-- isBlindAppend: boolean (nullable = true)\n",
      " |    |-- operation: string (nullable = true)\n",
      " |    |-- operationMetrics: struct (nullable = true)\n",
      " |    |    |-- numFiles: string (nullable = true)\n",
      " |    |    |-- numOutputBytes: string (nullable = true)\n",
      " |    |    |-- numOutputRows: string (nullable = true)\n",
      " |    |-- operationParameters: struct (nullable = true)\n",
      " |    |    |-- mode: string (nullable = true)\n",
      " |    |    |-- partitionBy: string (nullable = true)\n",
      " |    |-- timestamp: long (nullable = true)\n",
      " |-- metaData: struct (nullable = true)\n",
      " |    |-- createdTime: long (nullable = true)\n",
      " |    |-- format: struct (nullable = true)\n",
      " |    |    |-- provider: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- partitionColumns: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- schemaString: string (nullable = true)\n",
      " |-- protocol: struct (nullable = true)\n",
      " |    |-- minReaderVersion: long (nullable = true)\n",
      " |    |-- minWriterVersion: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>add</th><th>commitInfo</th><th>metaData</th><th>protocol</th></tr>\n",
       "<tr><td>null</td><td>[false, WRITE, [3, 3729, 3], [Overwrite, []], 1628951091358]</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>null</td><td>null</td><td>[1, 2]</td></tr>\n",
       "<tr><td>null</td><td>null</td><td>[1628951089003, [parquet], eaf1458e-43a5-4d6b-a738-089d6316ddd1, [], {&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{...</td><td>null</td></tr>\n",
       "<tr><td>[true, 1628951089811, part-00000-f05b7216-70cb-45c2-bd80-b2ea8c63397b-c000.snappy.parquet, 1189]</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>[true, 1628951089812, part-00001-e7cce94f-bc22-4cfb-83f6-b41dc2f5a122-c000.snappy.parquet, 1243]</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>[true, 1628951089812, part-00002-0492c0ec-c744-4731-9473-c7cd728a8a51-c000.snappy.parquet, 1297]</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------------------------------------------------------------------------------------+------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------+\n",
       "|                                                                                             add|                                                  commitInfo|                                                                                            metaData|protocol|\n",
       "+------------------------------------------------------------------------------------------------+------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------+\n",
       "|                                                                                            null|[false, WRITE, [3, 3729, 3], [Overwrite, []], 1628951091358]|                                                                                                null|    null|\n",
       "|                                                                                            null|                                                        null|                                                                                                null|  [1, 2]|\n",
       "|                                                                                            null|                                                        null|[1628951089003, [parquet], eaf1458e-43a5-4d6b-a738-089d6316ddd1, [], {\"type\":\"struct\",\"fields\":[{...|    null|\n",
       "|[true, 1628951089811, part-00000-f05b7216-70cb-45c2-bd80-b2ea8c63397b-c000.snappy.parquet, 1189]|                                                        null|                                                                                                null|    null|\n",
       "|[true, 1628951089812, part-00001-e7cce94f-bc22-4cfb-83f6-b41dc2f5a122-c000.snappy.parquet, 1243]|                                                        null|                                                                                                null|    null|\n",
       "|[true, 1628951089812, part-00002-0492c0ec-c744-4731-9473-c7cd728a8a51-c000.snappy.parquet, 1297]|                                                        null|                                                                                                null|    null|\n",
       "+------------------------------------------------------------------------------------------------+------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 첫 번째 version 의 트랜잭션 로그를 읽어봅시다 - /work/jovyan/tmp/delta-table/_delta_log/00000000000000000000.json\n",
    "_delta_log = f\"{work_dir}/tmp/say-hello/_delta_log/00000000000000000000.json\"\n",
    "log_v1 = spark.read.json(_delta_log)\n",
    "log_v1.printSchema()\n",
    "display(log_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 트랜잭션 정보는 commit, add 그리고 CRC pieces 에 대한 정보들을 담고 있습니다\n",
    "\n",
    "##### Commit Information : Delta transaction log 는 메타데이터를 커밋합니다\n",
    "\n",
    "![metadata](images/delta.commitInfo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------------+-------------------+-------------+\n",
      "|isBlindAppend|operation|operationMetrics|operationParameters|timestamp    |\n",
      "+-------------+---------+----------------+-------------------+-------------+\n",
      "|false        |WRITE    |[3, 3729, 3]    |[Overwrite, []]    |1628951091358|\n",
      "+-------------+---------+----------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commitInfo = log_v1.select(\"commitInfo\").where(\"commitInfo is not null\")\n",
    "c = commitInfo.withColumnRenamed(\"commitInfo\", \"c\")\n",
    "c.select(\"c.*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Information : Delta transaction log 가 `add` 한 metadata 는 아래와 같이 확인할 수 있습니다.\n",
    "\n",
    "![metadata](images/delta.add.png)\n",
    "\n",
    "> 스파크의 경우 경로가 지정된 경우에는 해당 디렉토리에 포함된 모든 파일의 목록을 읽어오는데(listFrom) 특히 클라우드 스토리지의 경우 매우 비효율적이나, Delta Lake 의 경우 transaction log 통한 파일목록을 가져올 수 있으며, 지정된 version 에 해당하는 파일목록만 사용하기 때문에 인터넷 환경 혹은 분산환경의 저장소에서 페타바이트 수준의 데이터 처리 성능을 높일 수 있습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- add: struct (nullable = true)\n",
      " |    |-- dataChange: boolean (nullable = true)\n",
      " |    |-- modificationTime: long (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |    |-- size: long (nullable = true)\n",
      "\n",
      "+----------+----------------+-------------------------------------------------------------------+----+\n",
      "|dataChange|modificationTime|path                                                               |size|\n",
      "+----------+----------------+-------------------------------------------------------------------+----+\n",
      "|true      |1628951089811   |part-00000-f05b7216-70cb-45c2-bd80-b2ea8c63397b-c000.snappy.parquet|1189|\n",
      "|true      |1628951089812   |part-00001-e7cce94f-bc22-4cfb-83f6-b41dc2f5a122-c000.snappy.parquet|1243|\n",
      "|true      |1628951089812   |part-00002-0492c0ec-c744-4731-9473-c7cd728a8a51-c000.snappy.parquet|1297|\n",
      "+----------+----------------+-------------------------------------------------------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add = log_v1.select(\"add\").where(\"add is not null\")\n",
    "add.printSchema()\n",
    "a = add.withColumnRenamed(\"add\", \"a\")\n",
    "a.select(\"a.*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CRC file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000.json  00000000000000000001.json\n"
     ]
    }
   ],
   "source": [
    "!ls tmp/say-hello/_delta_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.session.timeZone', 'Asia/Seoul'),\n",
       " ('spark.jars',\n",
       "  'file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.7.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
       " ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.7.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/root/.ivy2/jars/io.delta_delta-core_2.12-0.7.0.jar,/root/.ivy2/jars/org.antlr_antlr4-4.7.jar,/root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,/root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,/root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,/root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,/root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,/root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.files',\n",
       "  'file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.7.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.jars.packages', 'io.delta:delta-core_2.12:0.7.0'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.spark.sql.delta.catalog.DeltaCatalog')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quickly Recomputing State With Checkpoint Files\n",
    "\n",
    "> transaction log 생성 과정에서 너무 많은 로그 파일이 생성되면 다시 hadoop small file 문제가 생길 수 있기 때문에, Delta Lake 는 매 10번째 커밋 마다 Parquet format 으로 구성된 checkpoint file 을 생성하는데, 이 체크포인트 파일은 현재 시점까지의 모든 상태를 저장합니다. 즉, 스파크 입장에서는 특정 시점까지의 많고 작은 JSON 파일들을 읽기 보다 효과적인 '지름길'을 가지고 있다고 보면 됩니다.\n",
    "\n",
    "![commit](images/figure.1-9.png)\n",
    "\n",
    "> 스파크는 아래와 같이 1개의 json 파일이 있다면 모두 읽어서 결과(3개의 커밋 결과)를 메모리에 캐시(cache_v2)하고, 추가적인 커밋이 없는 한 캐시를 활용합니다.\n",
    "\n",
    "![cache](images/figure.1-10.png)\n",
    "\n",
    "> 위의 그림에서 추가적인 5개의 커밋이 발생했으므로, 처음(version 0)부터 모든 파일을 읽어서 캐시(cache_v7)합니다.\n",
    "\n",
    "![cache-2](images/figure.1-11.png)\n",
    "\n",
    "> 이후 추가적인 5개의 커밋이 발생하였고, Delta Lake 는 10번째의 커밋이 발생한 이후에 11번째 커밋에서 checkpoint file (0000010.checkpoint.parquet) 파일을 생성합니다. Delta Lake 는 지연된 트랜잭션들을 피하기 위해서, 처음(version 0)부터 현재 시점까지의 정보를 캐시(cache_v12) 합니다.\n",
    "\n",
    "![cache-3](images/figure.1-12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_appended = [\n",
    "    Row(\"정휘센\", \"첫 출근입니다\", 300),\n",
    "    Row(\"김싸이언\", \"처음 뵙겠습니다\", 200),\n",
    "    Row(\"유코드제로\", \"오늘 두 번째 출근이네요\", 100)\n",
    "]\n",
    "schema = \"`author` string, `title` string, `pages` int\"\n",
    "df4 = spark.createDataFrame(more_appended, schema)\n",
    "df4.write.format(\"delta\").mode(\"append\").save(\"tmp/say-hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 ~ 9 까지 json 파일 생성 이후에 약간의 지연 이후 10 넘버의 checkpoint 생성 이후에 다시 12번까지 생성\n",
    "iter_schema = \"`author` string, `title` string, `pages` int\"\n",
    "for num in range(0, 1000, 100):\n",
    "    iter_row = [Row(\"오토봇\", \"인사-{}\".format(num), num)]\n",
    "    iter_df = spark.createDataFrame(iter_row, iter_schema)\n",
    "    iter_df.write.format(\"delta\").mode(\"append\").save(\"tmp/say-hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- txn: struct (nullable = true)\n",
      " |    |-- appId: string (nullable = true)\n",
      " |    |-- version: long (nullable = true)\n",
      " |    |-- lastUpdated: long (nullable = true)\n",
      " |-- add: struct (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |    |-- partitionValues: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- modificationTime: long (nullable = true)\n",
      " |    |-- dataChange: boolean (nullable = true)\n",
      " |    |-- stats: string (nullable = true)\n",
      " |    |-- tags: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |-- remove: struct (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |    |-- deletionTimestamp: long (nullable = true)\n",
      " |    |-- dataChange: boolean (nullable = true)\n",
      " |-- metaData: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- format: struct (nullable = true)\n",
      " |    |    |-- provider: string (nullable = true)\n",
      " |    |    |-- options: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- schemaString: string (nullable = true)\n",
      " |    |-- partitionColumns: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- configuration: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- createdTime: long (nullable = true)\n",
      " |-- protocol: struct (nullable = true)\n",
      " |    |-- minReaderVersion: integer (nullable = true)\n",
      " |    |-- minWriterVersion: integer (nullable = true)\n",
      " |-- commitInfo: struct (nullable = true)\n",
      " |    |-- version: long (nullable = true)\n",
      " |    |-- timestamp: timestamp (nullable = true)\n",
      " |    |-- userId: string (nullable = true)\n",
      " |    |-- userName: string (nullable = true)\n",
      " |    |-- operation: string (nullable = true)\n",
      " |    |-- operationParameters: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- job: struct (nullable = true)\n",
      " |    |    |-- jobId: string (nullable = true)\n",
      " |    |    |-- jobName: string (nullable = true)\n",
      " |    |    |-- runId: string (nullable = true)\n",
      " |    |    |-- jobOwnerId: string (nullable = true)\n",
      " |    |    |-- triggerType: string (nullable = true)\n",
      " |    |-- notebook: struct (nullable = true)\n",
      " |    |    |-- notebookId: string (nullable = true)\n",
      " |    |-- clusterId: string (nullable = true)\n",
      " |    |-- readVersion: long (nullable = true)\n",
      " |    |-- isolationLevel: string (nullable = true)\n",
      " |    |-- isBlindAppend: boolean (nullable = true)\n",
      " |    |-- operationMetrics: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- userMetadata: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>txn</th><th>add</th><th>remove</th><th>metaData</th><th>protocol</th><th>commitInfo</th></tr>\n",
       "<tr><td>null</td><td>[part-00000-f05b7216-70cb-45c2-bd80-b2ea8c63397b-c000.snappy.parquet, [], 1189, 1628951089811, fa...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00000-2036c214-e2fd-4061-9d44-eb1046e8b6ce-c000.snappy.parquet, [], 1045, 1628951880429, fa...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00001-e7cce94f-bc22-4cfb-83f6-b41dc2f5a122-c000.snappy.parquet, [], 1243, 1628951089812, fa...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00002-9134f9ef-6a29-4431-991b-512a3c69fc90-c000.snappy.parquet, [], 964, 1628951883436, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00000-b6e0e0ae-e9d6-4bec-a3f7-59ac7c7a5a4e-c000.snappy.parquet, [], 468, 1628951884816, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00002-f58e284d-ab69-4374-ab09-6b78682bd92a-c000.snappy.parquet, [], 964, 1628951887619, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00000-035b3a33-0a65-4edc-ac6f-fdaf165d0d50-c000.snappy.parquet, [], 468, 1628951095838, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00000-2cf95065-04dc-415e-8c54-b7dba86b6fb6-c000.snappy.parquet, [], 468, 1628951891370, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00002-49bf0928-b09e-4c32-b519-f7d0a4c1be4a-c000.snappy.parquet, [], 1225, 1628951880420, fa...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00000-9a8c9c48-ab88-44c1-bf25-cf73853cd396-c000.snappy.parquet, [], 468, 1628951882009, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00001-9cd90836-3994-4156-a53d-a37643906aec-c000.snappy.parquet, [], 1072, 1628951095847, fa...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00002-09201fdf-32d0-46c0-b00a-c193d8976caf-c000.snappy.parquet, [], 964, 1628951890130, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00002-419c4b96-0a11-47d1-b889-9c71a0f66b9b-c000.snappy.parquet, [], 946, 1628951882002, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00001-b84b5f16-b2c7-4f98-bdb3-19f4076d5a1b-c000.snappy.parquet, [], 1099, 1628951880451, fa...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00000-d9df257e-6af5-4128-95ae-c7dbfad44689-c000.snappy.parquet, [], 468, 1628951887610, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00000-e95bd70a-09b3-4dc0-b9e1-7f1a8924468c-c000.snappy.parquet, [], 468, 1628951890140, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00002-0492c0ec-c744-4731-9473-c7cd728a8a51-c000.snappy.parquet, [], 1297, 1628951089812, fa...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00002-d88e0638-f509-45da-80cb-345b84b77f42-c000.snappy.parquet, [], 964, 1628951888903, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>[part-00002-330b5c95-b3bc-4c0a-94a6-ca4a32678331-c000.snappy.parquet, [], 964, 1628951891397, fal...</td><td>null</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>null</td><td>null</td><td>null</td><td>null</td><td>[1, 2]</td><td>null</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----+----------------------------------------------------------------------------------------------------+------+--------+--------+----------+\n",
       "| txn|                                                                                                 add|remove|metaData|protocol|commitInfo|\n",
       "+----+----------------------------------------------------------------------------------------------------+------+--------+--------+----------+\n",
       "|null|[part-00000-f05b7216-70cb-45c2-bd80-b2ea8c63397b-c000.snappy.parquet, [], 1189, 1628951089811, fa...|  null|    null|    null|      null|\n",
       "|null|[part-00000-2036c214-e2fd-4061-9d44-eb1046e8b6ce-c000.snappy.parquet, [], 1045, 1628951880429, fa...|  null|    null|    null|      null|\n",
       "|null|[part-00001-e7cce94f-bc22-4cfb-83f6-b41dc2f5a122-c000.snappy.parquet, [], 1243, 1628951089812, fa...|  null|    null|    null|      null|\n",
       "|null|[part-00002-9134f9ef-6a29-4431-991b-512a3c69fc90-c000.snappy.parquet, [], 964, 1628951883436, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00000-b6e0e0ae-e9d6-4bec-a3f7-59ac7c7a5a4e-c000.snappy.parquet, [], 468, 1628951884816, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00002-f58e284d-ab69-4374-ab09-6b78682bd92a-c000.snappy.parquet, [], 964, 1628951887619, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00000-035b3a33-0a65-4edc-ac6f-fdaf165d0d50-c000.snappy.parquet, [], 468, 1628951095838, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00000-2cf95065-04dc-415e-8c54-b7dba86b6fb6-c000.snappy.parquet, [], 468, 1628951891370, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00002-49bf0928-b09e-4c32-b519-f7d0a4c1be4a-c000.snappy.parquet, [], 1225, 1628951880420, fa...|  null|    null|    null|      null|\n",
       "|null|[part-00000-9a8c9c48-ab88-44c1-bf25-cf73853cd396-c000.snappy.parquet, [], 468, 1628951882009, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00001-9cd90836-3994-4156-a53d-a37643906aec-c000.snappy.parquet, [], 1072, 1628951095847, fa...|  null|    null|    null|      null|\n",
       "|null|[part-00002-09201fdf-32d0-46c0-b00a-c193d8976caf-c000.snappy.parquet, [], 964, 1628951890130, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00002-419c4b96-0a11-47d1-b889-9c71a0f66b9b-c000.snappy.parquet, [], 946, 1628951882002, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00001-b84b5f16-b2c7-4f98-bdb3-19f4076d5a1b-c000.snappy.parquet, [], 1099, 1628951880451, fa...|  null|    null|    null|      null|\n",
       "|null|[part-00000-d9df257e-6af5-4128-95ae-c7dbfad44689-c000.snappy.parquet, [], 468, 1628951887610, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00000-e95bd70a-09b3-4dc0-b9e1-7f1a8924468c-c000.snappy.parquet, [], 468, 1628951890140, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00002-0492c0ec-c744-4731-9473-c7cd728a8a51-c000.snappy.parquet, [], 1297, 1628951089812, fa...|  null|    null|    null|      null|\n",
       "|null|[part-00002-d88e0638-f509-45da-80cb-345b84b77f42-c000.snappy.parquet, [], 964, 1628951888903, fal...|  null|    null|    null|      null|\n",
       "|null|[part-00002-330b5c95-b3bc-4c0a-94a6-ca4a32678331-c000.snappy.parquet, [], 964, 1628951891397, fal...|  null|    null|    null|      null|\n",
       "|null|                                                                                                null|  null|    null|  [1, 2]|      null|\n",
       "+----+----------------------------------------------------------------------------------------------------+------+--------+--------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chkpt0 = spark.read.parquet(\"tmp/say-hello/_delta_log/00000000000000000010.checkpoint.parquet\")\n",
    "chkpt0.printSchema()\n",
    "display(chkpt0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- add: struct (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |    |-- partitionValues: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- modificationTime: long (nullable = true)\n",
      " |    |-- dataChange: boolean (nullable = true)\n",
      " |    |-- stats: string (nullable = true)\n",
      " |    |-- tags: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+\n",
      "|path                                                               |partitionValues|size|modificationTime|dataChange|stats|tags|\n",
      "+-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+\n",
      "|part-00000-f05b7216-70cb-45c2-bd80-b2ea8c63397b-c000.snappy.parquet|[]             |1189|1628951089811   |false     |null |null|\n",
      "|part-00000-2036c214-e2fd-4061-9d44-eb1046e8b6ce-c000.snappy.parquet|[]             |1045|1628951880429   |false     |null |null|\n",
      "|part-00001-e7cce94f-bc22-4cfb-83f6-b41dc2f5a122-c000.snappy.parquet|[]             |1243|1628951089812   |false     |null |null|\n",
      "|part-00002-9134f9ef-6a29-4431-991b-512a3c69fc90-c000.snappy.parquet|[]             |964 |1628951883436   |false     |null |null|\n",
      "|part-00000-b6e0e0ae-e9d6-4bec-a3f7-59ac7c7a5a4e-c000.snappy.parquet|[]             |468 |1628951884816   |false     |null |null|\n",
      "|part-00002-f58e284d-ab69-4374-ab09-6b78682bd92a-c000.snappy.parquet|[]             |964 |1628951887619   |false     |null |null|\n",
      "|part-00000-035b3a33-0a65-4edc-ac6f-fdaf165d0d50-c000.snappy.parquet|[]             |468 |1628951095838   |false     |null |null|\n",
      "|part-00000-2cf95065-04dc-415e-8c54-b7dba86b6fb6-c000.snappy.parquet|[]             |468 |1628951891370   |false     |null |null|\n",
      "|part-00002-49bf0928-b09e-4c32-b519-f7d0a4c1be4a-c000.snappy.parquet|[]             |1225|1628951880420   |false     |null |null|\n",
      "|part-00000-9a8c9c48-ab88-44c1-bf25-cf73853cd396-c000.snappy.parquet|[]             |468 |1628951882009   |false     |null |null|\n",
      "|part-00001-9cd90836-3994-4156-a53d-a37643906aec-c000.snappy.parquet|[]             |1072|1628951095847   |false     |null |null|\n",
      "|part-00002-09201fdf-32d0-46c0-b00a-c193d8976caf-c000.snappy.parquet|[]             |964 |1628951890130   |false     |null |null|\n",
      "|part-00002-419c4b96-0a11-47d1-b889-9c71a0f66b9b-c000.snappy.parquet|[]             |946 |1628951882002   |false     |null |null|\n",
      "|part-00001-b84b5f16-b2c7-4f98-bdb3-19f4076d5a1b-c000.snappy.parquet|[]             |1099|1628951880451   |false     |null |null|\n",
      "|part-00000-d9df257e-6af5-4128-95ae-c7dbfad44689-c000.snappy.parquet|[]             |468 |1628951887610   |false     |null |null|\n",
      "|part-00000-e95bd70a-09b3-4dc0-b9e1-7f1a8924468c-c000.snappy.parquet|[]             |468 |1628951890140   |false     |null |null|\n",
      "|part-00002-0492c0ec-c744-4731-9473-c7cd728a8a51-c000.snappy.parquet|[]             |1297|1628951089812   |false     |null |null|\n",
      "|part-00002-d88e0638-f509-45da-80cb-345b84b77f42-c000.snappy.parquet|[]             |964 |1628951888903   |false     |null |null|\n",
      "|part-00002-330b5c95-b3bc-4c0a-94a6-ca4a32678331-c000.snappy.parquet|[]             |964 |1628951891397   |false     |null |null|\n",
      "|part-00000-6c51c7f3-bf52-4fc1-b8c3-9ec71687c3bd-c000.snappy.parquet|[]             |468 |1628951886274   |false     |null |null|\n",
      "|part-00002-68da8e48-796b-4d5e-b9d5-a293f671b4d9-c000.snappy.parquet|[]             |964 |1628951884859   |false     |null |null|\n",
      "|part-00002-66a1573a-3a10-4cf6-a643-dd7056209ec2-c000.snappy.parquet|[]             |1036|1628951095836   |false     |null |null|\n",
      "|part-00000-649a0ae0-4d4c-4d08-a96b-60c1d37fc885-c000.snappy.parquet|[]             |468 |1628951883445   |false     |null |null|\n",
      "|part-00000-7234663d-cf8e-42fa-b14b-e1f24898c12a-c000.snappy.parquet|[]             |468 |1628951888890   |false     |null |null|\n",
      "|part-00002-d8aaaae8-b8a9-412e-9d09-fa84f6ba4eeb-c000.snappy.parquet|[]             |964 |1628951886264   |false     |null |null|\n",
      "+-------------------------------------------------------------------+---------------+----+----------------+----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add = chkpt0.select(\"add\").where(\"add is not null\")\n",
    "add.printSchema()\n",
    "a = add.withColumnRenamed(\"add\", \"a\")\n",
    "a.select(\"a.*\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cache-4](images/figure.1-14.png)\n",
    "\n",
    "```json\n",
    "stats_parsed:{\n",
    "    \"numRecords\": 7,\n",
    "    \"minValues\": {\n",
    "        \"addr_state\": \"IA\",\n",
    "        \"count\": 3,\n",
    "        \"stream_no\": 3\n",
    "    },\n",
    "    \"maxValues\": {\n",
    "        \"addr_state\": \"TX\",\n",
    "        \"count\": 9,\n",
    "        \"stream_no\": 3\n",
    "    },\n",
    "    \"nullCount\": {\n",
    "        \"addr_state\": 0,\n",
    "        \"count\": 0,\n",
    "        \"stream_no\": 0\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "> 스파크는 페타바이트 규모의 데이터를 읽을 때에 모든 데터를 읽지 않고도 stats_parsed 항목의 통계정보를 통해 보다 빠르게 통계정보를 획득할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Dealing With Multiple Concurrent Reads and Writes\n",
    "\n",
    "> 여태까지 우리는 하이레벨 Delta Lake 트랜젝션 로그를 이해했습니다. 이제는 동시성에 대해 알아봅니다. Delta Lake 는 어떻게 다수의 동시성 읽기와 쓰기를 다루는지 알아보겠습니다. 스파크 기반의 데이터 처리를 수행하므로, 하나의 테이블에 대해 다수의 이용자가 사용하게 되며, Delta Lake 는 `낙관적 동시성 제어`를 합니다.\n",
    "\n",
    "#### What Is Optimistic Concurrency Control?\n",
    "\n",
    "> '낙관적 동시성 제어'는 다수의 이용자가 하나의 테이블을 다룰 때에 서로 충돌하지 않고 트랜잭션이 잘 완료될 것이라는 가정을 말합니다. 이는 수 페타바이트 규모의 데이터를 다룬다고 가정한다면 이용자들은 서로 다른 부분의 데이터를 다루고 있을 가능성이 더 높을 것입니다. 테이블을 다룰 때에 서로 다른 클라이언트들이 테이블의 서로 다른 부분을 수정하거나, 충돌이 발생하지 않는 액션을 수행하는 한, 그러한 오퍼레이션들은 문제가 되지 않기 때문에, 우리는 '난꽌적으로' 그 작업을 완수할 수 있습니다.\n",
    "\n",
    "> 반면 클라이언트가 동시에 같은 부분을 수정하는 경우, Delta Lake 는 이를 해결하기 위한 프로토콜을 가지고 있습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Solving conflicts optimistically\n",
    "\n",
    "##### 가. Ensuring serializability : `상호 배제 (Mutual Exclusion)` 즉, 여러 writers 가 있다고 하더라도 소위 데이터베이스에서의 `serializability` 변경 순서대로 수행되는 속성을 보장합니다. **사건이 동시에 발생하더라도, 마치 순차적으로 발생한 것 처럼 수행**합니다.\n",
    "\n",
    "![user-1](images/figure.1-15.png)\n",
    "\n",
    "![user-2](images/figure.1-16.png)\n",
    "\n",
    "> \"User 1\"이 `00002.json` 파일을 커밋하려고 할 때에, \"User 2\"가 이미 점유하고 있다면 \"User 1\"은 이미 `000002.json`이 존재함을 알고, 상호배제에 따라 \"커밋에 실패했군\" 이라고 말하고, `00003.json` 파일을 커밋하게 됩니다.\n",
    "\n",
    "![serializability](images/figure.1-17.png)\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 나. Applying Optimistic Concurrency Control in Delta.\n",
    "\n",
    "> ACID 트랜잭션을 제공하기 위해서 Delta Lake 는 **어떻게 커밋들이 순서지어 져야 하는지(concept of `serializability` in databases) 규정하는**, 그리고 **두개 이상의 커밋이 동시에 수행되어질 때에 어떻게 처리해야 할 지를 결정**하는 `프로토콜`을 가집니다.\n",
    "\n",
    "> Delta Lake 는 이러한 문제를 `mutual exclusion` 의 규칙에 의해 구현되며, 어떤 충돌에 대해서도 낙관적으로 해결하려고 시도합니다. 이 프로토콜은 Delta Lake 로 하여금, `isolation` 의 ACID 원칙을 제공해주도록 허용합니다. 여기서 `isolation` 이란 테이블에 대한 다수의 동시성 writes 의 결과 상태가 마치 writes 들이 서로 격리되고, 순차적으로 발생한 것처럼 수행되는 것을 보장합니다. 대게 아래의 5단계를 통해 수행됩니다.\n",
    "\n",
    "* a. 테이블 시작 버전을 기록합니다\n",
    "* b. 읽기/쓰기를 기록합니다\n",
    "* c. 커밋을 시도합니다\n",
    "* d. 만일 누군가가 해당 리소스를 이미 획득(win)하고 있다면, 당신이 읽으려고 하는 것이 변경되었는지를 확인합니다\n",
    "* e. 다시 반복합니다\n",
    "\n",
    "> 예를 들어, 2명의 이용자가 동일한 테이블을 읽고, 동시에 데이터를 삽입하려고 한다면 ?\n",
    "\n",
    "![conflict](images/figure.1-18.png)\n",
    "\n",
    "* a. Delta Lake는 변경하기 전에 읽은 테이블(버전 0)의 시작 테이블 버전을 기록합니다.\n",
    "* b. 사용자 1과 2는 모두 동시에 테이블에 일부 데이터를 추가하려고 시도합니다. 여기에서 한 커밋만 다음에 올 수 있고 000001.json으로 기록될 수 있기 때문에 충돌이 발생했습니다.\n",
    "* c~d. Delta Lake는 상호배제 개념으로 이 충돌을 처리합니다. 즉, 한 명의 사용자만 000001.json을 성공적으로 커밋할 수 있어서, 사용자 1의 커밋은 수락되고 사용자 2의 커밋은 거부됩니다.\n",
    "* e. Delta Lake는 사용자 2에 대해 오류를 발생시키는 대신 이 충돌을 낙관적으로 처리하는 것을 선호합니다. 테이블에 대한 새로운 커밋이 있는지 확인하고 해당 변경 사항을 반영하도록 테이블을 자동으로 업데이트한 다음 데이터 처리 없이 새로 업데이트된 테이블에서 사용자 2의 커밋을 재시도하여 000002.json을 성공적으로 커밋합니다.\n",
    "\n",
    "> 대부분의 경우 이러한 조정은 조용하고 원활하며 성공적으로 이루어집니다. 그러나 Delta Lake가 낙관적으로 해결할 수 없는 양립할 수 없는 문제가 있는 경우 (예: 사용자 1이 사용자 2도 삭제한 파일을 삭제한 경우) 유일한 옵션은 오류를 발생시키는 것입니다. \n",
    "\n",
    "> 끝으로 Delta Lake 테이블에서 이루어진 모든 트랜잭션은 스토리지에 직접 저장되기 때문에 이 프로세스는 `durability`라는 ACID 속성을 만족하므로 시스템 장애 시에도 지속됩니다.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 다. Multiversion Concurrency Control.\n",
    "\n",
    "> 이러한 일들이 파일 시스템 내에서 수행될 수 있는 것은 Delta 의 트랜잭션들이 `Multiversion Concurrency Control (MVCC)`을 이용하여 구현되었기 때문입니다. 이는 **관계형 데이터베이스 관리 시스템에서 데이터베이스에 동시에 접근하는 경우에 흔히 사용되는 동시성 제어 방법**입니다. \n",
    "\n",
    "> Delta Lake 의 데이터 객체와 로그들은 `immutable` 속성을 가지며, Delta Lake 는 MVCC 를 이용하여 **기 존재하는 데이터를 보호**하고(예를 들어 writes 들 간에 트랜잭션을 보장하는), 더불어 **질의 속도와 쓰기 퍼포먼스를 향상**시킵니다. 이러한 매커니즘을 통해 쓰기 연산은 아래의 3가지 스테이지를 가집니다.\n",
    "\n",
    "* *Read* : 수정이 필요한 로우들을 식별하기 위해서 최근의 가용한 테이블의 버전을 읽어옵니다\n",
    "\n",
    "* *Write* : 새로운 데이터 파일들을 쓰게되어 발생한 모든 변경사항들을 스테이지 합니다. 여기서 발생하는 모든 삽입 및 수정사항 들은 새로운 파일들의 형태로 저장됩니다.\n",
    "\n",
    "* *Validate and commit* : 변경사항들을 커밋하기 전에, 읽어왔던 스냅샷 시점 이후에 동시에 커밋이 이루어진 어떠한 변경 사항들에 대해서도 충돌여부를 확인합니다. 충돌이 없다면 모든 스테이징된 변경 사항은 커밋되어 새로운 버전의 스냅샷으로 생성됩니다. 반면에 충돌이 발생한 경우에, 쓰기 연산은 '동시 변경 예외'를 발생시키고 실패하는데, Parqeut 테이블에 쓰기에 실패했을 때와 같이 손상시키게 됩니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "> 테이블의 변경사항이 늘어남에 따라 Delta의 MVCC 알고리즘은 **업데이트 또는 제거 중인 레코드가 포함된 파일을 즉시 교체하지 않고 여러 데이터 복사본을 유지**합니다. MVCC는 테이블 상태의 일관된 보기(consistent view)에 대한 직렬화(serializability) 및 스냅샷 격리(snapshot isolation)를 허용하므로 reader 들은 작업 중에 테이블이 수정된 경우에도 Apache Spark 작업이 시작된 테이블의 일관된 스냅샷 보기를 계속 볼 수 있습니다. writers 들이 동시에 수정 작업을 할 때 트랜잭션 로그를 사용하여 처리할 데이터 파일을 선택적으로 골라, 스냅샷을 효율적으로 쿼리할 수 있습니다. writers 가 테이블 수정 시에는 아래의 2개의 페이즈를 수행합니다.\n",
    "\n",
    "* a. 새로운 파일들 혹은 갱신된 이미 존재하는 것들의 복제본에 대해 낙관적인 쓰기를 수행합니다\n",
    "* b. 그런다음 커밋 후, 로그에 새로운 엔트리르 추가함으로써 테이블의 원자적(atomic) 최신 버전을 생성합니다. 이 로그 엔트리에는 테이블에 대한 다른 메타데이터의 변경 사항과 함께 논리적으로 추가 및 제거할 데이터 파일이 기록됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Other Use Cases\n",
    "\n",
    "#### Time Travel\n",
    "\n",
    "> 모든 테이블은 많고 작음의 차이는 있더라도, `Delta Lake 트랜잭션 로그의 저장된 커밋들의 집합체`라고 말할 수 있습니다. *트랜잭션 로그는 순차적 명령어 가이드*라고 말할 수 있는데, 최초의 상태에서 현재의 상태에 이르기 까지의 과정을 설명한다고 말할 수 있습니다. 그래서 우리는 테이블의 어던 특정 시점에서부터 시작하더라도 상태를 재구성할 수 있으며, **time travel** 혹은 *data versioning* 이라고 알려진 강력한 능력을 가지게 됩니다. \n",
    "\n",
    "> 추가적인 이해는 [Introducing Delta Time Travel for Large Scale Data Lakes](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html) 자료와 [Query an older snapshot of a table (time travel)](https://docs.databricks.com/delta/delta-batch.html?_ga=2.97316378.466340956.1612291545-1203283123.1598416965#query-an-older-snapshot-of-a-table-time-travel) 을 통해 학습할 수 있습니다.\n",
    "\n",
    "\n",
    "#### Data Lineage and Debugging\n",
    "\n",
    "> 모든 변화를 유지하는 트랜잭션 로그의 특징 덕분에 Delta Lake 트랜잭션 로그를 통해 `governance, 감사(audit) 그리고 규정준수(compliance purpose)를 위한 검증 가능한 data lineage`를 제공받을 수 있습니다. 즉, 의도하지 않은 변경의 원인을 추적하거나, 파이프라인의 버그를 찾을 때에 유용할 것입니다. `DESCRIBE HISTORY` 명령을 이용하여 변경된 메타데이터 정보를 통해 확인할 수 있습니다\n",
    "\n",
    "> 추가적인 명령어는 [Table utility commands](https://docs.delta.io/latest/delta-utility.html#history) 페이지에서 찾아볼 수 있습니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. Diving further into the transaction log\n",
    "\n",
    "> 이번 섹션에서는 Delta Lake 트랜잭션 로그가 어떻게 동작하는지에 대해 학습하였습니다.\n",
    "\n",
    "* 트랜잭션 로그가 무엇인지, 어떻게 구성되어 있는지, 커밋이 디스크에 파일로 저장되는 방식.\n",
    "* Delta Lake가 원자성 원칙을 구현할 수 있도록 트랜잭션 로그가 단일 정보 소스 역할을 하는 방법.\n",
    "* Delta Lake가 각 테이블의 상태를 계산하는 방법 - 트랜잭션 로그를 사용하여 가장 최근의 체크포인트를 따라잡는 방법을 포함합니다.\n",
    "* 낙관적 동시성 제어를 사용하여 테이블이 변경되더라도 여러 동시 읽기 및 쓰기를 허용합니다.\n",
    "* Delta Lake가 커밋이 올바르게 직렬화되도록 하기 위해 상호 배제를 사용하는 방법과 충돌 발생 시 자동으로 재시도하는 방법.\n",
    "\n",
    "> 추가로 참고할 만한 세션과 영상입니다 \n",
    "\n",
    "* Unpacking the Transaction Log\n",
    "  - [Unpacking the Transaction Log I](https://databricks.com/discover/diving-into-delta-lake-talks/unpacking-transaction-log)\n",
    "  - [Unpacking the Transaction Log II](https://databricks.com/session_eu20/diving-into-delta-lake-unpacking-the-transaction-log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[맨 위로](#CHAPTER-1.-Basic-Operations-on-Delta-Lakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Table Utilities\n",
    "\n",
    "### 5-1. Review table history\n",
    "\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>foo</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>hello</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>say</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>say_hello</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+---------+-----------+\n",
       "|database|tableName|isTemporary|\n",
       "+--------+---------+-----------+\n",
       "| default|      foo|      false|\n",
       "| default|    hello|      false|\n",
       "| default|      say|      false|\n",
       "| default|say_hello|      false|\n",
       "+--------+---------+-----------+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(1, 10)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"foo\")\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>id</td><td>bigint</td><td></td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Partitioning</td><td></td><td></td></tr>\n",
       "<tr><td>Not partitioned</td><td></td><td></td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------------+---------+-------+\n",
       "|       col_name|data_type|comment|\n",
       "+---------------+---------+-------+\n",
       "|             id|   bigint|       |\n",
       "|               |         |       |\n",
       "| # Partitioning|         |       |\n",
       "|Not partitioned|         |       |\n",
       "+---------------+---------+-------+"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"describe foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr>\n",
       "<tr><td>0</td><td>2021-08-14 23:38:46.66</td><td>null</td><td>null</td><td>WRITE</td><td>[mode -&gt; Overwrite, partitionBy -&gt; []]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>false</td><td>[numFiles -&gt; 3, numOutputBytes -&gt; 1426, numOutputRows -&gt; 9]</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|version|             timestamp|userId|userName|operation|                   operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|\n",
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|      0|2021-08-14 23:38:46.66|  null|    null|    WRITE|[mode -> Overwrite, partitionBy -> []]|null|    null|     null|       null|          null|        false|[numFiles -> 3, numOutputBytes -> 1426, numOutputRows -> 9]|        null|\n",
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"tmp/foo\")\n",
    "spark.sql(f\"describe history delta.`{work_dir}/tmp/foo`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, f\"{work_dir}/tmp/foo\")\n",
    "fullHistroyDF = deltaTable.history()\n",
    "last5OperationsDF = deltaTable.history(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr>\n",
       "<tr><td>0</td><td>2021-08-14 23:38:46.66</td><td>null</td><td>null</td><td>WRITE</td><td>[mode -&gt; Overwrite, partitionBy -&gt; []]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>false</td><td>[numFiles -&gt; 3, numOutputBytes -&gt; 1426, numOutputRows -&gt; 9]</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|version|             timestamp|userId|userName|operation|                   operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|\n",
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|      0|2021-08-14 23:38:46.66|  null|    null|    WRITE|[mode -> Overwrite, partitionBy -> []]|null|    null|     null|       null|          null|        false|[numFiles -> 3, numOutputBytes -> 1426, numOutputRows -> 9]|        null|\n",
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(last5OperationsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. Vacuum History\n",
    "\n",
    "> 더 이상 필요하지 않은 과거 이력에 대해서 `VACUUM` 명령을 통해서 삭제할 수 있습니다. 즉, 갱신 혹은 삭제를 통해 더 이상 참조하지 않는 파일들이 존재할 것이며, 이에 대한 정리작업이 필요할 수 있습니다. \n",
    "\n",
    "> Delta Table 에 의해 더 이상 참조되지 않으며, Rentention threshold 값 보다 오래된 파일들에 대해서 `VACUUM` 명령어를 통해 삭제할 수 있습니다. 주의할 사항은 VACUUM 명령어는 자동으로 트리거링 되지 않으며, 트리거링 되었을 때에 기본 Retention 은 7일입니다. \n",
    "\n",
    "> 예를 들어, 7일 이상 지난 레퍼런스된 파일에 대해서는 더 이상 삭제되지 않는다는 의미입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+\n",
       "|path|\n",
       "+----+\n",
       "+----+"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"vacuum foo dry run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "<tr><td>file:/home/jovyan/work/lgde-spark-delta/spark-warehouse/foo</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------------------------------------------+\n",
       "|                                                       path|\n",
       "+-----------------------------------------------------------+\n",
       "|file:/home/jovyan/work/lgde-spark-delta/spark-warehouse/foo|\n",
       "+-----------------------------------------------------------+"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"vacuum delta.'/home/jovyan/work/tmp/foo'\")\n",
    "# spark.sql(\"vacuum foo\")\n",
    "# spark.sql(\"vacuum foo retain 100 hours\")\n",
    "spark.sql(\"VACUUM foo RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr>\n",
       "<tr><td>0</td><td>2021-08-14 23:38:16.531</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>[isManaged -&gt; true, description -&gt;, partitionBy -&gt; [], properties -&gt; {}]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>false</td><td>[numFiles -&gt; 3, numOutputBytes -&gt; 1426, numOutputRows -&gt; 9]</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------------+------+--------+---------------------------------+------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|version|              timestamp|userId|userName|                        operation|                                                     operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|\n",
       "+-------+-----------------------+------+--------+---------------------------------+------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|      0|2021-08-14 23:38:16.531|  null|    null|CREATE OR REPLACE TABLE AS SELECT|[isManaged -> true, description ->, partitionBy -> [], properties -> {}]|null|    null|     null|       null|          null|        false|[numFiles -> 3, numOutputBytes -> 1426, numOutputRows -> 9]|        null|\n",
       "+-------+-----------------------+------+--------+---------------------------------+------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"describe history foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, f\"{work_dir}/tmp/foo\")\n",
    "deltaTable = DeltaTable.forName(spark, \"foo\")\n",
    "\n",
    "# 아래와 같이 Delta Table 은 데이터프레임과 달라 저장할 수 없기 때문에 toDF 메소드로 변환이 필요하지만, 스키마가 달라 오류가 발생한다\n",
    "# deltaTable.toDF().write.format(\"delta\").mode(\"overwrite\").save(\"/home/jovyan/work/tmp/copy_of_say_hello\")\n",
    "\n",
    "# 스키마가 다른 경우 '.option(\"overwriteSchema\", \"true\")' 을 통해 overwrite 할 수 있다\n",
    "deltaTable.toDF().write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{work_dir}/tmp/copy_of_say_hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr>\n",
       "<tr><td>0</td><td>2021-08-14 23:41:19.4</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>[isManaged -&gt; true, description -&gt;, partitionBy -&gt; [], properties -&gt; {&quot;overwriteSchema&quot;:&quot;true&quot;}]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>false</td><td>[numFiles -&gt; 3, numOutputBytes -&gt; 1426, numOutputRows -&gt; 9]</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+---------------------+------+--------+---------------------------------+------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|version|            timestamp|userId|userName|                        operation|                                                                             operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|\n",
       "+-------+---------------------+------+--------+---------------------------------+------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|      0|2021-08-14 23:41:19.4|  null|    null|CREATE OR REPLACE TABLE AS SELECT|[isManaged -> true, description ->, partitionBy -> [], properties -> {\"overwriteSchema\":\"true\"}]|null|    null|     null|       null|          null|        false|[numFiles -> 3, numOutputBytes -> 1426, numOutputRows -> 9]|        null|\n",
       "+-------+---------------------+------+--------+---------------------------------+------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_of_say_hello = spark.read.format(\"delta\").load(\"tmp/copy_of_say_hello\")\n",
    "copy_of_say_hello.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"copy_of_say_hello\")\n",
    "spark.sql(\"describe history copy_of_say_hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-da7a9f2867fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 7일 이하로 vacuum 시도 시에 아래와 같은 오류 메시지를 뿌리고, duration check 를 false 로 해야 수행된다고 한다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"set spark.databricks.delta.retentionDurationCheck.enabled = true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"vacuum delta.`{work_dir}/tmp/foo` retain 10 hours\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       "
     ]
    }
   ],
   "source": [
    "# 7일 이하로 vacuum 시도 시에 아래와 같은 오류 메시지를 뿌리고, duration check 를 false 로 해야 수행된다고 한다\n",
    "spark.sql(\"set spark.databricks.delta.retentionDurationCheck.enabled = true\")\n",
    "spark.sql(f\"vacuum delta.`{work_dir}/tmp/foo` retain 10 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "<tr><td>file:/home/jovyan/work/lgde-spark-delta/tmp/foo</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------------------------------+\n",
       "|                                           path|\n",
       "+-----------------------------------------------+\n",
       "|file:/home/jovyan/work/lgde-spark-delta/tmp/foo|\n",
       "+-----------------------------------------------+"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시간 단위가 아니라 분 단위로도 설정이 가능하고, 기존에 3회 반복적으로 write 한 이력을 확인할 수 있었다\n",
    "# 6분 전이 아닌 과거의 파케이 파일들은 삭제가 되었으나, `_delta_log` 에는 이력이 존재하긴 한다\n",
    "spark.sql(\"set spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "spark.sql(\"set spark.databricks.delta.vacuum.parallelDelete.enabled = true\")\n",
    "spark.sql(f\"vacuum delta.`{work_dir}/tmp/foo` retain 0.1 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr>\n",
       "<tr><td>0</td><td>2021-08-14 23:38:46.66</td><td>null</td><td>null</td><td>WRITE</td><td>[mode -&gt; Overwrite, partitionBy -&gt; []]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>false</td><td>[numFiles -&gt; 3, numOutputBytes -&gt; 1426, numOutputRows -&gt; 9]</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|version|             timestamp|userId|userName|operation|                   operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|\n",
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|      0|2021-08-14 23:38:46.66|  null|    null|    WRITE|[mode -> Overwrite, partitionBy -> []]|null|    null|     null|       null|          null|        false|[numFiles -> 3, numOutputBytes -> 1426, numOutputRows -> 9]|        null|\n",
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"describe history delta.`{work_dir}/tmp/foo`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th></tr>\n",
       "<tr><td>4</td></tr>\n",
       "<tr><td>5</td></tr>\n",
       "<tr><td>6</td></tr>\n",
       "<tr><td>7</td></tr>\n",
       "<tr><td>8</td></tr>\n",
       "<tr><td>9</td></tr>\n",
       "<tr><td>1</td></tr>\n",
       "<tr><td>2</td></tr>\n",
       "<tr><td>3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+\n",
       "| id|\n",
       "+---+\n",
       "|  4|\n",
       "|  5|\n",
       "|  6|\n",
       "|  7|\n",
       "|  8|\n",
       "|  9|\n",
       "|  1|\n",
       "|  2|\n",
       "|  3|\n",
       "+---+"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 마지막 상태인 파일들로만 구성된 파케이 파일로 조회해도 결과는 동일하다\n",
    "spark.sql(f\"select * from delta.`{work_dir}/tmp/foo`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr>\n",
       "<tr><td>0</td><td>2021-08-14 23:38:46.66</td><td>null</td><td>null</td><td>WRITE</td><td>[mode -&gt; Overwrite, partitionBy -&gt; []]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>false</td><td>[numFiles -&gt; 3, numOutputBytes -&gt; 1426, numOutputRows -&gt; 9]</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|version|             timestamp|userId|userName|operation|                   operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|\n",
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+\n",
       "|      0|2021-08-14 23:38:46.66|  null|    null|    WRITE|[mode -> Overwrite, partitionBy -> []]|null|    null|     null|       null|          null|        false|[numFiles -> 3, numOutputBytes -> 1426, numOutputRows -> 9]|        null|\n",
       "+-------+----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과거 특정시점으로 이동이 가능한지 확인해보았다\n",
    "spark.sql(f\"describe history delta.`{work_dir}/tmp/foo`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th></tr>\n",
       "<tr><td>4</td></tr>\n",
       "<tr><td>5</td></tr>\n",
       "<tr><td>6</td></tr>\n",
       "<tr><td>7</td></tr>\n",
       "<tr><td>8</td></tr>\n",
       "<tr><td>9</td></tr>\n",
       "<tr><td>1</td></tr>\n",
       "<tr><td>2</td></tr>\n",
       "<tr><td>3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+\n",
       "| id|\n",
       "+---+\n",
       "|  4|\n",
       "|  5|\n",
       "|  6|\n",
       "|  7|\n",
       "|  8|\n",
       "|  9|\n",
       "|  1|\n",
       "|  2|\n",
       "|  3|\n",
       "+---+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이미 삭제된 FileNotFoundException 파일이라고 나오면서 오류가 발생\n",
    "v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(f\"{work_dir}/tmp/foo\")\n",
    "display(v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot time travel Delta table to version 2. Available versions: [0, 0].;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d8f245bb0b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 현재 존재하는 버전을 가져올 때에는 정상적으로 조회가 된다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"versionAsOf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{work_dir}/tmp/foo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot time travel Delta table to version 2. Available versions: [0, 0].;"
     ]
    }
   ],
   "source": [
    "# 현재 존재하는 버전을 가져올 때에는 정상적으로 조회가 된다\n",
    "v2 = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(f\"{work_dir}/tmp/foo\")\n",
    "display(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Log and Data History\n",
    "> [spark.databricks.delta Configuration](https://books.japila.pl/delta-lake-internals/DeltaSQLConf/) 페이지를 참고합니다. \n",
    "\n",
    "##### Log history. : Delta Transaction Log 가 지정된 리텐션 기간을 넘어서는 커밋이 발생하는 경우 과거의 트랜잭션 로그는 삭제됩니다.\n",
    "* `spark.databricks.delta.logRetentionDuration` : default = 30 days\n",
    "* 이는 Delta Log 가 무한정 증가하는 것을 막기 위해 고안된 사항이며 기본값은 30일입니다\n",
    "  - 물리적인 파일과는 다르게 **트랜잭션 로그는 자동적으로 삭제** 됩니다.\n",
    "\n",
    "##### Data history. : \n",
    "* `spark.databricks.delta.deletedFileRetentionDuration` : defatul = 7 days\n",
    "* 필요 없게된 데이터 파일을 삭제하기 위한 기능이며 기본값은 7일입니다\n",
    "  - 트랜잭션 로그와는 다르게 **물리적인 데이터 파일은 삭제되지 않으**므로 `VACUUM` 명령으로 스케줄링 되어야만 합니다\n",
    "\n",
    "##### Parallel deletion of files during vacuum.\n",
    "* `spark.databricks.delta.vacuum.parallelDelete.enabled` : default = false\n",
    "* `VACUUM` 명령 수행 시에 병렬로 데이터 파일을 삭제할 수 있으며 기본값은 false 입니다\n",
    "  - 파일 삭제 시에 셔플파티션 수가 많은 경우 병렬로 설정이 가능하며 세션 설정에서 변경할 수 있습니다\n",
    "\n",
    "##### Preventing very short retetion period vacuum.\n",
    "* `spark.databricks.delta.retentionDurationCheck.enabled` : default = true\n",
    "* 리텐션 인터벌을 7일 이하로 지정하는 것을 권장하지 않으며 기본 설정은 true 입니다. \n",
    "  - **대상 테이블에 concurrent readers 혹은 writers 들이 오래된 스냅샷 혹은 커밋되지 않은 파일에 존재**할 수 있기 때문입니다.\n",
    "* 반드시 가장 길다고 판단되는 동시성 트랜잭션 보다 길게 인터벌을 지정해야 의도하지 않은 데이터파일 삭제를 막을 수 있습니다.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. Retrieve Delta table details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4. Generate a manifest file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-5. Convert a Parquet table to a Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-6. Convert a Delta table to a Parquet table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-7. Restore a table version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[맨 위로](#CHAPTER-1.-Basic-Operations-on-Delta-Lakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Table w/ Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(5, 10)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional update without overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  1|\n",
      "|  3|\n",
      "|  7|\n",
      "|  0|\n",
      "|  4|\n",
      "|  9|\n",
      "| 18|\n",
      "| 19|\n",
      "| 11|\n",
      "|  2|\n",
      "| 15|\n",
      "| 16|\n",
      "| 13|\n",
      "| 14|\n",
      "| 10|\n",
      "| 12|\n",
      "|  6|\n",
      "|  8|\n",
      "| 17|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"tmp/delta-table\")\n",
    "\n",
    "# Update every even value by adding 100 to it\n",
    "deltaTable.update(\n",
    "  condition = expr(\"id % 2 == 0\"),\n",
    "  set = { \"id\": expr(\"id + 100\") })\n",
    "\n",
    "# Delete every even value\n",
    "deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
    "\n",
    "# Upsert (merge) new data\n",
    "newData = spark.range(0, 20)\n",
    "\n",
    "deltaTable.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    newData.alias(\"newData\"),\n",
    "    \"oldData.id = newData.id\") \\\n",
    "  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n",
    "  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n",
    "  .execute()\n",
    "\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().sort(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read older versions of data using time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a stream of data to a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDf = spark.readStream.format(\"rate\").load()\n",
    "stream = streamingDf.selectExpr(\"value as id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"tmp/checkpoint\").start(\"tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a stream of changes from a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table schema is not set.  Write data into it or use CREATE TABLE to set the schema.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50/3784503046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstream2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/delta-table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 raise ValueError(\"If the path is provided for stream, it needs to be a \" +\n\u001b[1;32m    479\u001b[0m                                  \"non-empty string. List of paths are not supported.\")\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table schema is not set.  Write data into it or use CREATE TABLE to set the schema."
     ]
    }
   ],
   "source": [
    "stream2 = spark.readStream.format(\"delta\").load(\"/tmp/delta-table\").writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Delta using Spark SQL Magic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select 1 as col1, 1L as col2\")\n",
    "df.printSchema()\n",
    "df2 = df.withColumn(\"col3\", expr(\"case when col2 is not null then 10 else col2 end\"))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a,b\".find(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select 'a,b' as col1\")\n",
    "def splitColumnA(col, sep):\n",
    "    return split(col, sep)[0]\n",
    "def splitColumnB(col, sep):\n",
    "    return split(col, sep)[1]\n",
    "df2 = df.withColumn(\"col2\", splitColumnA(expr(\"col1\"), \",\")).withColumn(\"col3\", splitColumnB(expr(\"col1\"), \",\"))\n",
    "df3 = df2.na.fill({\"col3\": \"default_value\"})\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "\n",
    "CREATE TABLE events (\n",
    "  date DATE,\n",
    "  eventId STRING,\n",
    "  eventType STRING,\n",
    "  data STRING)\n",
    "USING DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "CREATE TABLE foo\n",
    "USING DELTA\n",
    "LOCATION 'tmp/delta-table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from foo order by id asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고 사이트\n",
    "* [Data Engineer Training](https://github.com/psyoblade/data-engineer-training)\n",
    "* [SparkSQL Magic](https://github.com/cryeo/sparksql-magic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
