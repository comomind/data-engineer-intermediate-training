{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 스트리밍 실습 4교시 : 외부 인터페이스\n",
    "\n",
    "> 파일, 카프카 등의 외부 소스로부터 데이터를 읽고, 카산드라 혹은 MySQL 등의 외부 저장소에 저장하는 실습을 합니다\n",
    "\n",
    "## 학습 목표\n",
    "* `File`에 저장된 데이터를 처리합니다\n",
    "  - 로컬 JSON 파일을 직접 생성 및 수정을 통해 스트리밍 데이터를 생성합니다\n",
    "* `Kafka`에 저장된 스트리밍 데이터를 처리합니다\n",
    "  - 카프카 메시지 프로듀서를 통해서 카프카에 스트림 데이터를 생성합니다\n",
    "* `Cassandra`에 스트림 데이터를 저장합니다\n",
    "  - 로컬 JSON 파일을 읽어서 처리하는 애플리케이션을 구현합니다\n",
    "  - 직접 연동이 어렵기 때문에 foreachBatch 메소드를 통해 카산드라에 저장합니다\n",
    "* `MySQL`에 스트림 데이터를 저장합니다\n",
    "  - 로컬 JSON 파일을 읽어서 처리하는 애플리케이션을 구현합니다\n",
    "  - 레코드 단위의 트랜잭션 지원을 위해 foreach 메소드를 통해 MySQL에 저장합니다\n",
    "\n",
    "## 목차\n",
    "* [1. File 소스 스트리밍 실습](#1.-File-소스-스트리밍-실습)\n",
    "* [2. Kafka 소스 스트리밍 실습](#2.-Kafka-소스-스트리밍-실습)\n",
    "* [3. Cassandra 싱크 스트리밍 실습](#3.-Cassandra-싱크-스트리밍-실습)\n",
    "* [4. MySQL 싱크 스트리밍 실습](#4.-MySQL-싱크-스트리밍-실습)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://db01eb5cb84f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9ecd7aac70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "\n",
    "# 현재 기동된 스파크 애플리케이션의 포트를 확인하기 위해 스파크 정보를 출력합니다\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트림 테이블을 주기적으로 조회하는 함수 (name: 이름, sql: Spark SQL, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStream(name, sql, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)              # 출력 Cell 을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Query: '+sql)\n",
    "        display(spark.sql(sql))              # Spark SQL 을 수행합니다\n",
    "        sleep(sleep_secs)                    # sleep_secs 초 만큼 대기합니다\n",
    "        i += 1\n",
    "\n",
    "# 스트림 쿼리의 상태를 주기적으로 조회하는 함수 (name: 이름, query: Streaming Query, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStatus(name, query, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)      # Output Cell 의 내용을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Status: '+query.status['message'])\n",
    "        display(query.lastProgress)  # 마지막 수행된 쿼리의 상태를 출력합니다\n",
    "        sleep(sleep_secs)            # 지정된 시간(초)을 대기합니다\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File 소스 스트리밍 실습\n",
    "\n",
    "> 기본으로 제공되는 stream 싱크와 소스를 이용하여 데이터 프레임 생성이 가능하며 SparkSession.readStream(), DataFrame.writeStream() 을 이용합니다\n",
    "\n",
    "* 파일로부터 읽기\n",
    "  - 기본적으로 텍스트 파일(CSV, TSV 등)의 경우 스키마가 없는 경우가 많기 때문에 스키마를 직접 생성해 줍니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td></tr>\n",
       "<tr><td>3</td><td>엘지에너지솔루션</td></tr>\n",
       "<tr><td>100</td><td>엘지</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+----------------+\n",
       "|emp_id|        emp_name|\n",
       "+------+----------------+\n",
       "|     1|        엘지전자|\n",
       "|     2|        엘지화학|\n",
       "|     3|엘지에너지솔루션|\n",
       "|   100|            엘지|\n",
       "+------+----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filePath = f\"{work_data}/json\"\n",
    "fileJson = spark.read.json(filePath)\n",
    "fileJson.printSchema() # 기본적으로 스키마를 추론하는 경우 integer 가 아니라 long 으로 잡는다\n",
    "display(fileJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[fileSource] Iteration: 3, Query: select * from fileSource'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td></tr>\n",
       "<tr><td>3</td><td>엘지에너지솔루션</td></tr>\n",
       "<tr><td>100</td><td>엘지</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+----------------+\n",
       "|emp_id|        emp_name|\n",
       "+------+----------------+\n",
       "|     1|        엘지전자|\n",
       "|     2|        엘지화학|\n",
       "|     3|엘지에너지솔루션|\n",
       "|   100|            엘지|\n",
       "+------+----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filePath = f\"{work_data}/json\"\n",
    "queryName = \"fileSource\"\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "fileSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", LongType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    ")\n",
    "fileReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(fileSchema)\n",
    "    .load(filePath)\n",
    ")\n",
    "fileSelector = fileReader.select(\"emp_id\", \"emp_name\")\n",
    "fileWriter = (\n",
    "    fileSelector\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"update\")\n",
    ")\n",
    "fileTrigger = (\n",
    "    fileWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "fileQuery = fileTrigger.start()\n",
    "displayStream(\"fileSource\", f\"select * from {queryName}\", 3, 3)\n",
    "fileQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kafka 소스 스트리밍 실습\n",
    "\n",
    "![table.8-1](images/table.8-1.png)\n",
    "\n",
    "#### 1. 카프카에 데이터 쓰기 - producer.py\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import names\n",
    "\n",
    "# 카프카에 데이터를 전송하는 코드\n",
    "def produce(port):\n",
    "    try:\n",
    "        hostname=\"kafka:%d\" % port\n",
    "        producer = KafkaProducer(bootstrap_servers=[hostname],\n",
    "            value_serializer=lambda x: dumps(x).encode('utf-8')\n",
    "        )\n",
    "        data = {}\n",
    "        for seq in range(9999):\n",
    "            print(\"Sequence\", seq)\n",
    "            first_name = names.get_first_name()\n",
    "            last_name = names.get_last_name()\n",
    "            data[\"first\"] = first_name\n",
    "            data[\"last\"] = last_name\n",
    "            producer.send('events', value=data)\n",
    "            sleep(0.5)\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc(sys.stdout)\n",
    "\n",
    "# 카프카의 데이터 수신을 위한 내부 포트는 9093\n",
    "if __name__ == \"__main__\":\n",
    "    port = 9093\n",
    "    if len(sys.argv) == 2:\n",
    "        port = int(sys.argv[1])\n",
    "    produce(port)\n",
    "```\n",
    "#### 2. 터미널에 접속하여 카프카 메시지를 생성합니다\"\n",
    "```bash\n",
    "(base) # python producer.py\n",
    "```\n",
    "\n",
    "#### 3. 스파크를 통해 생성된 스트리밍 데이터를 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[kafkaSource] Iteration: 10, Query: select first, count from kafkaSource order by count desc limit 5'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>first</th><th>count</th></tr>\n",
       "<tr><td>Martin</td><td>2</td></tr>\n",
       "<tr><td>Michael</td><td>2</td></tr>\n",
       "<tr><td>Elizabeth</td><td>2</td></tr>\n",
       "<tr><td>Raymond</td><td>1</td></tr>\n",
       "<tr><td>Shana</td><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+-----+\n",
       "|    first|count|\n",
       "+---------+-----+\n",
       "|   Martin|    2|\n",
       "|  Michael|    2|\n",
       "|Elizabeth|    2|\n",
       "|  Raymond|    1|\n",
       "|    Shana|    1|\n",
       "+---------+-----+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 컨테이너 내부에서 토픽에 접근하기 위한 포트는 9093\n",
    "kafkaReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "    .option(\"subscribe\", \"events\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "kafkaSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"first\", StringType()))\n",
    "    .add(StructField(\"last\", StringType()))\n",
    ")\n",
    "kafkaSelector = (\n",
    "    kafkaReader\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), kafkaSchema).alias(\"name\")\n",
    "    )\n",
    "    .selectExpr(\"name.first as first\", \"name.last as last\")\n",
    "    .groupBy(\"first\")\n",
    "    .count().alias(\"count\")\n",
    ")\n",
    "\n",
    "queryName = \"kafkaSource\"\n",
    "kafkaWriter = (\n",
    "    kafkaSelector\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "kafkaTrigger = (\n",
    "    kafkaWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "kafkaQuery = kafkaTrigger.start()\n",
    "displayStream(\"kafkaSource\", f\"select first, count from {queryName} order by count desc limit 5\", 10, 3)\n",
    "kafkaQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaQuery.status\n",
    "kafkaQuery.stop()\n",
    "# 카프카 메시지를 특정 토픽에서 완전히 삭제 하고 다시 적재하려면 어떻게 해야 하는가?\n",
    "# 9092 와 9093 포트의 차이점은 무엇인가?\n",
    "# pip3 install kafka-python\n",
    "# pip3 install names\n",
    "# producer.py 파일 추가\n",
    "# kafka monitoring tools 추가하여 확인하면 좋을 듯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cassandra 싱크 스트리밍 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5-3. 커스텀 스트리밍 소스와 싱크 - Custom Streaming Sources and Sinks\n",
    "\n",
    "> 빌트인으로 지원하지 않는 저장소 엔진(예: Cassandra 등)의 경우 foreachBatch() 혹은 foreach() 메소드를 통해 커스텀 로직을 추가할 수 있습니다.\n",
    "\n",
    "\n",
    "#### 모든 스토리지 시스템에 쓰기 - Writing to any storage system\n",
    "\n",
    "> 현재 '스파크 스트리밍'에는 카산드라에 스트리밍 데이터프레임을 저장할 수 있는 방법은 없기 때문에, Batch DataFrame 방식으로 저장합니다\n",
    "\n",
    "#### 가. foreachBatch : 임의의 작업들 혹은 매 '마이크로 배치'의 결과에 대해 커스텀 로직을 적용할 수 있는 배치 메소드\n",
    "  - foreachBatch(arg1: DataFrame or Dataset, arg2: Unique Identifier)\n",
    "\n",
    "* 로컬 파일을 카산드라 데이터베이스에 저장합니다\n",
    "  - 로컬에 저장된 JSON 파일을 읽어서 스트리밍 처리를 합니다\n",
    "  - 초기 test_keyspace 와 test_table 에 저정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"data/json\"\n",
    "checkpointDir = \"tmp/cassandra-stream\"\n",
    "!rm -rf $checkpointDir\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    ")\n",
    "df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(schema)\n",
    "    .load(source)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostAddr = \"cassandra\"\n",
    "keyspaceName = \"test_keyspace\"\n",
    "tableName = \"test_table\"\n",
    "spark.conf.set(\"spark.cassandra.connection.host\", hostAddr)\n",
    "\n",
    "def writeCountsToCassandra(updatedCountsDF, batchId):\n",
    "    # Use Cassandra batch data source to write the updated counts\n",
    "    (\n",
    "        updatedCountsDF\n",
    "        .write\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .mode(\"append\")\n",
    "        .options(table=tableName, keyspace=keyspaceName)\n",
    "        .save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.select(\"emp_id\", \"emp_name\")\n",
    "streamingQuery = (\n",
    "    summary\n",
    "    .writeStream\n",
    "    .foreachBatch(writeCountsToCassandra)\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", checkpointDir)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 카산드라에 접속하여, 데이터를 조회합니다\n",
    "```bash\n",
    "$> docker compose exec cassandra cqlsh\n",
    "cqlsh> use test_keyspace;\n",
    "cqlsh:test_keyspace> select emp_id, emp_name from test_table;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### foreachBatch() 로 아래의 동작들이 가능합니다\n",
    "\n",
    "* 이전에 존재하는 배치 데이터 소스들을 재사용 하기 - *Reuse existing batch data sources* \n",
    "  - 이전 예제와 같이 배치에서 사용하는 데이터 소스들을 그대로 사용할 수 있습니다\n",
    "\n",
    "* 다수의 경로에 저장하기 - *Write to multiple locations*\n",
    "  - OLAP 및 OLTP 데이터베이스 등에 다양한 위치로 저장할 수 있습니다\n",
    "  - 출력 데이터에 대해 데이터프레임 캐시를 통해 재계산을 회피할 수 있습니다\n",
    "\n",
    "```python\n",
    "# In Python\n",
    "def writeCountsToMultipleLocations(updatedCountsDF, batchId):\n",
    "    updatedCountsDF.persist()\n",
    "    updatedCountsDF.write.format(...).save() # Location 1\n",
    "    updatedCountsDF.write.format(...).save() # Location 2\n",
    "    updatedCountsDF.unpersist()\n",
    "}\n",
    "```\n",
    "\n",
    "* 추가적인 데이터프레임 연산 하기 - *Apply additional DataFrame operations*\n",
    "  - Incremental Plan 생성이 어렵기 때문에 '스트리밍 데이터프레임'에 대한 연산 지원이 되지 않습니다 - **아래**\n",
    "  - '마이크로 배치' 출력에 대하여 foreachBatch 메소드를 통해 작업 수행이 가능합니다.\n",
    "  - **at-least-once** 지원만 가능하며, **exactly-once** 는 후처리 dedup 작업을 통해 가능합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### '스파크 스트리밍'에 지원되지 않는 연산 - Unsupported Operations\n",
    "\n",
    "> 스트리밍 '데이터프레임/데이터셋' 에서는 몇 가지 지원하지 않는 연산이 있습니다. 실행 시에는 ***DataFrame/Datasets 에서 지원하지 않는 연산이라는 AnalysisException*** 을 반환합니다. 생각해보면 끝없이 수신되는 데이터에 대한 정렬을 하는 것은 모호한 동작이며, 수신된 데이터를 모두 모니터링 해야 하므로, 비효율적인 연산일 가능성이 높습니다.\n",
    "\n",
    "| 연산자 | 설명 |\n",
    "| --- | --- |\n",
    "| 다수 스트리밍 집계 연산 | 여러개의 스트리밍 데이터프레임의 집계는 현재 지원하지 않습니다 |\n",
    "| limit or take | 스트리밍 데이터셋은 일부 로우를 가져오는 연산은 지원하지 않습니다 |\n",
    "| distinct | 스트리밍 데이터셋은 유일 데이터 연산은 지원하지 않습니다 |\n",
    "| sort | 집계 연산 이후의 Complete 출력모드에서만 정렬을 사용할 수 있습니다 |\n",
    "| outer join | 스트림 사이드에 대해서만 left inner, outer 조인을 지원 |\n",
    "| count | 단일 빈도수를 반환하지 않기 때문에, 실행 횟수를 반환하는 ds.groupBy().count() 를 사용하세요 |\n",
    "| foreach | 대신 ds.writeStream.foreach(...) 를 사용하세요 (다음 챕터에서) |\n",
    "| show | 콘솔 sink 를 사용하세요 (다음 챕터에서) |\n",
    "| cube, rollup | 연산 비용 및 실용성 면에서 지원하지 않는 연산 UnsupportedOperationException |\n",
    "\n",
    "* [support-matrix-for-joins-in-streaming-queries](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#support-matrix-for-joins-in-streaming-queries)\n",
    "  - 양쪽이 모두 스트림인 경우는 워터마크를 사용해야만 합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MySQL 싱크 스트리밍 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 나. foreach : 커스텀 저장 로직을 매 로우마다 적용하는 메소드 (고정된 map 함수 같은 방식)\n",
    "> 배치저장을 위한 인터페이스가 제공되지 않는 경우 직접 데이터를 저장하는 로직을 구현해야 하며, open, process, close 단계에 해당하는 구현을 해야만 합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"data/json\"\n",
    "checkpointDir = \"tmp/foreach-stream\"\n",
    "!rm -rf $checkpointDir\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    ")\n",
    "df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(schema)\n",
    "    .load(source)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 화면에 데이터를 그대로 출력하는 예제\n",
    "def process_row(row):\n",
    "    print(\"%d, %s\" % (row[0], row[1]))\n",
    "query = df.writeStream.foreach(process_row).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees table already exists.\n",
      "employees table deleted.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "cnx = mysql.connector.connect(user='sqoop', password='sqoop', host='mysql', database='testdb')\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "def createEmployee(cursor):\n",
    "    try:\n",
    "        employee = \"CREATE TABLE employees ( emp_id int, emp_name varchar(30) )\"\n",
    "        cursor.execute(employee)\n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == errorcode.ER_TABLE_EXISTS_ERROR:\n",
    "            return \"employees table already exists.\"\n",
    "        else:\n",
    "            return err.msg\n",
    "    else:\n",
    "        return \"employees table created.\"\n",
    "\n",
    "print(createEmployee(cursor))\n",
    "\n",
    "def deleteEmployee(delete_employee):\n",
    "    try:\n",
    "        cursor.execute(delete_employee)\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"failed to delete employees\"\n",
    "    return \"employees table deleted.\"\n",
    "\n",
    "delete_employee = \"DELETE FROM employees\"\n",
    "print(deleteEmployee(delete_employee))\n",
    "\n",
    "def insertEmployee(add_employee, data_employee):\n",
    "    emp_no = -1\n",
    "    try:\n",
    "        cursor.execute(add_employee, data_employee)\n",
    "        emp_no = cursor.lastrowid\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return emp_no\n",
    "    return emp_no\n",
    "\n",
    "add_employee = \"INSERT INTO employees ( emp_id, emp_name ) VALUES ( %s, %s )\"\n",
    "data_employee = ( 1, '박수혁' )\n",
    "print(insertEmployee(add_employee, data_employee))\n",
    "\n",
    "cnx.commit()\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 저장된 데이터를 확인합니다\n",
    "```bash\n",
    "$ docker compose exec mysql mysql -usqoop -psqoop\n",
    "mysql> use testdb;\n",
    "mysql> select emp_id, emp_name from employees;\n",
    "+--------+--------------------------+\n",
    "| emp_id | emp_name                 |\n",
    "+--------+--------------------------+\n",
    "|      1 | 엘지전자                   |\n",
    "|      2 | 엘지화학                   |\n",
    "|      3 | 엘지에너지솔루션             |\n",
    "|    100 | 엘지                      |\n",
    "+--------+--------------------------+\n",
    "```\n",
    "\n",
    "#### ForeachWriter class 를 생성하고 외부 데이터베이스에 저장하는 예제\n",
    "> 아래의 경우는 foreach 메소드를 사용하는 예제를 설명하기 위함이며, 실무에서는 ***절대 사용해서는 안 되는 예제*** 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f97701dbe80>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "class ForeachWriter:    \n",
    "    \n",
    "    def createEmployee(self, cursor):\n",
    "        try:\n",
    "            employee = \"CREATE TABLE employees ( emp_id int, emp_name varchar(30) )\"\n",
    "            cursor.execute(employee)\n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == errorcode.ER_TABLE_EXISTS_ERROR:\n",
    "                return \"employees table already exists.\"\n",
    "            else:\n",
    "                return err.msg\n",
    "        else:\n",
    "            return \"employees table created.\"\n",
    "        \n",
    "    def deleteEmployee(self, cursor):\n",
    "        try:\n",
    "            delete_employee = \"DELETE FROM employees\"\n",
    "            cursor.execute(delete_employee)\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return \"failed to delete employees\"\n",
    "        return \"employees table deleted.\"\n",
    "        \n",
    "    def insertEmployee(self, cursor, data_employee):\n",
    "        emp_no = -1\n",
    "        try:\n",
    "            add_employee = \"INSERT INTO employees ( emp_id, emp_name ) VALUES ( %s, %s )\"\n",
    "            cursor.execute(add_employee, data_employee)\n",
    "            emp_no = cursor.lastrowid\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return emp_no\n",
    "        return emp_no\n",
    "    \n",
    "    def insertDocument(self, data_employee):\n",
    "        emp_id = -1\n",
    "        try:\n",
    "            cnx = mysql.connector.connect(user='sqoop', password='sqoop', host='mysql', database='testdb')\n",
    "            cursor = cnx.cursor()\n",
    "            emp_id = self.insertEmployee(cursor, data_employee)\n",
    "            cnx.commit()\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            cnx.close()\n",
    "        return emp_id\n",
    "    \n",
    "    def selectEmployee(self, cursor):\n",
    "        emp_no = -1\n",
    "        try:\n",
    "            get_employees = \"SELECT emp_id, emp_name FROM employees\"\n",
    "            cursor.execute(get_employees)\n",
    "            for (emp_id, emp_name) in cursor:\n",
    "                print(\"{} in '{}' has retrieved.\".format(emp_id, emp_name))\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return emp_no\n",
    "        return emp_no\n",
    "    \n",
    "    # 데이터베이스 커넥션, 테이블 생성 및 테이블 데이터 삭제\n",
    "    def open(self, partitionId, epochId):\n",
    "        try:\n",
    "            cnx = mysql.connector.connect(user='sqoop', password='sqoop', host='mysql', database='testdb')\n",
    "            cursor = cnx.cursor()\n",
    "            print(self.createEmployee(cursor))\n",
    "            print(self.deleteEmployee(cursor))\n",
    "            cnx.commit()\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            cnx.close()\n",
    "        return True\n",
    "    \n",
    "    def process(self, row):\n",
    "        data_employee = (row[0], row[1])\n",
    "        result = self.insertDocument(data_employee)\n",
    "        if (result >= 0):\n",
    "            print(\"[%s] '%s' is inserted\" % data_employee)\n",
    "    \n",
    "    def close(self, error):\n",
    "        try:\n",
    "            cnx = mysql.connector.connect(user='sqoop', password='sqoop', host='mysql', database='testdb')\n",
    "            cursor = cnx.cursor()\n",
    "            self.selectEmployee(cursor)\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            cnx.close()\n",
    "\n",
    "df.writeStream.foreach(ForeachWriter()).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 변환 - Data Transformations\n",
    "\n",
    "> '구조화된 스트리밍' 에서는 점진적으로 수행 가능한 데이터프레임 연산만 지원하며, 이를 크게 '스테이트리스', '스테이트풀' 연산이라 합니다\n",
    "\n",
    "### 6-1. 증분 실행과 스트리밍 처리상태 - Incremental Execution and Streaming State\n",
    "\n",
    "> 한 번에 모든 '논리적 계획'을 '물리적계획'으로 변환하는 배치 처리와 다르게, 연속적인 실행의 계획을 생성하는 역할을 수행합니다. 매 실행은 '마이크로배치' 작업을 말하며, **작업과 작업 간의 중간 결과물**을 스트리밍 **'상태'**라고 말합니다.\n",
    "\n",
    "* '스테이트리스' 데이터 변환 - Stateless Transformations\n",
    "  - 모든 프로젝션 연산(select, explode, map, flatMap) 들과 셀렉트 연산(filter, where) 들은 이전 로우의 상태와 무관한 '스테이트리스' 연산자들입니다\n",
    "  - 이러한 **'스테이트리스' 연산자 들만 append, update 출력 모드를 지원**합니다.\n",
    "    - '중간결과물인 상태'가 없다는 의미는 현재 로우에만 영향 및 의존성을 가진다는 말이므로, append 모드로 동작 하더라도 중복문제가 생기지 않습니다\n",
    "  - **반면에 complete 모드를 지원하지 않습**니다,\n",
    "    - 이는 매 새로운 데이터를 반복적으로 처리하는 것은 비용적으로 크기 때문입니다.\n",
    "\n",
    "* '스테이트풀' 데이터 변환 - Stateful Transformations\n",
    "  - DataFrame.groupBy().count() 스트리밍의 시작부터 발생하는 레코드의 수를 나타내는데, 이전 '마이크로배치'의 빈도수를 저장하고 있어야 하며 이를 '상태'라고 부릅니다\n",
    "  - 상태저장이 어떻게 동작하는 지 테스트 하는데, 파일 스트림을 통한 연속적인 테스트는 어렵다는 사실을 알 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointDir = \"tmp/stateful-stream\"\n",
    "!rm -rf $checkpointDir\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    ")\n",
    "source = \"data/tmp\"\n",
    "reader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(schema)\n",
    "    .load(source)\n",
    ")\n",
    "groupby = reader.groupBy().count()\n",
    "writer = (\n",
    "    groupby\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", checkpointDir)\n",
    "    .start()\n",
    ")\n",
    "writer.awaitTermination(120)\n",
    "writer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 해서 이번에는 소켓을 통해 지속적으로 넣어주면서 groupBy count 를 살펴봅니다\n",
    "```bash\n",
    "$ nc -lvp 9999\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    ")\n",
    "groupby = reader.groupBy().count()\n",
    "writer = (\n",
    "    groupby\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "writer.awaitTermination(120)\n",
    "writer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분산 내결함성 상태 관리 - Distributed and fault-tolerant state management\n",
    "* 드라이버에서 기동되는 스파크 스케줄러는 애플리케이션을 잡으로 잡을 다수의 타스크로 나누고, 이러한 타스크들을 큐에 집어 넣습니다\n",
    "* 클러스터에 여유 리소스가 존재하는 경우 익스큐터는 기동되어 타스크가 저장된 큐로부터 작업을 가져와서 수행합니다\n",
    "* 스트리밍 쿼리의 매 '마이크로배치'는 주어진 집합의 데이터를 '스트리밍 소스'들로 부터 가져와서, 처리한 후, '스트리밍 싱크'에 저장합니다\n",
    "* 이 과정에서 생성되는 '중간 상태 데이터'들은 다음 '마이크로배치' 작업에서 소비되게 됩니다\n",
    "\n",
    "![figure.8-6](images/figure.8-6.png)\n",
    "\n",
    "* 집계를 위해 익스큐터 내에서 셔플링이 일어나고, 익스큐터 내의 메모리에서 캐싱이 일어납니다\n",
    "* 해당 단어는 항상 동일한 익스큐터 내에서 수행되므로, 로컬 읽기와 갱신이 이루어지게 됩니다 (파티셔닝 개념)\n",
    "* 상태저장을 위한 메모리가 부족하거나, 익스큐터가 예기치 못한 상황에 종료되는 경우에 체크포인트 위치에 상태의 체인지로그를 키값 쌍으로 저장합니다\n",
    "  - 만에 하나 실패하는 경우에도 해당 체크포인트의 체인지로그를 통해서 동일한 상태로 다시 수행하므로써 복구 가능합니다.\n",
    "  - 이러한 과정을 통해서 **end-to-end exactly-once 를 보장**합니다\n",
    "\n",
    "\n",
    "#### 스테이트풀 연산자의 유형 - Types of stateful operations\n",
    "\n",
    "* 매니지드 스테이트풀 연산자 - Managed stateful operations\n",
    "  - 구체적으로 \"오래된\" 것으로 정의된 연산을 기반으로, 자동으로 오래된 상태를 구별하고 정리합니다\n",
    "    - Streaming aggregations\n",
    "    - Stream-stream joins\n",
    "    - Streaming deduplication\n",
    "\n",
    "* 언매니지드 스테이트풀 연산자 - Unmanaged stateful operations\n",
    "  - 이용자 스스로 정리 로직을 구현하는 '세션처리'와 같은 임의의 상태저장을 말합니다\n",
    "    - MapGroupsWithState\n",
    "    - FlatMapGroupsWithState\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 상태 저장 스트리밍 집계 - Stateful Streaming Aggregations\n",
    "\n",
    "### 7-1. 시간에 기반하지 않는 집계 - Aggregations Not Based on Time\n",
    "\n",
    "#### Global aggregations - 스트림 데이터 전역적인 집계함수\n",
    "* 스트리밍의 경우 **데이터프레임에 직접 빈도 함수를 호출할 수 없습**니다 (bounded vs. unbounded)\n",
    "```python\n",
    "runningCount = sensorReadings.groupBy().count()\n",
    "```\n",
    "\n",
    "#### Grouped aggregations - 특징 그룹필드에 대한 집계함수\n",
    "```python\n",
    "baselineValues = sensorReadings.groupBy(\"sensorId\").mean(\"value\")\n",
    "```\n",
    "\n",
    "#### [All built-in aggregation functions](https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions)\n",
    "  - sum, mean, stddev, countDistinct, collect_set, approx_count_distinct 등\n",
    "\n",
    "#### Multiple aggregations computed together - 다수의 집계 연산\n",
    "```python\n",
    "multipleAggs = (\n",
    "    sensorReadings\n",
    "    .groupBy(\"sensorId\")\n",
    "    .agg(\n",
    "        count(\"*\")\n",
    "        , mean(\"value\").alias(\"baselineValue\")\n",
    "        , collect_set(\"errorCode\").alias(\"allErrorCodes\")\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "#### [User-defined aggregation functions](https://spark.apache.org/docs/latest/sql-ref-functions-udf-aggregate.html#untyped-user-defined-aggregate-functions)\n",
    "* 이용자 정의 집계함수는 Jar 형태로 배포 및 등록 되어야 하므로, Scala, Java 언어만 지원하며, 등록된 UDF, UDAF 등을 SQL에서 사용할 수 있습니다\n",
    "![ch8-udf](images/ch8-udf.png)\n",
    "\n",
    "#### 시간을 기준으로 하지 않는 집계의 경우 유의해야 할 2가지\n",
    "* ***1. Output Mode*** : 워터마크를 사용할 수 없으므로 출력 모드가 제한적이다\n",
    "* ***2. Planning the resource usage by state*** : 워터마크 사용이 불가하므로 특정 상태에 과도한 메모리 사용이 발생할 수 있다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7-2. 이벤트타임 윈도우 기반 집계 - Aggregations Not Based on Time\n",
    "\n",
    "> 수신되는 전체 데이터의 집계 대신 '타임윈도우'에 근거한 집계를 하려고 합니다. 예를 들어 비정상적으로 많은 빈도를 나타내는 센서를 인식하는 등의 임의의 단위시간 내의 빈도를 측정할 수 있습니다\n",
    "\n",
    "* 애플리케이션의 강건성을 확보하기 위하여, 센서 데이터가 단위 시간 당 생성되는 빈도 혹은 양을 기준으로 인터벌을 정해야만 합니다. \n",
    "* 실제 사건이 발생한 시간을 나타내는 *event time* 기준으로 데이터의 타임 윈도우를 측정합니다\n",
    "  - 아래의 예제에서는 eventTime 컬럼을 기준으로 5분 단위 윈도우를 구성하여 계산합니다\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "(\n",
    "    sensorReadings\n",
    "    .groupBy(\"sensorId\", window(\"eventTime\", \"5 minute\"))\n",
    "    .count()\n",
    ")\n",
    "```\n",
    "* 데이터가 발생한 시간기준으로 단위시간 5분 내 이벤트를\n",
    "* sensorId 키를 기준으로 복합 그룹을 통해 계산하며\n",
    "* 복합 그룹 내의 빈도를 갱신합니다\n",
    "\n",
    "\n",
    "#### 5분 텀플링 윈도우 (nonoverlapping)\n",
    "\n",
    "![figure.8-7](images/figure.8-7.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointDir = \"checkpoint/tumbling-stream\"\n",
    "!rm -rf $checkpointDir\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    "    .add(StructField(\"timestamp\", TimestampType()))\n",
    "    .add(StructField(\"time\", StringType()))\n",
    ")\n",
    "source = \"data/stream/tumbling\"\n",
    "reader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(schema)\n",
    "    .load(source)\n",
    ")\n",
    "groupby = reader.groupBy(\"emp_id\", window(\"timestamp\", \"5 minute\")).count()\n",
    "writer = (\n",
    "    groupby\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", checkpointDir)\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "writer.awaitTermination(10)\n",
    "writer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 텀블링 윈도우 데이터 생성 및 결과 확인을 위해 예제 데이터를 생성하고 파일스트리밍 결과를 확인합니다. 집계함수의 complete 모드이지만 정렬은 되지 않았으나, 정해진 타임슬롯에 전체 데이터를 출력할 수 있었습니다\n",
    "\n",
    "```bash\n",
    "notebook  | -------------------------------------------\n",
    "notebook  | Batch: 0\n",
    "notebook  | -------------------------------------------\n",
    "notebook  | +------+------------------------------------------+-----+\n",
    "notebook  | |emp_id|window                                    |count|\n",
    "notebook  | +------+------------------------------------------+-----+\n",
    "notebook  | |1     |[2016-03-20 15:05:00, 2016-03-20 15:10:00]|1    |\n",
    "notebook  | |4     |[2016-03-20 12:50:00, 2016-03-20 12:55:00]|2    |\n",
    "notebook  | |2     |[2016-03-20 12:35:00, 2016-03-20 12:40:00]|1    |\n",
    "notebook  | |3     |[2016-03-20 11:45:00, 2016-03-20 11:50:00]|1    |\n",
    "notebook  | |1     |[2016-03-20 12:20:00, 2016-03-20 12:25:00]|3    |\n",
    "notebook  | +------+------------------------------------------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![figure.8-8](images/figure.8-8.png)\n",
    "\n",
    "> 이전 윈도우와 오버랩되는 구간이 발생하는 슬라이딩 윈도우 (윈도우는 5분이고, 윈도우 간에 5분씩 오버랩)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointDir = \"checkpoint/sliding-stream\"\n",
    "!rm -rf $checkpointDir\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    "    .add(StructField(\"timestamp\", TimestampType()))\n",
    "    .add(StructField(\"time\", StringType()))\n",
    ")\n",
    "source = \"data/stream/sliding\"\n",
    "reader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(schema)\n",
    "    .load(source)\n",
    ")\n",
    "groupby = reader.groupBy(\"emp_id\", window(\"timestamp\", \"10 minute\", \"5 minute\")).count()\n",
    "writer = (\n",
    "    groupby\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", checkpointDir)\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "writer.awaitTermination(10)\n",
    "writer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 슬라이딩 윈도우의 경우 5분 오버랩 윈도우를 통해 겹치는 구간이 존재하도록 출력됩니다 \n",
    "\n",
    "```bash\n",
    "notebook  | -------------------------------------------\n",
    "notebook  | Batch: 0\n",
    "notebook  | -------------------------------------------\n",
    "notebook  | +------+------------------------------------------+-----+\n",
    "notebook  | |emp_id|window                                    |count|\n",
    "notebook  | +------+------------------------------------------+-----+\n",
    "notebook  | |3     |[2016-03-20 11:40:00, 2016-03-20 11:50:00]|1    |\n",
    "notebook  | |1     |[2016-03-20 12:20:00, 2016-03-20 12:30:00]|3    |\n",
    "notebook  | |1     |[2016-03-20 12:15:00, 2016-03-20 12:25:00]|3    |\n",
    "notebook  | |4     |[2016-03-20 12:45:00, 2016-03-20 12:55:00]|2    |\n",
    "notebook  | |4     |[2016-03-20 12:50:00, 2016-03-20 13:00:00]|2    |\n",
    "notebook  | |3     |[2016-03-20 11:45:00, 2016-03-20 11:55:00]|1    |\n",
    "notebook  | |1     |[2016-03-20 15:05:00, 2016-03-20 15:15:00]|1    |\n",
    "notebook  | |2     |[2016-03-20 12:30:00, 2016-03-20 12:40:00]|1    |\n",
    "notebook  | |1     |[2016-03-20 15:00:00, 2016-03-20 15:10:00]|1    |\n",
    "notebook  | |2     |[2016-03-20 12:35:00, 2016-03-20 12:45:00]|1    |\n",
    "notebook  | +------+------------------------------------------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이와 같이 누적으로 모든 데이터를 담고 있다면 리소스 사용 관점에서 지속적으로 늘어나는 상태를 관리하기 어려울 것이므로, 최신 데이터만 유지하되, 어느정도 까지 데이터 입력지연을 감내할 것인지를 정의해야 합니다. 이러한 쿼리의 바운드의 범위를 ***watermarks*** 라고 부릅니다.\n",
    "\n",
    "#### Handling late data with watermarks\n",
    "> ***watermark***는 처리된 데이터 내의 쿼리에 의해 발견된 **maximum event time** 이후에 발생하는 **이벤트 타임의 임계치 값**이라고 말할 수 있습니다.\n",
    "\n",
    "* 주어진 그룹에 더 이상 데이터가 도달하지 않는다는 것을 아는 시점을 말하며, 엔진에 의해서 자동적으로 해당 집계가 완료됨을 인지할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![figure.8-9](images/figure.8-9.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointDir = \"checkpoint/watermark-stream\"\n",
    "!rm -rf $checkpointDir\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    "    .add(StructField(\"timestamp\", TimestampType()))\n",
    "    .add(StructField(\"time\", StringType()))\n",
    ")\n",
    "source = \"data/stream/watermark\"\n",
    "reader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(schema)\n",
    "    .load(source)\n",
    ")\n",
    "groupby = (\n",
    "    reader\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\")\n",
    "    .groupBy(\"emp_id\", window(\"timestamp\", \"30 minute\", \"30 minute\"))\n",
    "    .count()\n",
    ")\n",
    "writer = (\n",
    "    groupby\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", checkpointDir)\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "writer.awaitTermination(10)\n",
    "writer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 파일스트림의 경우 하나의 파일내의 순서와 무관하게 병렬처리하게 되어 한 번에 동시에 온 것처럼 인식되어 워터마크와 딜레이 영향을 받지 않는 것 처럼 보인다\n",
    "\n",
    "* 예를 들어 센서 데이터는 아무리 늦어도 10분 이상 지연되지 않는다는 것을 안다면, **워터마크를 10분으로** 지정합니다\n",
    "  - 반드시 groupBy 집계 이전에 withWatermark 선언이 되어야만 합니다.\n",
    "  - 쿼리가 수행될 때에 '구조화된 스트리밍'은 ***이벤트 타임***의 최대 값을 계속해서 관찰합니다\n",
    "  - 워터마크를 갱신함에 따라서 ***너무 늦은*** 데이터를 필터할 수 있게 됩니다\n",
    "  - 그리고나서 '오래된 상태'를 정리하게 됩니다. 여기서 ***10분 이상 지연된 어떠한 데이터도 무시***된다는 의미입니다\n",
    "  - 이벤트 타임 기준으로 가장 늦게 발생한 입력 데이터 보다 ***10분 이상 오래된 모든 타임 윈도우는 상태에서 정리***되게 됩니다\n",
    "\n",
    "```python\n",
    "df = (\n",
    "    sensorReadings\n",
    "    .withWatermark(\"eventTime\", \"10 minutes\") # 최대 10분까지 지연 로그를 대기한다는 의미\n",
    "    .groupBy(\"sensorId\", window(\"eventTime\", \"10 minutes\", \"5 minutes\")) # window(windowLength: '타임 윈도우 크기', slideInterval: '오버랩 시간')\n",
    "    .mean(\"value\")\n",
    ")\n",
    "```\n",
    "\n",
    "#### 윈도우 그룹 빈도수에 대한 워터마킹 동작 방식\n",
    "\n",
    "* 매 5분 단위로 트리거링이 발생하며, 집계연산이 수행됩니다\n",
    "* X축은 Processing Time 즉, Scheduler 를 통해 트리거링이 발생하는 '프로세싱 타임'을 말하며, Y축은 실제 사건이 발생한 '이벤트 타임'을 가리킵니다\n",
    "* 맨 처음 발생하는 12:05 분에는 어떠한 데이터도 수신되지 않았기 때문에 '쿼리 테이블'에는 아무런 데이터도 없습니다\n",
    "* 12:10 트리거링 시에는 2건의 데이터가 수신되었고, '이벤트 타임'이 각 각 \"12:07,id1 12:08,id2\" 으로 사건이 발생한 시간과 수신된 시간이 거의 동일합니다\n",
    "  - X, Y 축의 위치가 일치한 점으로 알 수 있으며 기울기가 1인 위치에 있는 데이터는 지연이 거의 없다고 말할 수 있습니다\n",
    "* 12:10 트리거링 시에 윈도우가 10분이고, 슬라이딩이 5분이기 때문에 2개의 '타임 슬라이드' 그룹에 대한 계산이 이루어져야 합니다\n",
    "  - \"12:00 ~ 12:10\" 과 \"12:05 ~ 12:15\" 2개가 그것입니다\n",
    "* 12:15 트리거링 시에는 지연된 데이터 \"12:09,id3\"가 도착했고, 워터마크 내에 존재하여 '쿼리 테이블'에 반영되어집니다\n",
    "  - 워터마크 계산은 '프로세싱 타임' 기준으로 가장 최신에 수신된 '이벤트 타임' - '12:14,id2' 기준으로 10분 과거 시간까지 허용합니다\n",
    "  - 즉, watermark = 12:14 - 10mins = 12:04 분이전 데이터는 지연된 데이터로 버리고, 그 이후 데이터는 워터마크 내의 데이터입니다\n",
    "* 12:10 트리거링 시에는 총 4개의 데이터가 수신되었고, 2개는 정상, 2개는 지연된 데이터가 존재합니다만, 버리는 경우는 없습니다\n",
    "  - 지연을 결정짓는 가장 큰 기준은 절대적인 시간이 아니며, ***수신된 데이터의 maximum(eventTime) 시간을 기준으로 계산***합니다\n",
    "* 현재 '프로세싱 타임'이 아무리 흘러도, 수신되는 '이벤트 타임' 시간이 천천히 흘러간다면 지연이 아니게 됩니다 \n",
    "  - Y 축으로 늘어나는 시간은 '이벤트 타임' 기준으로 거리가 일정하고 정확하지만, 상대적으로 X 축은 그렇지 못 합니다\n",
    "  - 일례로 \"12:07 ~ 12:08\" 과 \"12:14 ~ 12:15\" 의 Y 축의 거리는 일정하고 동일하지만, X 축의 거리는 전자가 다소 짧습니다 (상대적인 워터마크 계산임을 알아야 합니다)\n",
    "* '스파크 스트리밍'은 그림에서의 ***파랑색 점선*** 을 항상 모니터링 하면서 '워터마크'를 계산합니다\n",
    "  - 항상 가장 최신의 '이벤트 타임'을 저장하고 있다가, 트리거링 되는 시점에 watermark 시간을 계산하여 너무 지연된 데이터를 필터링 하게 됩니다\n",
    "  - 결국, **'쿼리 테이블'도 '이벤트 타임'기준으로 슬라이딩 하면서 갱신**한다고 보면 됩니다\n",
    "  \n",
    "  \n",
    "> ***워터마킹이 적용된 스트리밍 처리는 가장 최신 이벤트 타임 기준으로 워터마크 시간(그림은 10분)만큼 지연된 데이터까지 처리하는 것을 보장합니다*** \n",
    "최대 지연 시간의 데이터는 확실히 보장하지만, 누락되지 않는 방향은 확실히 보장하지만, 반면에 ***지연이 되었다고 해서 반드시 누락되는 것을 보장하지는 않습니다.***\n",
    "이러한 누락되도 되는 데이터를 집계하는 것은 *레코드가 수신된 시점과, 마이크로 배치 처리가 트리거링된 시점*에 기인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![figure.8-10](images/figure.8-10.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "checkpointDir = \"checkpoint/kafka-watermark-stream\"\n",
    "!rm -rf $checkpointDir\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", IntegerType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    "    .add(StructField(\"timestamp\", TimestampType()))\n",
    "    .add(StructField(\"time\", StringType()))\n",
    ")\n",
    "# 컨테이너 내부에서 토픽에 접근하기 위한 포트는 9093\n",
    "reader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "    .option(\"subscribe\", \"events\")\n",
    "    .option(\"startingOffsets\", \"latest\") # earliest\n",
    "    .load()\n",
    ")\n",
    "# counter = reader.selectExpr(\"CAST(value AS STRING)\")\n",
    "# counter = reader.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"json\"))\n",
    "\n",
    "gropuby = (\n",
    "    reader\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"x\"))\n",
    "    .selectExpr(\"x.emp_id as emp_id\", \"x.timestamp as timestamp\")\n",
    "    .withWatermark(\"timestamp\", \"25 minutes\")\n",
    "    .groupBy(\"emp_id\", window(\"timestamp\", \"25 minute\", \"25 minute\"))\n",
    "    .count()\n",
    ")\n",
    "query = (\n",
    "    gropuby\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")\n",
    "query.awaitTermination(30)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지원하는 출력 모드 - Supported output modes\n",
    "\n",
    "> '시간을 포함하지 않은 스트리밍 집계'와는 다르게 '이벤트 타임을 포함한 스트리밍 집계'는 3가지 모드를 모두 지원합니다\n",
    "\n",
    "* Update mode\n",
    "  - 매 '마이크로 배치'집계 결과에서 변경된 사항만 출력으로 생성되고, 워터마킹은 주기적으로 '상태'를 갱신합니다\n",
    "  - 하지만, ***Parquet, ORC 와 같은 파일 기반의 싱크의 경우는 '업데이트 모드'를 사용할 수 없***습니다 (단, Delta Lake 는 제외)\n",
    "  - \"수시로 변경 사항만을 지속적으로 갱신하는 경우\"\n",
    "\n",
    "* Complete mode\n",
    "  - 시간 혹은 변화에 무관하게 모든 출력을 항상 만들어내고 갱신합니다\n",
    "  - 이 모드는 ***워터마크를 명시하였다고 하더라도 '상태'는 클린업 되지 않으며, 모든 과거의 '상태'를 유지***합니다.\n",
    "  - 이러한 이유로 무한히 상태가 증가하여 메모리 사용에 항상 주의해야만 합니다\n",
    "\n",
    "* Append mode\n",
    "  - ***event-time window 와 watermarking 을 사용했을 때***에만 이용 가능한 모드이며, 이전 결과(상태)에 대해 수정이 불가합니다\n",
    "  - *워터마크를 명시하지 않은 집계*의 경우는 미래의 데이터로 사용되므로, append 모드에서 *출력이 불가*능합니다\n",
    "  - 즉, *워터마크가 명시되어야만, 해당 집계 그룹 내의 데이터가 더 이상 업데이트 되지 않음을 알 수* 있습니다\n",
    "  - append 모드의 경우 '키'와 '집계결과'가 더 이상 과거의 데이터를 갱신하지 않는다는 것을 보장하는 경우에만 *최종 집계 결과를 출력*합니다\n",
    "  - 이러한 이유로, ***파일과 같은 싱크에 append-only** 출력이 가능하지만, 반면에 워터마크 시간 만큼 ***실시간 처리가 지연***되게 됩니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 스트리밍 조인 - Streaming Joins\n",
    "\n",
    "> 스트리밍 데이터 집합간의 조인에 대해 학습합니다. 어떤 조인(이너, 아우터)이 지원되며, 스테이트풀 조인을 위해 저장되는 '상태'를 워터마크를 이용하여 제한하는 방법에 대해 배웁니다.\n",
    "\n",
    "### 8-1. 스트림 정적 테이블 조인 - Stream–Static Joins\n",
    "\n",
    "> 데이터 로그에 모든 정보를 다 담을 수 없기 때문에, 로그의 타입과 그 정보를 가진 맵핑 테이블을 통해 집계 정보를 통해 집계 정보를 보여줄 수 있습니다. 광고 클릭 로그(stream)와 광고 당 발생하는 노출정보(static)를 이용하여 실시간 수익을 계산할 수 있습니다. 즉, 자주 변경되지 않으며 사전에 계획된 광고 노출 데이터(static)와의 조인이 필요한 경우에 사용하면 상대적으로 성능과 스트리밍의 효과를 누릴 수 있습니다\n",
    "\n",
    "```python\n",
    "impressionStatic = spark.read.parquet(\"/static/impression/dt=20210707\")\n",
    "clickStream = spark.readStream.format(\"kafka\").load()\n",
    "# 광고 구분자를 조인 키로 조인을 수행합니다 - stream inner-join static\n",
    "matched = clickStream.join(impressionStatic, \"adId\")\n",
    "# 혹은 stream left-outer-join 혹은 right-outer-join stream 만 지원합니다\n",
    "matchedOuter = clickStream.join(impressionStatic, \"adId\", \"leftOuter\")\n",
    "```\n",
    "\n",
    "#### 스트림 조인에서 유의해야 할 사항\n",
    "* 스트림-정적 조인의 경우 스테이트리스 연산이므로, '워터마킹'을 필요로 하지 않습니다\n",
    "  - 워터마크는 스트림 처리의 중간 결과 상태 데이터를 말합니다\n",
    "* 정적 데이터의 경우 반복적으로 읽게 되므로 캐싱하여 성능을 높일 수 있습니다\n",
    "* 정적 데이터가 변경되는 경우 소스의 종류에 따라 반영되지 않을 수 있습니다\n",
    "  - 파일 소스에 데이터가 추가(append)되는 경우 스트리밍 쿼리가 재시작 되기 전까지는 반영되기 어렵습니다\n",
    "\n",
    "\n",
    "### 8-2. 스트림 스트림 테이블 조인 - Stream–Stream Joins\n",
    "\n",
    "> 스트림과 스트림 데이터집합의 조인의 경우 양쪽 스트림 모두 지연될 수 있기 때문에 완전하지 않은 데이터 소스에 대한 조인이 일어날 수 있으므로 주의해서 다루어야 합니다. 이번에는 click 뿐만 아니라 impression 까지도 스트림 데이터 소스로 조인하는 예제를 학습합니다\n",
    "\n",
    "![figure.8-11](images/figure.8-11.png)\n",
    "\n",
    "\n",
    "#### 스트림 간의 조인의 특징\n",
    "\n",
    "* 엔진은 소스가 스트림 - 스트림 임을 인지하고, impression-and-click 정보의 상태를 버퍼링합니다\n",
    "* 버퍼링 과정에서 서로 매칭되는 데이터를 통해 조인 연산이 수행됩니다. 코드와 시각화한 화면은 아래와 같습니다.\n",
    "\n",
    "```python\n",
    "impressions = spark.readStream.format(\"kafka\").option(\"...\").load()\n",
    "clicks = spark.readStream.format(\"kafka\").option(\"...\").load()\n",
    "matched = clicks.join(impressions, \"adId\")\n",
    "```\n",
    "\n",
    "![figure.8-12](images/figure.8-12.png)\n",
    "\n",
    "> 그림에서의 파란색 점이 impression 과 click 의 '이벤트 타임'을 나타내고 있으며, '마이크로 배치'를 넘나드는 매칭되는 데이터 집합을 통해 조인연산이 일어나게 됩니다.\n",
    "\n",
    "#### 스트리밍 간의 조인의 가정\n",
    "\n",
    "* 이러한 '이벤트 타임' 기준 시간으로 조인이 이루어지기 때문에 두 이벤트는 같은 시간 \"same wall clock time\" 으로 저장되어야 문제가 없습니다\n",
    "* 위의 이벤트 처리에서는 대기 시간을 지정하지 않았기 때문에 조인되지 않은 데이터의 '스트리밍 상태'는 계속 대기하게 됩니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 스트리밍 간의 조인 처리 시에 아래와 같은 가정을 해보겠습니다\n",
    "\n",
    "* 광고노출 이후 클릭 사이에 소요되는 최대 시간은 얼마인가?\n",
    "  - 최대 0초(바로 클릭)에서 최대 한 시간까지 대기했다가 클릭이 발생할 수 있다고 가정해 보았습니다\n",
    "* 광고의 노출 및 클릭 정보가 네트워크 장애 등의 이슈로 얼마나 지연되어 수신될 수 있는가?\n",
    "  - 노출 및 클릭이 각 각 2시간, 3시간까지 지연 될 수 있다고 가정해 보았습니다\n",
    "\n",
    "> 위에서 얘기한 '이벤트 타임' 제약이나 지연 등의 조건은 워터마크와 시간 범위 조건을 이용하여 데이터프레임 연산에 적용할 수 있습니다. \n",
    "\n",
    "* 1. 두 가지 '데이터 소스'는 얼마나 지연될 수 있는지 '워터마크'를 통해 정의합니다\n",
    "* 2. 두 가지 '데이터 소스'간의 '이벤트 타임'에 대한 제약을 정의합니다\n",
    "  - 오래된 입력 로우가 언제 필요 없어지는 지에 대한 시점을 엔진이 인지하게 되는 제약 조건을 말합니다\n",
    "\n",
    "* 예제를 통해서 정의해 보겠습니다\n",
    "  - Time range join conditions : 노출 이후에 클릭이 최대 1시간 정도까지만 발생할 수 있다고 가정했으므로\n",
    "    * \"leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR\"\n",
    "    * 클릭 시간이 노출 시간 + 1시간 까지의 시간에 대해서 조인될 수 있습니다\n",
    "  - Join on event-time windows : \n",
    "    * leftTimeWindow = rightTimeWindow\n",
    "    * 클릭의 윈도우 시간은 3시간, 노출은 2시간으로 두었으므로 그 시간 범위에 대해 조인될 수 있습니다\n",
    "\n",
    "![figure.8-13](images/figure.8-13.png)\n",
    "\n",
    "#### 그림을 통해 2가지 스트림에 대한 가정을 분석합니다\n",
    "\n",
    "* 1. 노출 데이터를 기준 : 노출은 2시간 지연이고, 클릭은 3시간 지연이므로, 노출 이후 1시간 클릭 가정을 고려하면 최대 4시간 노출 버퍼가 유지되어야 합니다\n",
    "* 2. 클릭 데이터를 기준 : 유사하게, 3시간 짜리 클릭 윈도우는 2시간의 버퍼만 유지되더라도 매칭이 가능합니다\n",
    "\n",
    "```python\n",
    "impressionsWithWatermark = (\n",
    "    impressions\n",
    "    .selectExpr(\"adId AS impressionAdid\", \"impressionTime\")\n",
    "    .withWatermark(\"impressionTime\", \"2 hours\")\n",
    ")\n",
    "clickWithWatermark = (\n",
    "    clicks\n",
    "    .selectExpr(\"adId AS clickAdid\", \"clickTime\")\n",
    "    .withwatermark(\"clickTime\", \"3 hours\")\n",
    ")\n",
    "joined = impressionsWithWatermark.join(\n",
    "    clickWithWatermark,\n",
    "    exp(\"\"\"\n",
    "        clickAdId = impressionAdid AND\n",
    "        clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour\n",
    "        \"\"\")\n",
    ")\n",
    "```\n",
    "\n",
    "#### Inner Join 연산에 대한 기억해 두어야하는 키포인트\n",
    "\n",
    "* Inner Join 의 경우, 워터마킹과, 이벤트 타임 제약 두 가지 모두 선택 사항이므로, 'Unbounded State' 에 대한 위험성을 내포하고 있습니다. 즉, 두 가지 옵션을 지정해 주었을 때에만 더 이상 조인되지 않을 수 있는 '상태'가 정리 대상으로 관리된다는 의미입니다\n",
    "* 워터마킹 집계 연산에서 다루었던 것과 동일하게, 워터마킹에 의해 지정된 시간 범위내에 지연된 로그가 누락되지 않음을 보장하지만, 지연 로그가 처리되지 않는 것은 보장하지는 않습니다.\n",
    "\n",
    "\n",
    "### 워터마킹을 이용한 아우터 조인 - Outer joins with watermarking\n",
    "\n",
    "> 위의 Inner Join 예제의 경우 노출은 되더라도 클릭되지 않은 경우는 전혀 리포팅 되지 않게 됩니다. 그래서 노출은 되지만 클릭되지 않은 경우를 고려하기 위해서 Outer Join 을 사용하게 됩니다\n",
    "\n",
    "```python\n",
    "(\n",
    "    impressionsWithWatermark.join(\n",
    "        clickWithWatermark,\n",
    "        expr(\"\"\"\n",
    "        clickAdId = impressionAdId AND\n",
    "        clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour\n",
    "        \"\"\"),\n",
    "        \"leftOuter\"\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "#### Outer Join 의 특징\n",
    "* 예상대로 매 '마이크로 배치'의 노출에 대한 값에 대해 클릭이 없더라도 출력됩니다\n",
    "  - Inner Join 의 경우, 워터마킹에 대한 정보가 없고, 제약 시간 조건만 있어도 클릭로그를 처리하지 않는 시점을 인터벌 시간이 지난 클릭로그 시간만으로 인지할 수 있으나,\n",
    "  - Outer Join 의 경우, '노출 로그'에 매 '마이크로 배치' 시간 마다 NULL 로 채워진 클릭 로그에 대한 결과를 출력해야만 합니다 (지연일 수도, 클릭을 안 한 것일 수도 있으므로), 만약 명시되지 않는 경우 해당 NULL 값을 채워줄 수 없기 때문에 해당 타임 테이블의 출력 시점을 정할 수 없기 때문에 반드시 '워터마킹'과 '인터벌' 정보에 대한 제약을 반드시 명시되어야만 합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 임의의 상태 저장 연산 - Arbitrary Stateful Computations\n",
    "\n",
    "### 9-1. mapGroupsWithState 함수를 이용한 임의의 상태 저장 연산 모델링 - Modeling Arbitrary Stateful Operations with mapGroupsWithState()\n",
    "\n",
    "### 9-2. 타임아웃을 활용한 비활성 그룹 관리 - Using Timeouts to Manage Inactive Groups\n",
    "\n",
    "### 9-3. flatMapGroupsWithState 함수를 이용한 일반화 - Generalization with flatMapGroupsWithState()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 성능 개선 - Performance Tuning\n",
    "\n",
    "> 배치 작업과 다르게 '마이크로 배치' 잡들은 비교적 작은 볼륨의 데이터를 다루기 때문에, 살짝 다른 튜닝 접근을 하고 있습니다.\n",
    "\n",
    "* 클러스터 리소스 프로비저닝 - Cluster resource provisioning\n",
    "  - 24/7 서비스를 수행하기 위해 적절한 리소스 프로비저닝이 필수적입니다. 다만, 너무 많은 리소스 할당은 낭비를 초래하고, 적으면 작업이 실패하게 됩니다\n",
    "  - ***'스테이트리스' 쿼리들은 코어***가 많이 필요지만, ***'스테이트풀' 쿼리들은 상대적으로 메모리***를 많이 필요하기 때문에 쿼리의 특성에 따라 할당하는 것을 고려합니다\n",
    "\n",
    "* 셔플에 따른 파티션 수 - Number of partitions for shuffles\n",
    "  - 배치 작업 대비 다소 작은 셔플 파티션의 수를 가지는데, 너무 많은 작업으로 구분하는 것에 따른 오버헤드를 증가시키거나, 처리량을 감소시킬 수 있습니다\n",
    "  - 또한 셔플링은 '스테이트풀 오퍼레이션'에 따른 '체크포인팅'의 오버헤드가 커질 수 있다는 점도 유의할 필요가 있습니다\n",
    "  - '스테이트풀' 작업이면서 트리거 간격이 몇 초에서 몇 분인 스트리밍 쿼리의 경우 ***셔플 수를 기본값 인 200에서 할당 된 코어 수의 최대 2~3배로 조정***하는 것이 좋습니다.\n",
    "\n",
    "* 안정화를 위한 소스 비율 리미트 조정 - Setting source rate limits for stability\n",
    "  - 초기에 최적화된 설정으로 잘 운영되더라도, 급격하게 늘어난 데이터에 대한 불안정한 상황이 발생할 수 있는데, 리소스를 많이 투입하는 '오버 프로비저닝' 방법 외에도 '소스 속도 제한'을 통한 불안정성을 극복할 수도 있습니다. \n",
    "  - 카프카 등의 소스의 임계치를 설정함으로써 싱글 '마이크로 배치' 작업에서 너무 많은 데이터를 처리하지 않도록 막는 방법이며, 증가한 데이터는 소스에서 버퍼링 된 상태로 유지되며, 결국에는 데이터 처리가 따라잡게 됩니다\n",
    "  - 다만, 아래의 몇 가지를 기억해야만 합니다\n",
    "    - 리미트를 너무 낮게 설정하면 쿼리가 할당 된 리소스를 충분히 활용하지 못하고 입력 속도보다 느려질 수 있습니다.\n",
    "    - 리미트 설정 만으로는 입력 속도의 지속적인 증가를 효과적으로 해결하지 못하며, 안정성이 유지되는 동안 버퍼링되고 처리되지 않은 데이터의 양은 소스에서 무한히 증가하므로 종단 간 지연 시간도 증가한다는 점을 알아야 합니다\n",
    "\n",
    "* 동일한 스파크 애플리케이션 내에서 다수의 스트리밍 쿼리 - Multiple streaming queries in the same Spark application\n",
    "  - 동일한 SparkContext 또는 SparkSession에서 여러 스트리밍 쿼리를 실행하면 fine-grained 된 리소스 공유가 발생할 수 있습니다.\n",
    "    - 각 쿼리를 실행하면 Spark 드라이버 (즉, 실행중인 JVM)의 리소스가 계속 사용됩니다. 결국, 드라이버가 동시에 실행할 수있는 쿼리 수가 제한되게 됩니다.\n",
    "      - 이러한 제한에 도달하면 작업 예약에 병목 현상이 발생하거나 (즉, 실행 프로그램을 제대로 활용하지 못함) 메모리 제한을 초과 할 수 있습니다.\n",
    "    - 별도의 스케줄러 풀에서 실행되도록 설정하여 동일한 컨텍스트의 쿼리간에보다 공정한 리소스 할당을 보장 할 수 있습니다.\n",
    "      - SparkContext의 스레드 로컬 속성 spark.scheduler.pool을 각 스트림에 대해 다른 문자열 값으로 설정합니다.\n",
    "\n",
    "```python\n",
    "# Run streaming query1 in scheduler pool1\n",
    "spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool1\")\n",
    "df.writeStream.queryName(\"query1\").format(\"parquet\").start(path1)\n",
    "\n",
    "# Run streaming query2 in scheduler pool2\n",
    "spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool2\")\n",
    "df.writeStream.queryName(\"query2\").format(\"parquet\").start(path2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Connection refused (Connection refused)\n=== Streaming Query ===\nIdentifier: [id = cf94b137-8b9d-4198-826b-bfd6909f87df, runId = e8673ef6-a4fb-491d-b478-dc530c53f34f]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {TextSocketV2[host: localhost, port: 9999]: -1}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nSubqueryAlias count\n+- Aggregate [word#17], [word#17, count(1) AS count#21L]\n   +- Project [split(value#15, \\s, -1) AS word#17]\n      +- StreamingDataSourceV2Relation [value#15], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@2cc0d0a4, TextSocketV2[host: localhost, port: 9999]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a7ff4b995994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mstreamingQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Connection refused (Connection refused)\n=== Streaming Query ===\nIdentifier: [id = cf94b137-8b9d-4198-826b-bfd6909f87df, runId = e8673ef6-a4fb-491d-b478-dc530c53f34f]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {TextSocketV2[host: localhost, port: 9999]: -1}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nSubqueryAlias count\n+- Aggregate [word#17], [word#17, count(1) AS count#21L]\n   +- Project [split(value#15, \\s, -1) AS word#17]\n      +- StreamingDataSourceV2Relation [value#15], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@2cc0d0a4, TextSocketV2[host: localhost, port: 9999]\n"
     ]
    }
   ],
   "source": [
    "# 스트리밍 데이터에 대한 디버깅을 위해 사전 정의한 함수\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    print(\"{} - {}\".format(epoch_id, df.collect()))\n",
    "\n",
    "streamingQuery = (\n",
    "    counts\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(processingTime=\"1 second\") # 1 second micro batch interval\n",
    "    .foreachBatch(foreach_batch_function)\n",
    "    .start()\n",
    ")\n",
    "streamingQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624735618\n"
     ]
    }
   ],
   "source": [
    "!rm -rf \"tmp/checkpoint\"\n",
    "\n",
    "import calendar;\n",
    "import time;\n",
    "ts = calendar.timegm(time.gmtime())\n",
    "print(ts)\n",
    "# 1624239264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-53943ba918e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mstreamingQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "checkpointDir = \"tmp/checkpoint\"\n",
    "words = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "streamingQuery = (\n",
    "    counts\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(processingTime=\"1 second\") # 1 second micro batch interval\n",
    "    .option(\"checkpointLocation\", checkpointDir)\n",
    "    .start()\n",
    ")\n",
    "streamingQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-1-a6d1a7399678>:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f6c6cdc9b574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a local StreamingContext with two working thread and batch interval of 1 second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[2]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NetworkWordCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mssc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    337\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-1-a6d1a7399678>:4 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:40\n",
      "-------------------------------------------\n",
      "('test', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:44\n",
      "-------------------------------------------\n",
      "('ok', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:50\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d9a5cad3daa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# Start the computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for the computation to terminate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \"\"\"\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:14:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:15:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:16:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:17:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:18:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:19:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:20:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-11 15:21:35\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* [Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "* [Spark Runtime Configuration Guide](https://spark.apache.org/docs/latest/configuration.html#spark-sql)\n",
    "* 5-2. [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#creating-streaming-dataframes-and-streaming-datasets)\n",
    "* 5-2. [Apache Kafka Client](https://docs.confluent.io/clients-confluent-kafka-python/current/overview.html)\n",
    "  - [Kafka python on github](https://github.com/dpkp/kafka-python/blob/master/example.py)\n",
    "  - [Kafka Producer & Consumer](https://needjarvis.tistory.com/607)\n",
    "* 5-2. [Cassandar Data Manipulation](https://cassandra.apache.org/doc/latest/cql/dml.html)\n",
    "  - [Creating Cassandra keyspace & table on docker start up](https://www.linkedin.com/pulse/creating-cassandra-keyspace-table-docker-start-up-amon-peter/)\n",
    "* 5-2. [MySQL JDBC Connection Example](https://dev.mysql.com/doc/connector-python/en/connector-python-examples.html)\n",
    "* [Linux Kill](https://www.lesstif.com/system-admin/unix-linux-kill-12943674.html)\n",
    "* [Python Current Timestamp](https://timestamp.online/article/how-to-get-current-timestamp-in-python)\n",
    "* [NetCat on Windows](https://mentha2.tistory.com/65)\n",
    "* [NetCat on Ubuntu](https://epicarts.tistory.com/43)\n",
    "* [VisualVM](https://visualvm.github.io/download.html)\n",
    "* [Monitoring spark w/ prometheus](https://argus-sec.com/monitoring-spark-prometheus/)\n",
    "* [Dynamic Docker Monitoriong](https://www.datadoghq.com/dg/monitor/docker-benefits/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 자주 사용되는 용어\n",
    "* Structured API 란?\n",
    "  - 일반적인 함수 혹은 메소드 형식의 API 와 다르게, 데이터프레임을 통하여 다양한 메소드를 연결하여 활용할 수 있는 구조를 가졌기 때문에 구조화된 API 라고 말할 수 있습니다\n",
    "* Map-Reduce 란?\n",
    "  - 병렬처리가 가능한 상태가 없는 단순 계산 연산을 Map, 병렬처리가 불가능한 상태가 존재하는 집계 연산을 Reduce 라고말할 수 있으며, 가장 쉬운 예제로 WordCount 가 있습니다\n",
    "* Idempotence 란?\n",
    "  - 반복적으로 수행 혹은 처리하여도 항상 결과가 동일하게 나오는 것\n",
    "* Structured Streaming vs. DStreams ?\n",
    "  - DStream : RDD 를 이용하여 스트리밍 처리를 직접 구현할 수 있는 Spark 1.x 버전부터 제공 되었던 인터페이스를 말합니다\n",
    "  - Structured Streaming : Spark 2.x 버전부터 제공 되는 Spark SQL 즉, Dataframe (Dataset) 기반의 API 통한 스트리밍 처리 인터페이스를 말합니다\n",
    "* [RDD vs. Dataset vs. Dataframe](https://www.analyticsvidhya.com/blog/2020/11/what-is-the-difference-between-rdds-dataframes-and-datasets/)\n",
    "  - RDD : 최적화 및 데이터 처리에 대한 구현을 직접해야 하는 경우\n",
    "```python\n",
    "sc = SparkContext(\"local\",\"PySpark Word Count Exmaple\")\n",
    "words = sc.textFile(\"tmp/source\").flatMap(lambda line: line.split(\" \"))\n",
    "wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)\n",
    "wordCounts.saveAsTextFile(\"tmp/target\")\n",
    "```\n",
    "  - Dataframe : 제공되는 API 통한 데이터 처리를 수행하는 경우\n",
    "```python\n",
    "df = spark.read.text(\"tmp/source\")\n",
    "wordCounts = df.withColumn(\"word\", explode(split(\"value\", \" \"))).groupBy(\"word\").count()\n",
    "wordCounts.write.format(\"text\").save(\"tmp/target\")\n",
    "```\n",
    "  - Dataset : Typed Compile 언어인 Java, Scala 만 지원하며, Compile 시점에 Type-safe 보장\n",
    "* Event-time vs. Processing-time ?\n",
    "  - Event-time : 실제 사건 혹은 이벤트가 발생한 시간 (ex_ 핸드폰에서 광고를 클릭한 시간)\n",
    "  - Processing-time : 데이터를 수집 혹은 수신한 시간 (ex_ 브로커를 통해 로그가 수신된 시간)\n",
    "* Bounded vs. Unbounded ?\n",
    "  - Bounded : 데이터를 처리하는 시점에 그 범위가 명확하게 정해진 경우를 말합니다 (ex_ 배치 처리와 같이 어제 하루에 수신된 모든 로그를 처리해야 하는 경우)\n",
    "  - Unbounded : 데이터를 처리하는 시점에 그 범위가 명확하게 정해지지 않은 경우를 말합니다 (ex_ 스트리밍 데이터와 같이 끝없이 이어지는 데이터를 수신하고 처리하는 경우)\n",
    "* Source vs. Sink ?\n",
    "  - Source : 처리해야 하는 데이터 소스 위치\n",
    "  - Sink : 데이터를 저장 혹은 전달하는 타겟 위치\n",
    "* incrementalization ?\n",
    "  - 스트리밍 처리에서 스파크는 Unbounded 데이터를 마치 정적인 테이블인 것처럼 동작하게 하고, 최종 출력 싱크에 저장될 결과 테이블을 계산합니다.\n",
    "  - **여기서의 배치와 유사한 쿼리 수행을 스트리밍 실행 계획으로 변환**하게 하는데 이 과정을 '증분화(*incrementalization*)'라고 합니다. \n",
    "  - 스파크는 레코드가 도착할 때마다 결과를 업데이트 하기 위해 어떤 상태를 유지해야 하는지를 확인하여 점진적으로 결과를 업데이트 합니다.\n",
    "  - 즉, 마치 배치 처리를 통한 소스 테이블 쿼리 후 타겟 테이블에 저장하는 것 처럼 보이지만, 실제로는 증분화된 스트리밍 처리 계획이며 이를 증분화라고 합니다\n",
    "* materializing ?\n",
    "  - RDD 혹은 Dataframe 의 데이터 객체 상태의 meterialized 의 의미는 lazy evaluation 관점에서 보았을 때에 not materialized 의 의미는 실행 계획이 아직 검토되지 않았다고 말할 수 있으며, materialized 되었다는 의미는 실행계획을 통해 대상 데이터가 메모리에 올라와서 접근가능한 상태정도라고 말할 수 있습니다.\n",
    "    - we allocate memory for it. That is, we store it into memory (.persist()) or even store it into durable storage (.persist(RELIABLE)).\n",
    "    - Sparks runtime does lazy evaluation so until an action is taken the RDD is not materialized.\n",
    "  - 한편으로 논리적인 테이블 관점에서 보았을 때에는, createOrReplaceTempView 는 가상의 테이블에 대한 메타정보만 가진 테이블을 말하며, saveAsTable 명령은 실제 데이터 프레임을 물리적인 저장 경로에 저장한 상태를 말합니다.\n",
    "    - Unlike the createOrReplaceTempView command, saveAsTable will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore. \n",
    "  - 참고로 스파크에서는 meterialized view 를 아직 지원하지(SPARK-29038) 않습니다\n",
    "* epoch ?\n",
    "  - 컴퓨팅 컨텍스트에서 에포크는 컴퓨터의 시계 및 타임스탬프 값이 결정되는 날짜와 시간입니다.\n",
    "  - 에포크는 일반적으로 시스템마다 다른 특정 날짜의 0시 0분 0초(00:00:00) UTC(협정 세계시)에 해당합니다.\n",
    "  - 예를 들어 대부분의 Unix 버전은 1970년 1월 1일을 epoch 날짜로 사용합니다. - [Epoch Time](https://searchdatacenter.techtarget.com/definition/epoch)\n",
    "    - 스트리밍 처리에 있어서 데이터 처리 트랜잭션 관리 및 유지를 위해 commit log 의 commits 정보를 metadata checkpoint directory 에 저장하게 됩니다.\n",
    "    - `Offset Commit Log — commits Metadata Checkpoint Directory` 체크포인트 경로에 메타데이터를 커밋하는 행위를 말합니다.\n",
    "* micro-batch ?\n",
    "  - 스파크의 Structured Streaming 은 엄밀히 말하면 Continuous Streaming 처리가 아니라 500ms 수준의 작은 배치 작업으로 쪼개어 (deterministic) 수행하는데 이 작은 배치 작업을 말합니다\n",
    "    - Minimum batch size Spark Streaming can use.is 500 milliseconds, is has proven to be a good minimum size for many applications.\n",
    "    - The best approach is to start with a larger batch size (around 10 seconds) and work your way down to a smaller batch size.\n",
    "* *stateless* and *stateful* in spark transformation?\n",
    "  - stateless : 스트리밍 처리에 있어 이전 상태에 의존하지 않고 현재의 상태만 이용하여 변환 혹은 처리를 수행하는 경우 (ex_ 현재 그룹 내의 빈도 혹은 특정 값의 상태)\n",
    "  - stateful : 스트리밍 처리에 있어 이전 상태에 의존한 데이터 변환을 말합니다 (ex_ 누적 값 혹은 집계 연산 결과 상태)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 예상되는 질문들\n",
    "\n",
    "* 스트리밍의 경우 explain 명령을 통해 '로지컬 플랜' 혹은 '최적화된 플랜'을 확인할 수 없나요?\n",
    "  - 스트리밍 쿼리에 대해 아래와 같이 explain 함수를 통해 확인할 수 있습니다\n",
    "```python\n",
    "query.processAllAvailable()\n",
    "query.explain()\n",
    "```\n",
    "\n",
    "* 동일한 키가 존재하는 정렬의 경우 왜 결정론적이지 못 한가요?\n",
    "  - 분산환경에서 셔플링이 발생하는 경우 일시적인 네트워크 지연, 특정 노드의 장애에 따라 Reduce 노드에 도착하는 데이터의 순서는 언제든지 변경될 수 있습니다\n",
    "\n",
    "* 스트리밍 처리가 1초 단위라고 했는데 소캣을 통한 테스트 시에는 화면 출력이 아주 느려 보이는데 왜 그런가요?\n",
    "  - 내부 트리거는 계속 발생하지만, 데이터 소스로부터 가져올 데이터가 없기 때문에 수행되지 않는 것처럼 보입니다\n",
    "  - 화면에 출력되는 것은 최종 출력 싱크에 트리거로부터 저장할 데이터가 발생하는 경우에만 발생하는 이벤트라서 그렇습니다\n",
    "\n",
    "* 계속 지켜보면 데이터가 없지만 결과 테이블이 출력되는 경우도 있는데 왜 그런가요?\n",
    "  - 메시지를 잘 살펴보시면 아래와 같은데요, 해석해 보면, '현재 배치는 지연되고 있으며, 트리거는 1초 이지만 약 8초 정도 소요되었다'라고 나오고 있습니다.\n",
    "  - `21/06/27 08:19:06 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 8314 milliseconds`\n",
    "  - '스파크 스트리밍' 처리에 대한 기준은 몇 가지가 있는데 1: 초기 기동 시에 실행, 2: 소스에 처리할 데이터가 존재하는 경우 실행, 3: 임계시간이 지난경우 \n",
    "\n",
    "* JSON 혹은 header 가 있는 TSV 등은 스키마 infer 가 가능한데 왜 schema 를 명시적으로 생성해 주어야 하나요?\n",
    "  - '스파크 스트리밍'의 경우 데이터 타입에 민감하며 자칫 실수하는 경우 전체 스트리밍 파이프라인의 장애로 이어지기 때문에 엄격한 스키마 정의가 반드시 필요합니다\n",
    "  - spark.readStream.schema(schema) 와 같이 정의되어야만 하며, 그렇지 않으면 아래와 같은 오류를 출력합니다\n",
    "  - `IllegalArgumentException: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.`\n",
    "\n",
    "* 집계함수 출력이 안 되는데 왜 그런가?\n",
    "  - 기본 출력 모드가 append 모드이기 떄문에, watermark 지정을 해주지 않는 경우 집계함수를 append 모드 통해서 저장할 수 없습니다. (consistency 문제)\n",
    "  - 출력모드를 console complete 로 지정하여 테스트하거나, watermark 를 통해 출력하여야만 하며 그렇지 않으면 아래와 같은 오류를 출력합니다\n",
    "  - `AnalysisException: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark`\n",
    "\n",
    "* 입력 데이터 지정시에 JSON 파일을 넣을 수는 없는가?\n",
    "  - '스파크 스트리밍' 데이터는 파일 단위로 생성된 데이터 여부를 판단하기 때문에 경로를 입력해야만 합니다 그렇지 않으면 아래와 같은 오류를 콘솔에 출력합니다\n",
    "  - `java.lang.IllegalArgumentException: Option 'basePath' must be a directory`\n",
    "    \n",
    "* 카프카 엔진의 포트가 왜 9092, 9093 으로 나누어져 있고, kafka:9093, localhost:9092 로 접근해야만 하는가?\n",
    "  - 카프카 디폴트 포트는 9092 포트입니다\n",
    "  - 리스너 설정 시에 기본 구성의 경우 `listeners=PLAINTEXT://broker1:9091,SSL://broker1:9092,SASL_SSL://broker1:9093` 으로 9091 ~ 9093 까지 용도에 따라 설정합니다\n",
    "  - [Security Protocol](http://kafka.apache.org/11/javadoc/org/apache/kafka/common/security/auth/SecurityProtocol.html) - 프로토콜은 아래와 같습니다.\n",
    "  | Enum Constant | Description |\n",
    "  | --- | --- |\n",
    "  | PLAINTEXT | Un-authenticated, non-encrypted channel |\n",
    "  | SASL_PLAINTEXT | SASL authenticated, non-encrypted channel |\n",
    "  | SASL_SSL | SASL authenticated, SSL channel |\n",
    "  | SSL | SSL channel |\n",
    "\n",
    "* 스파크 스트리밍도 배치 처리와 유사하게 몇 개의 익스큐터가 동시에 기동되고 항상 떠서 서로간의 데이터를 주고 받는 방식인가요?\n",
    "  - 익스큐터들 내에 집계 연산을 위해 셔플을 통해 데이터를 전송합니다\n",
    "\n",
    "* 텀블링 윈도우는 항상 그 시간대를 유지하면서 메모리에 저장되어 있는가?\n",
    "  - 워터마크가 없는 스트리밍은 끝없이 출력되는 것으로 간주하고 수행됩니다\n",
    "\n",
    "\n",
    "\n",
    "> 중요한 질문\n",
    "\n",
    "* 스트리밍 윈도우 워터마크 동작방식이 이벤트타임 시간을 기준으로 상대적인 시간인지, 마지막 시간 이후의 절대적 시간인지?\n",
    "\n",
    "* 수행할 수 있는 Executor 수를 임의로 지정할 수 있는지?\n",
    "\n",
    "* dynamic allocation 방식이 streaming 은 더 유용한 것이 아닌지?\n",
    "\n",
    "* 상태를 backend storage 에 저장하게 되는지 여기는 어디인지? hdfs? local disk?\n",
    "\n",
    "* 파일을 통한 스트리밍 읽을 때에는 추가된 내용에 대해서는 왜 처리가 안되는지, 그리고 한 번에 모든 데이터가 처리되는지?\n",
    "\n",
    "* 지연된다고 하더라도 집계에 포함되는 경우도 있다고 하는데 언제 이러한 현상이 발생하는가? 위에서 만든 생성자 예제와 비슷한 상황인가?\n",
    "\n",
    "> 기타 질문\n",
    "\n",
    "* 스트리밍 처리를 통한 화면 출력이 즉각적으로 출력되지 않고 몇 초 간의 딜레이가 있는데 왜 그런지, 어떻게 바로 출력할 수 있는지?\n",
    "\n",
    "* 스트리밍 모니터링을 위해서 awaitTermination 호출이 진행되고 있는 동안에는 상태를 확인할 수 없는데 어떻게 모니터링하면 좋은가?\n",
    "\n",
    "* 스트리밍 모니터링을 JMX 통한 매트릭 조회는 불가능한가요?\n",
    "\n",
    "* 입력은 TSV 가 되는데 출력은 왜 TSV 로 할 수 없는가?\n",
    "\n",
    "* 카프카 연동 시에 다양한 옵션에 대한 설명과 예제는 없나요?\n",
    "\n",
    "* 옵션을 주지 않으면 텀블링 윈도우인가? 구별은 어떻게 하는가\n",
    "\n",
    "* 스트리밍 결과가 지연된 데이터 출력 때문에 시간 순으로 보이지 않는데 어떻게 정렬할 수 있나요?\n",
    "\n",
    "* 스트리밍 조인 시에 '스트리밍 쿼리'를 재시작 하지 않고 반영할 수 있는 정적 데이터 소스에는 어떤 것들이 있나요?\n",
    "\n",
    "* 하나의 스파크 컨텍스트에서 여러개의 쿼리를 어떻게 수행할 수 있나요?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
