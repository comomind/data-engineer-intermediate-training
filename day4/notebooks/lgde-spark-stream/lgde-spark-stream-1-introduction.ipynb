{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 스트리밍 실습 1교시 : 스파크 스트리밍 소개\n",
    "\n",
    "> 구조화된 스파크 스트리밍 애플리케이션은 크게 5단계로 구분하여 설계합니다 (스키마/소스읽기, 변환코드작성, 싱크코드작성, 트리거코드작성, 애플리케이션기동), 이번 과정에는 JSON 파일을 읽어서 스트리밍 소스로 활용하는 가장 간단한 예제를 통해서 스파크 스트리밍의 동작방식에 대해 이해합니다\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "* 스파크 스트리밍 애플리케이션을 구현하는 5가지 단계를 이해합니다\n",
    "  - 스키마/소스 읽기\n",
    "  - 변환 코드 작성\n",
    "  - 싱크 코드 작성\n",
    "  - 트리거 코드 작성\n",
    "  - 애플리케이션 기동 및 종료\n",
    "* 특정 경로에 존재하는 JSON 파일을 파일 단위로 읽는 스트리밍 소스를 생성합니다\n",
    "* 특정 컬럼을 기준으로 집계함수를 적용한 변환 로직을 생성합니다\n",
    "* 콘솔에 결과를 출력하는 싱크를 구성하여 스트리밍 애플리케이션을 시작합니다\n",
    "* 약 1분간 스트리밍 처리를 수행하고, 애플리케이션을 종료합니다\n",
    "\n",
    "\n",
    "## 목차\n",
    "* [1. 소스 스키마 읽기](#1.-소스-스키마-읽기)\n",
    "* [2. 소스 읽기 코드 작성](#2.-소스-읽기-코드-작성)\n",
    "* [3. 변환 코드 작성](#3.-변환-코드-작성)\n",
    "* [4. 싱크 코드 작성](#4.-싱크-코드-작성)\n",
    "* [5. 트리거 코드 작성](#5.-트리거-코드-작성)\n",
    "* [6. 애플리케이션 기동 및 종료](#6.-애플리케이션-기동-및-종료)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 소스 스키마 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Arrival_Time</th><th>Creation_Time</th><th>Device</th><th>Index</th><th>Model</th><th>User</th><th>gt</th><th>x</th><th>y</th><th>z</th></tr>\n",
       "<tr><td>1424686735090</td><td>1424686733090638193</td><td>nexus4_1</td><td>18</td><td>nexus4</td><td>g</td><td>stand</td><td>3.356934E-4</td><td>-5.645752E-4</td><td>-0.018814087</td></tr>\n",
       "<tr><td>1424686735292</td><td>1424688581345918092</td><td>nexus4_2</td><td>66</td><td>nexus4</td><td>g</td><td>stand</td><td>-0.005722046</td><td>0.029083252</td><td>0.005569458</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
       "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
       "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
       "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
       "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
       "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 소스 스키마\n",
    "activityPath = f\"{work_data}/activity-data\"\n",
    "activityData = spark.read.option(\"inferSchema\", \"true\").json(activityPath)\n",
    "display(activityData.limit(2))\n",
    "activityData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 소스 읽기 코드 작성\n",
    "\n",
    "> 예제에서는 json 파일을 소스로 사용했으며, `format(\"json\").load(path)` 대신 `json(path)` 를 사용해도 동일합니다\n",
    "\n",
    "#### [Input Sources](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources)\n",
    "| 포맷 | 특징 | 옵션 |\n",
    "| --- | --- | --- |\n",
    "| File Source | parquet, json, csv 등의 파일의 읽기 | .option(\"path\", \"path/to/read\"), 콤마 구분자로 `여러 경로를 지정할 수 없습니다` |\n",
    "| Kafka Source | kafka 읽기를 위한 소스 | .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") 옵션을 통해 카프카 클러스터를 지정하고, .option(\"topic\", \"updates\") 옵션을 통해 토픽을 지정합니다 |\n",
    "| Socket Source | 소켓 서버로부터 스트리밍을 읽기 위한 소스 | .option(\"host\", \"localhost\"), .option(\"port\", 9999) 옵션을 통해 소켓서버를 지정합니다, includeTimestamp 옵션으로 `value`, `timestamp` 지원 |\n",
    "| Rate Source | 트리밍 디버깅을 위한 포맷이며, 콘솔로 바로 출력합니다 | Debug Only |\n",
    "\n",
    "* File Source Options\n",
    "  - maxFilesPerTrigger : maximum number of new files to be considered in every trigger\n",
    "  - latestFirst : whether to process the latest new files first, useful when there is a large backlog of files (default: false)\n",
    "  - maxFileAge : A mapping from a file that we have processed to some timestamp it was last modified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소스 읽기\n",
    "activitySchema = activityData.schema\n",
    "activityReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .schema(activitySchema)\n",
    "    .format(\"json\")\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .load(activityPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 변환 코드 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 변환\n",
    "activityCounter = activityReader.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 싱크 코드 작성\n",
    "\n",
    "> 예제에서는 디버깅을 위한 console 포맷과 update 모드를 사용했습니다\n",
    "\n",
    "#### [Output Sinks](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks)\n",
    "| 포맷 | 특징 | 옵션 |\n",
    "| --- | --- | --- |\n",
    "| File Sink | parquet, json, csv 등의 파일을 저장 | .option(\"path\", \"path/to/write\") |\n",
    "| Kafka Sink | kafka 저장을 위한 싱크 | .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") 옵션을 통해 카프카 클러스터를 지정하고, .option(\"topic\", \"updates\") 옵션을 통해 토픽을 지정합니다 |\n",
    "| Foreach Sink | 스트리밍 싱크를 지원하지 않는 경우에도 임의의 연산을 수행을 통해 저장할 수 있는 옵션 | - |\n",
    "| Console Sink | 트리밍 디버깅을 위한 포맷이며, 콘솔로 바로 출력합니다 | Debug Only |\n",
    "| Memory Sink | 스트리밍 디버깅을 위한 포매싱며, 임의의 테이블로 출력합니다 | Debug Only, .queryName(\"tableName\") 옵션을 통해 테이블 이름을 지정합니다 |\n",
    "\n",
    "\n",
    "#### [Output Modes](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes)\n",
    "\n",
    "| 출력 모드 | 특징 | 옵션 |\n",
    "| --- | --- | --- |\n",
    "| Append Mode | 신규로 발생한 로우의 추가만 가능한 모드이며, 기존 결과 테이블에 저장된 로우들은 변경되지 않습니다 | .option(\"path\", \"path/to/file\") |\n",
    "| Update Mode | 마지막 트리거 이후에 발생한 모든 로우가 외부 저장소에 변경되는 모드 | MySQL 과 같이 업데이트를 지원하는 싱크 저장소만 사용할 수 있으며, 쿼리가 Aggregation 을 포함하지 않는다면 Complete 와 동일하게 동작합니다 |\n",
    "| Complete Mode | 매번 결과 테이블 전체를 갱신하는 모드이며 | 모든 로우를 매번 출력할 만큼 충분히 결과가 작은 경우에 사용할 수 있습니다 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 싱크 쓰기\n",
    "queryName = \"activity_count\"\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "activityWriter = (\n",
    "    activityCounter\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 트리거 코드 작성\n",
    "\n",
    "#### [Triggers](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers)\n",
    "\n",
    "| 트리거 유형 | 특징 | 옵션 |\n",
    "| --- | --- | --- |\n",
    "| Unspecified (Default) | 지정하지 않은 경우는 마이크로 배치 모드로 동작하며, 이전 마이크로 배치가 종료하는 즉시 다음 작업이 트리거 됩니다. | - |\n",
    "| Fixed interval micro-batches | 제공되는 인터벌에 따라 동작하지만, 이전 작업이 인터벌 보다 늦게 종료되는 경우, 종료 즉시 다음 작업이 트리거 됩니다 | 새로운 작업이 없는 경우 트리거링 되지 않습니다 |\n",
    "| One-time micro-batch | 처음 한 번만 수행됩니다 | 스케줄러 등을 통해 주기적으로 스트리밍 서비스를 수행할 때에 유용합니다 |\n",
    "| Continuous with fixed checkpoint interval (experimental) | 마이크로 배치가 아니라 Continuous 모드로 동작합니다 | [Continuous Processing](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리거\n",
    "activityTrigger = (\n",
    "    activityWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 애플리케이션 기동 및 종료\n",
    "\n",
    "* 기동 작업을 재수행 한다고 하더라도, 과거에 체크포인트에 저장된 값을 통해 기동되기 때문에 변화가 없습니다\n",
    "* 다시 재기동 하기 위해서는 체크포인트를 삭제하고, 처음부터 기동하여야 합니다\n",
    "```bash\n",
    "# 임시파일 삭제\n",
    "!rm -rf $checkpointLocation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기동\n",
    "activityQuery = activityTrigger.start()\n",
    "activityQuery.awaitTermination(30) # 노트북 특성상, 대기하면 다음 실행을 할 수 없으므로 30초만 수행하고 종료합니다\n",
    "activityQuery.stop() # 특정 타스크가 처리 중에 timeout 되는 경우, 해당 스레드가 종료되어 예외가 발생할 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 파일을 통한 스트리밍 시에 유의해야 할 특징 4가지\n",
    "* 하나. 모든 파일들의 포맷과 스키마는 동일해야 하며, 그렇지 않은 경우 쿼리의 실패로 이어지게 됩니다\n",
    "* 둘. 디렉토리 내의 파일 단위로 원자적으로 동작해야 하며, 한번에 정상적으로 읽어져야 합니다\n",
    "  - 즉, append 되고 있는 파일의 경우 문제가 될 수 있으며, update 혹은 append 되어서는 안 됩니다\n",
    "* 셋. 디렉토리 내의 새로운 파일들이 존재하더라도 모든 파일이 동시에 수행되지 않습니다\n",
    "  - 동시 수행 임계치에 따라 병렬로 수행되며, 정해진 순서를 보장하지 않습니다.\n",
    "  - maxFilesPerTrigger 옵션을 통해 처리율을 조정할 수 있습니다 - [프로그래밍 가이드](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#creating-streaming-dataframes-and-streaming-datasets)를 참고할 수 있습니다\n",
    "* 넷. 파일 쓰기는 append 모드만 지원하며, 이미 존재하는 파일에는 append 될 수 없습니다\n",
    "  - *end-to-end exatly-once gurantees* 지원은 _spark_metadata 경로에 존재하는 log 파일들을 통해 관리되어 가능하며, 종복 혹은 일부만 읽혀지는 경우는 없습니다. \n",
    "  - 만일 애플리케이션 재실행 시에 변경된 스키마로 데이터가 추가 되었다면, 쿼리 시에 해당 스키마가 조정되어야만 합니다\n",
    "  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>1. [중급]</font> 현재 생성된 스트리밍 애플리케이션을 처음부터 다시 수행해 보세요\n",
    "\n",
    "* 처음부터 다시 스트리밍 처리를 하기 위해서는 어떻게 해야하나요?\n",
    "  - 리눅스 삭제 명령 : `!rm -rf /path/to/remove`\n",
    "  - 체크포인트 위치 : /home/jovyan/work/lgde-spark-stream/tmp/activity_count\n",
    "  - 스트리밍 쓰기 객체 activityTrigger 는 실습 과정에서 정상적으로 생성되었다고 가정합니다\n",
    "\n",
    "* 현재 activityQuery 를 다시 수행하되 충분히 긴 시간동안 수행하여 오류 메시지가 발생하지 않도록 수행해 보세요\n",
    "  - 타임아웃 : 80초 이상\n",
    "  - 쿼리 실행 이후 애플리케이션을 반드시 종료해 주세요\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "! rm -rf /home/jovyan/work/lgde-spark-stream/tmp/activity_count\n",
    "activityQuery = activityTrigger.start()\n",
    "activityQuery.awaitTermination(80)\n",
    "activityQuery.stop()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "! rm -rf /home/jovyan/work/lgde-spark-stream/tmp/activity_count\n",
    "activityQuery = activityTrigger.start()\n",
    "activityQuery.awaitTermination(80)\n",
    "activityQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>2. [고급]</font> 스키마를 읽지 않고 직접 지정하는 코드를 작성해 보세요\n",
    "\n",
    "* [pyspark.sql.types.StructType](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html?highlight=structtype#pyspark.sql.types.StructType) 참고\n",
    "  - `customSchema = StructType([StructField(\"f1\", StringType(), True)])` 와 같은 형식으로 작성합니다\n",
    "\n",
    "* 기존에 작성된 코드를 활용하여 하나의 Cell 안에 작성해 보세요\n",
    "  - 타임아웃 : 10초 내외\n",
    "  - 쿼리 실행 이후 애플리케이션을 반드시 종료해 주세요\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "customSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"Arrival_Time\", LongType()))\n",
    "    .add(StructField(\"Creation_Time\", LongType()))\n",
    "    .add(StructField(\"Device\", StringType()))\n",
    "    .add(StructField(\"Index\", LongType()))\n",
    "    .add(StructField(\"Model\", StringType()))\n",
    "    .add(StructField(\"User\", StringType()))\n",
    "    .add(StructField(\"gt\", StringType()))\n",
    "    .add(StructField(\"x\", DoubleType()))\n",
    "    .add(StructField(\"y\", DoubleType()))\n",
    "    .add(StructField(\"z\", DoubleType()))\n",
    ")\n",
    "activityPath = f\"{work_data}/activity-data\"\n",
    "customReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(customSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .load(activityPath)\n",
    ")\n",
    "customCounter = (\n",
    "    customReader.groupBy(\"gt\").count()\n",
    ")\n",
    "queryName = \"custom_count\"\n",
    "customWriter = (\n",
    "    customCounter\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    ")\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "customTrigger = (\n",
    "    customWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "customQuery = customTrigger.start()\n",
    "customQuery.awaitTermination(10)\n",
    "customQuery.stop()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "customSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"Arrival_Time\", LongType()))\n",
    "    .add(StructField(\"Creation_Time\", LongType()))\n",
    "    .add(StructField(\"Device\", StringType()))\n",
    "    .add(StructField(\"Index\", LongType()))\n",
    "    .add(StructField(\"Model\", StringType()))\n",
    "    .add(StructField(\"User\", StringType()))\n",
    "    .add(StructField(\"gt\", StringType()))\n",
    "    .add(StructField(\"x\", DoubleType()))\n",
    "    .add(StructField(\"y\", DoubleType()))\n",
    "    .add(StructField(\"z\", DoubleType()))\n",
    ")\n",
    "activityPath = f\"{work_data}/activity-data\"\n",
    "customReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(customSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .load(activityPath)\n",
    ")\n",
    "customCounter = (\n",
    "    customReader.groupBy(\"gt\").count()\n",
    ")\n",
    "queryName = \"custom_count\"\n",
    "customWriter = (\n",
    "    customCounter\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    ")\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "customTrigger = (\n",
    "    customWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "customQuery = customTrigger.start()\n",
    "customQuery.awaitTermination(10)\n",
    "customQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>3. [고급]</font> 스트리밍 처리가 완료된 이후에 파일이 추가되면 어떻게 동작하나요?\n",
    "\n",
    "* 80초 이상 대기하여 카운트가 수렴한 시점에 data 경로 아래의 appended-1.json 파일을 수동으로 복사해 주고, 카운트에 변화가 있는지 확인합니다\n",
    "  - 타임아웃 : 180초 이상 (애플리케이션이 파일을 복사할 시간을 충분히 가질 수 있도록)\n",
    "  - 데이터 경로 : /home/jovyan/work/activity-data\n",
    "  - JSON 파일(\"part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")을 임의의 파일(appended-1.json\")로 변경하여 복사합니다\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "! rm -rf /home/jovyan/work/lgde-spark-stream/tmp/activity_count\n",
    "activityQuery = activityWriter.start()\n",
    "activityQuery.awaitTermination(180)\n",
    "activityQuery.stop()\n",
    "    \n",
    "# 80초 이후 카운트가 수렴되었을 때에 json 파일 추가 시에 아래와 같이 카운트 수가 증가합니다\n",
    "notebook     | -------------------------------------------\n",
    "notebook     | Batch: 80\n",
    "notebook     | -------------------------------------------\n",
    "notebook     | +----------+-------+\n",
    "notebook     | |        gt|  count|\n",
    "notebook     | +----------+-------+\n",
    "notebook     | |       sit| 997023|\n",
    "notebook     | |     stand| 922167|\n",
    "notebook     | |stairsdown| 758424|\n",
    "notebook     | |      walk|1073658|\n",
    "notebook     | |      null| 846174|\n",
    "notebook     | |  stairsup| 847050|\n",
    "notebook     | |      bike| 874506|\n",
    "notebook     | +----------+-------+\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /home/jovyan/work/lgde-spark-stream/tmp/activity_count\n",
    "activityQuery = activityWriter.start()\n",
    "activityQuery.awaitTermination(180)\n",
    "activityQuery.stop()\n",
    "\n",
    "# docker\n",
    "# $ cp /home/jovyan/work/data/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json /home/jovyan/work/data/activity-data/appended-1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>4. [중급]</font> 애플리케이션이 종료된 이후에 파일이 추가된 경우, 애플리케이션을 재기동하면 어떻게 동작하나요?\n",
    "\n",
    "* 이전 예제에서 이미 애플리케이션이 종료었기 때문에, data 경로에 임의의 json 파일을 appended-2.json 으로| 수동으로 복사해 주고, 애플리케이션을 재기동하여 카운트에 변화가 있는지 확인합니다\n",
    "  - 타임아웃 : 10초 이상 (기동 이후에 확인할 정도의 시간)\n",
    "  - 데이터 경로 : /home/jovyan/work/activity-data\n",
    "  - JSON 파일(\"part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")을 임의의 파일(appended-2.json\")로 변경하여 복사합니다\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다. 파일 복사는 수동으로 진행해도 무관합니다\n",
    "\n",
    "```python\n",
    "# 아래에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "!cp \"$work_data/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\" \"$work_data/activity-data/appended-2.json\"\n",
    "activityQuery = activityWriter.start()\n",
    "activityQuery.awaitTermination(10)\n",
    "activityQuery.stop()\n",
    "    \n",
    "# 애플리케이션이 재기동 되었을 때에 추가된 json 파일 크기만큼 아래와 같이 카운트 수가 증가합니다\n",
    "notebook     | -------------------------------------------\n",
    "notebook     | Batch: 81\n",
    "notebook     | -------------------------------------------\n",
    "notebook     | +----------+-------+\n",
    "notebook     | |        gt|  count|\n",
    "notebook     | +----------+-------+\n",
    "notebook     | |       sit|1009332|\n",
    "notebook     | |     stand| 933551|\n",
    "notebook     | |stairsdown| 767789|\n",
    "notebook     | |      walk|1086914|\n",
    "notebook     | |      null| 856623|\n",
    "notebook     | |  stairsup| 857502|\n",
    "notebook     | |      bike| 885302|\n",
    "notebook     | +----------+-------+\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기동\n",
    "!cp \"$work_data/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\" \"$work_data/activity-data/appended-2.json\"\n",
    "activityQuery = activityWriter.start()\n",
    "activityStatus = activityQuery.status\n",
    "activityQuery.awaitTermination(10) # 노트북 특성상, 대기하면 다음 실행을 할 수 없으므로 10초만 수행하고 종료합니다\n",
    "activityQuery.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
