{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4일차 2교시 기본 연산 다루기\n",
    "\n",
    "### 목차\n",
    "* 1. 기본 연산자\n",
    "* 2. 데이터 스키마\n",
    "* 3. 데이터프레임\n",
    "* 4. 참고자료\n",
    "  * https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/types/StructField.html\n",
    "  * https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 기본 연산자\n",
    "\n",
    "#### 1-1. Transformation & Lazy Evaluation\n",
    "+ RDD(Rsilient Distributed Dataset): 불변성을 가지며 병렬로 처리할 수 있는 파티셔닝된 레코드의 모음\n",
    "    + RDD: 불변성, 한번 생성하면 변경할 수 없음\n",
    "+ Transformation: 원하는 변경 방법을 알려주는 명령. 추상적인 변경 방법이며, 실제 수행X\n",
    "+ Lazy Evaluation(지연 연산): 특정 연산 명령을 하면 즉시 데이터를 수정하지 않고, 원시 데이터에 적용할 트랜스포메이션 실행계획 생성하고, 실제로 연산 그래프를 처리하기 직전까지 기다림\n",
    "+ 전체 데이터 흐름 최적화\n",
    "\n",
    "---\n",
    "#### 1-2. DataFrame\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.printSchema() | 스키마 정보를 출력합니다. | - |\n",
    "| df.schema | StructType 스키마를 반환합니다 | - |\n",
    "| df.columns | 컬럼명 정보를 반환합니다 | - |\n",
    "| df.show(n) | 데이터 n 개를 출력합니다 | - |\n",
    "| df.first() | 데이터 프레임의 첫 번째 Row 를 반환합니다 | - |\n",
    "| df.head(n) | 데이터 프레임의 처음부터 n 개의 Row 를 반환합니다 | - |\n",
    "| df.createOrReplaceTempView | 임시 뷰 테이블을 생성합니다 | - |\n",
    "| df.sample(withReplacement, fraction, seed) | 랜덤 샘플링을 수행합니다 (복원여부, 비율, 시드) | 시드 지정을 통해 일정한 샘플링 결과를 얻을 수 있습니다 |\n",
    "| df.randomSplit() | 랜덤 스플릿을 수행합니다 | - |\n",
    "| df.union(newdf) | 데이터프레임 간의 유니온 연산을 수행합니다 | - |\n",
    "| df.limit(n) | 추출할 로우수 제한 | T |\n",
    "| df.repartition(n) | 파티션 재분배, 셔플발생 | - |\n",
    "| df.coalesce() | 셔플하지 않고 파티션을 병합 | 마지막 스테이지의 reduce 수가 줄어드는 효과로 성능저하에 유의해야 합니다 |\n",
    "| df.collect() | 모든 데이터 수집, 반환 | A |\n",
    "| df.take(n) | 상위 n개 로우 반환 | A |\n",
    "| df.toLocalIterator() | iterator로 모든 파티션의 데이터를 드라이버에 전달, 파티션을 차례대로 반복처리 | - |\n",
    "\n",
    "---\n",
    "#### 1-3. DataFrame 컬럼관련 메서드\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.select | 컬럼이나 표현식 사용  | - |\n",
    "| df.selectExpr | 문자열 표현식 사용 = df.select(expr()) | - |\n",
    "| df.withColumn(컬럼명, 표현식) | 컬럼 추가, 비교, 컬럼명 변경 | - |\n",
    "| df.withColumnRenamed(old_name, new_name) | 컬럼명 변경 | - |\n",
    "| df.drop() | 컬럼 삭제 | - |\n",
    "| df.where | 로우 필터링 | - |\n",
    "| df.filter | 로우 필터링 | - |\n",
    "| df.sort, df.orderBy | 정렬 | - |\n",
    "| df.sortWithinPartitions | 파티션별 정렬 | - |\n",
    "\n",
    "---\n",
    "#### 1-4. 컬럼관련 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| expr(\"someCol - 5\") | 표현식 | - |\n",
    "| lit() | 리터럴 | - |\n",
    "| cast() | 컬럼 데이터 타입 변경 | - |\n",
    "| distinct() | unique row | - |\n",
    "| desc(), asc() | 정렬 순서 | - |\n",
    "\n",
    "---\n",
    "#### 1-5. 로우\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| Row() | 로우 생성 | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-6 Structured API 활용 가이드\n",
    "\n",
    "+ <strong>DataFrame</strong> 구성요소: 레코드, 컬럼\n",
    "    + 레코드: Row타입(Table의 로우)\n",
    "    + 컬럼: 레코드에 수행할 연산 표현식(테이블의 컬럼)\n",
    "+ <strong>스키마</strong>: 각 컬럼명과 데이터 타입을 정의\n",
    "+ <strong>파티셔닝</strong>: DataFrame이나 Dataset이 클러스터에서 물리적으로 배치되는 형태를 정의\n",
    "+ <strong>파티셔닝 스키마</strong>: 파이션을 배치하는 방법을 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data Engineer Intermediate Day4\") \\\n",
    "    .config(\"spark.dataengineer.intermediate.day4\", \"tutorial-2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 스키마 확인하기 \"\"\"\n",
    "df = spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\") # 미국 교통통계국이 제공하는 항공운항 데이터\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.load(\"./data/flight-data/json/2015-summary.json\", format=\"json\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.json(\"./data/flight-data/json/2015-summary.json\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위에서 사용한 3가지 모두 동일한 결과를 얻을 수 있으며 편한 방식의 구조화된 API 를 활용하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 스키마\n",
    "\n",
    "#### 2.1 직접 스키마를 지정하여 읽는 방법\n",
    "+ 스키마: 여러 개의 StructField 타입 필드로 구성된 StructType 객체\n",
    "+ StructField: (이름), (데이터 타입), (컬럼이 값이 없거나 null일 수 있는지 지정하는 불리언 값) 으로 구성\n",
    "---\n",
    "+ https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/types/StructField.html\n",
    "+ StructField(String name, DataType dataType, boolean nullable, Metadata metadata)\n",
    "+ Metadata: 해당 컬럼과 관련된 정보이며, 스파크의 머신러닝 라이브러리에서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 스키마를 직접 만들어 적용 \"\"\"\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"json\")\\\n",
    "    .schema(myManualSchema)\\\n",
    "    .load(\"data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.load(\"data/flight-data/json/2015-summary.json\", format=\"json\", schema=myManualSchema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위에서 사용한 2가지 모두 동일한 결과를 얻을 수 있으며 편한 방식의 구조화된 API 를 활용하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 컬럼\n",
    "+ <strong>컬럼: 표현식을 사용해 레코드 단위로 계산한 값을 단순하게 나타내는 논리적인 구조</strong> (테이블의 컬럼으로 생각할 수 있음)\n",
    "+ 컬럼의 실제값을 얻으려면 로우가 필요하고, 로우를 얻으려면 DataFrame이 필요 (로우는 데이터 레코드)\n",
    "+ DataFrame을 통하지 않으면 외부에서 컬럼에 접근 불가\n",
    "+ 즉, DF -> row -> column\n",
    "---\n",
    "+ 컬럼의 내용을 수정하려면 반드시 DataFrame의 스파크 트랜포메이션을 사용\n",
    "+ col, column 함수를 사용하는 것이 가장 간편 (책에서는 col 함수 사용)\n",
    "+ 컬럼은 컬럼명을 카탈로그에 저장된 정보와 비교하기 전까지 미확인 상태\n",
    "+ 분석기가 동작하는 단계에서 컬럼과 테이블을 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'someColumnName'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 컬럼 생성 \"\"\"\n",
    "from pyspark.sql.functions import col, column\n",
    "\n",
    "col(\"someColumnName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 표현식\n",
    "+ <strong>표현식: DataFrame 레코드의 여러 값에 대한 트랜스포메이션 집합을 의미</strong>\n",
    "+ 여러 컬럼명을 입력받아 식별하고 단일 값을 만들기 위해 다양한 표현식을 각 레코드에 적용하는 함수\n",
    "+ <strong>표현식은 expr함수로 사용</strong>\n",
    "+ <strong>컬럼은 단지 표현식일 뿐</strong>\n",
    "+ <strong>표현식은 연산 순서를 지정하는 논리적 트리로 컴파일됨</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'(someCol - 5)'>\n",
      "Column<b'(someCol - 5)'>\n",
      "Column<b'(someCol - 5)'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column<b'((((somecol + 5) * 200) - 6) < othercol)'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "print(expr(\"someCol - 5\"))\n",
    "print(expr(\"someCol\") - 5)\n",
    "print(col(\"someCol\") - 5)\n",
    "\n",
    "# 동일한 트랜스포메이션 과정: 스파크는 연산 순서를 지정하는 논리적 트리로 컴파일합니다\n",
    "(((col(\"somecol\") + 5) * 200) - 6) < col(\"othercol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 논리적 트리로 컴파일되는 표현식 \"\"\"\n",
    "from pyspark.sql.functions import expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dag.png](image/dag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" columns 속성을 사용 \"\"\"\n",
    "spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 레코드와 로우\n",
    "+ 스파크는 레코드를 Row 객체로 표현\n",
    "    + (각 로우는 하나의 레코드, '로우'와 '레코드'를 같은 의미로 사용)\n",
    "    + (대문자로 시작하는 Row는 Row 객체를 의미)\n",
    "+ Row 객체는 내부에 바이트 배열을 가지며, 오직 컬럼 표현식으로만 다룰 수 있으므로 사용자에게 노출되지 않음\n",
    "+ DataFrame을 사용해 드라이버에게 개별 로우를 반환하는 명령은 항상 하나 이상의 Row 타입을 반환\n",
    "+ Row 객체는 스키마 정보를 가지고 있지 않음 (DataFrame만 유일하게 스키마를 가지고 있음)\n",
    "+ Row 객체를 직접 생성하려면 DataFrame의 스키마와 같은 순서로 값을 명시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Row를 확인하는 예문 \"\"\"\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello', None, 1, False]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 로우 생성하기 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)\n",
    "\n",
    "\"\"\" 로우 접근하기 \"\"\"\n",
    "print(myRow[0])\n",
    "print(myRow[2])\n",
    "[*myRow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 데이터프레임\n",
    "\n",
    "![transform](image/transform.png)\n",
    "\n",
    "+ 로우나 컬럼 추가\n",
    "+ 로우나 컬럼 제거\n",
    "+ 로우를 컬럼으로 변환하거나, 그 반대로 변환\n",
    "+ 컬럼값을 기준으로 로우 순서 변경\n",
    "\n",
    "#### 3.1 DataFrame 생성하기\n",
    "+ 원시 데이터소스에서 DataFrame을 생성하고 임시 뷰를 등록\n",
    "+ Row 객체를 가진 Seq 타입을 직접 전환하여 DataFrame을 생성\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 원시 데이터소스 활용 \"\"\"\n",
    "df = spark.read.format(\"json\").load(\"data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"2015_summary\")\n",
    "\n",
    "sql_result = spark.sql(\"SELECT * FROM 2015_summary\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Row 객체 활용 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"some\", StringType(), True),\n",
    "    StructField(\"col\", StringType(), True),\n",
    "    StructField(\"names\", LongType(), False)\n",
    "])\n",
    "\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 select와 selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단일 혹은 다중 컬럼 설정 \"\"\"\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 컬럼을 참조하는 다앙한 방법 \"\"\"\n",
    "df.select(\n",
    "    \"DEST_COUNTRY_NAME\",\n",
    "    expr(\"DEST_COUNTRY_NAME\"),\n",
    "    col(\"DEST_COUNTRY_NAME\"),\n",
    "    column(\"DEST_COUNTRY_NAME\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" expr를 이용한 컬럼 참조 \"\"\"\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ select메서드에 expr 함수를 사용하는 패턴을 자주 사용함\n",
    "+ 스파크는 이를 위해 selectExpr 메서드를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "| newColmnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" selectExpr 활용 예문 \"\"\"\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColmnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|averageCount|distinctCount|\n",
      "+------------+-------------+\n",
      "| 1770.765625|          132|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 집계함수 지정하기 \"\"\"\n",
    "df.selectExpr(\"avg(count) as averageCount\", \"count(distinct(DEST_COUNTRY_NAME)) as distinctCount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 스파크 데이터 타입으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "리터럴(literal)을 사용한 컬럼 추가\n",
    "리터럴: 어떤 상수나 프로그래밍으로 생성된 변수값이 특정 컬럼의 값보다 큰지 확인할 때 사용\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 컬럼 추가하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "withColumn으로 컬럼 추가 \n",
    "withColumn(컬럼명, 표현식)\n",
    "\"\"\"\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 컬럼 비교 \"\"\"\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 컬럼명 바꾸기 \"\"\"\n",
    "before = df\n",
    "before.printSchema()\n",
    "\n",
    "after = before.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\"))\n",
    "after.printSchema()\n",
    "after.drop(\"Destination\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 컬럼명 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" withColumnRenamed \"\"\"\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 예약 문자와 키워드\n",
    "+ 공백이나 하이픈(-) 같은 예약 문자를 컬럼명에 사용하려면 백틱(`) 문자를 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania|   15|              Romania|\n",
      "|    United States|            Croatia|    1|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------------+\n",
      "|This Long Column-Name|\n",
      "+---------------------+\n",
      "|              Romania|\n",
      "|              Croatia|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" withColumn, selectExpr, select 차이점 \"\"\"\n",
    "dfWithLongColName = df.withColumn(\"This Long Column-Name\", expr(\"ORIGIN_COUNTRY_NAME\")) # 첫 번째 인수에서 사용하지 않음\n",
    "dfWithLongColName.show(2)\n",
    "\n",
    "dfWithLongColName.selectExpr(\"`This Long Column-Name`\", \"`This Long Column-Name` as `new col`\").show(2) # 사용함\n",
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 대소문자 구분\n",
    "+ 기본적으로 스파크는 대소문자를 가리지 않음\n",
    "```\n",
    "set spark.sql.caseSensitive true # 대소문자를 구분하기 위한 옵션\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 컬럼 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" drop 함수 \"\"\"\n",
    "\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "caseSensitive = df.drop(\"dest_country_name\")\n",
    "caseSensitive.printSchema()\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', False)\n",
    "caseInsensitive = df.drop(\"dest_country_name\")\n",
    "caseInsensitive.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count', 'This Long Column-Name']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" drop 함수 \"\"\"\n",
    "\n",
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns # 여러 컬럼을 지우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 컬럼의 데이터 타입 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|       15|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "|    United States|            Ireland|  344|      344|\n",
      "|            Egypt|      United States|   15|       15|\n",
      "|    United States|              India|   62|       62|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|int_count|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|    United States|            Romania|   15|       15|       15|\n",
      "|    United States|            Croatia|    1|        1|        1|\n",
      "|    United States|            Ireland|  344|      344|      344|\n",
      "|            Egypt|      United States|   15|       15|       15|\n",
      "|    United States|              India|   62|       62|       62|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      " |-- int_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" cast 함수 \"\"\"\n",
    "df.printSchema()\n",
    "\n",
    "int2str = df.withColumn(\"str_count\", col(\"count\").cast(\"string\"))\n",
    "int2str.show(5)\n",
    "int2str.printSchema()\n",
    "\n",
    "str2int = int2str.withColumn(\"int_count\", col(\"str_count\").cast(\"int\"))\n",
    "str2int.show(5)\n",
    "str2int.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10 로우 필터링하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" filter, where 함수 \"\"\"\n",
    "df.where(col(\"count\") < 2).show(2)\n",
    "df.filter(col(\"count\") < 2).show(2)\n",
    "\n",
    "df.where(\"count < 2\").show(2)\n",
    "df.filter(\"count < 2\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 같은 표현식에 여러 필터를 적용 \"\"\"\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.11 고유한 로우 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "\"\"\" distinct 함수 \"\"\"\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count())\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count())\n",
    "# distinctcount?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.12 무작위 샘플 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" sample 함수 \"\"\"\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.13 임의 분할하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "\"\"\" randomSplit 함수 \"\"\"\n",
    "seed = 42\n",
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "print(dataFrames[0].count())\n",
    "print(dataFrames[1].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.14 로우 합치기와 추가하기\n",
    "+ 동일한 스키마와 컬럼 수를 가져야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    5|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" union 함수 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "\n",
    "# Parallelized Collections :\n",
    "# Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program.\n",
    "# The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. \n",
    "\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows) \n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(newDF)\\\n",
    "    .where(\"count = 1\")\\\n",
    "    .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.15 로우 정렬하기\n",
    "+ asc, desc 함수를 사용하여 정렬 순서를 지정\n",
    "+ asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드로 null의 정렬 순서를 지정\n",
    "+ sortWithinPartitions 함수는 파티션별 정렬을 지원 (트랜스포메이션 처리 전 성능 최적화 목적)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" sort, orderBy 함수 \"\"\"\n",
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Vietnam|    2|\n",
      "|    United States|          Venezuela|  246|\n",
      "|    United States|            Uruguay|   13|\n",
      "|          Algeria|      United States|    4|\n",
      "|           Angola|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 정렬순서 지정하기 \"\"\"\n",
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "df.orderBy(expr(\"count desc\")).show(5)\n",
    "df.orderBy(col(\"ORIGIN_COUNTRY_NAME\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(5) # 예제 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 파티션별 정렬 \"\"\" \n",
    "# 최적화는 내일 세션에서 소개할 예정\n",
    "sortedWithPartitions = spark.read.format(\"json\").load(\"data/flight-data/json/*-summary.json\").sortWithinPartitions(\"count\")\n",
    "normalDataframe = spark.read.format(\"json\").load(\"data/flight-data/json/*-summary.json\").sort(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['count ASC NULLS FIRST], false\n",
      "+- Relation[DEST_COUNTRY_NAME#1100,ORIGIN_COUNTRY_NAME#1101,count#1102L] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint\n",
      "Sort [count#1102L ASC NULLS FIRST], false\n",
      "+- Relation[DEST_COUNTRY_NAME#1100,ORIGIN_COUNTRY_NAME#1101,count#1102L] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [count#1102L ASC NULLS FIRST], false\n",
      "+- Relation[DEST_COUNTRY_NAME#1100,ORIGIN_COUNTRY_NAME#1101,count#1102L] json\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Sort [count#1102L ASC NULLS FIRST], false, 0\n",
      "+- *(1) FileScan json [DEST_COUNTRY_NAME#1100,ORIGIN_COUNTRY_NAME#1101,count#1102L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/jovyan/work/data/flight-data/json/2011-summary.json, file:/home/jovy..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['count ASC NULLS FIRST], true\n",
      "+- Relation[DEST_COUNTRY_NAME#1112,ORIGIN_COUNTRY_NAME#1113,count#1114L] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint\n",
      "Sort [count#1114L ASC NULLS FIRST], true\n",
      "+- Relation[DEST_COUNTRY_NAME#1112,ORIGIN_COUNTRY_NAME#1113,count#1114L] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [count#1114L ASC NULLS FIRST], true\n",
      "+- Relation[DEST_COUNTRY_NAME#1112,ORIGIN_COUNTRY_NAME#1113,count#1114L] json\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Sort [count#1114L ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#1114L ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan json [DEST_COUNTRY_NAME#1112,ORIGIN_COUNTRY_NAME#1113,count#1114L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/jovyan/work/data/flight-data/json/2011-summary.json, file:/home/jovy..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n"
     ]
    }
   ],
   "source": [
    "sortedWithPartitions.explain(True)\n",
    "# print(sortedWithPartitions.rdd.getNumPartitions())\n",
    "\n",
    "normalDataframe.explain(True)\n",
    "# print(normalDataframe.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.16 로우 수 제한하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.17 repartition과 cocalesce\n",
    "+ 향후에 사용할 파티션 수가 현재 파티션 수보다 많거나 컬럼을 기준으로 파티션을 만드는 경우에 사용(repartition, 셔플이 필수로 발생)\n",
    "+ 자주 필터링되는 컬럼을 기준으로 파티션 재분배를 권장\n",
    "+ repartition: 물리적인 데이터 구성 제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "200\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 파티션 나누기 \"\"\"\n",
    "df.rdd.getNumPartitions()\n",
    "repart_1 = df.repartition(5)\n",
    "repart_2 = df.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "repart_3 = df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "\n",
    "# repartition 숫자 변경 등 실험내용 공유\n",
    "print(repart_1.rdd.getNumPartitions())\n",
    "print(repart_2.rdd.getNumPartitions())\n",
    "print(repart_3.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 셔플하지 않고 파티션을 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 파티션 합치기 \"\"\"\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.18 드라이버로 로우 데이터 수집하기\n",
    "+ 대규모 데이터셋에 collect 명령을 수행하면 드라이버 비정상 종료 우려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF = df.limit(5)\n",
    "collectDF.take(5) # 정수를 인수값으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collectDF.show()  # 결과를 정돈된 형태로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collectDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.collect() # 전체 모든 테이터를 수집, 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x7f360598fbd0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 대규모 데이터셋에 collect, toLocalIterator 수행하면 매우 큰 비용(cpu, 메모리, 네트워크) 발생, 드라이버 비정상적 종료 가능성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| num|count|\n",
      "+-----+----+-----+\n",
      "|Hello|   1|    1|\n",
      "|Hello|null|  aaa|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Row 객체 활용 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"some\", StringType(), True),\n",
    "    StructField(\"num\", StringType(), True),\n",
    "    StructField(\"count\", StringType(), True)\n",
    "])\n",
    "\n",
    "myRow1 = Row(\"Hello\", 1, 1)\n",
    "myRow2 = Row(\"Hello\", None, 'aaa')\n",
    "myDf = spark.createDataFrame([myRow1, myRow2], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+----+\n",
      "| some| num|count|num2|\n",
      "+-----+----+-----+----+\n",
      "|Hello|   1|    1|   1|\n",
      "|Hello|null|  aaa|null|\n",
      "+-----+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.withColumn(\"num2\", col(\"num\").cast(\"integer\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+------+\n",
      "| some| num|count|count2|\n",
      "+-----+----+-----+------+\n",
      "|Hello|   1|    1|     1|\n",
      "|Hello|null|  aaa|  null|\n",
      "+-----+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.withColumn(\"count2\", col(\"count\").cast(\"integer\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|count(DISTINCT ORIGIN_COUNTRY_NAME)|\n",
      "+-----------------------------------+\n",
      "|                                125|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"count(distinct(ORIGIN_COUNTRY_NAME))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "print(df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|count(DISTINCT ORIGIN_COUNTRY_NAME)|\n",
      "+-----------------------------------+\n",
      "|                                125|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, approxCountDistinct\n",
    "\n",
    "df.select(countDistinct(\"ORIGIN_COUNTRY_NAME\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/sql/functions.html#approx_count_distinct-org.apache.spark.sql.Column-double-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|approx_count_distinct(ORIGIN_COUNTRY_NAME)|\n",
      "+------------------------------------------+\n",
      "|                                       116|\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approxCountDistinct(\"ORIGIN_COUNTRY_NAME\", rsd=0.05)).show()\n",
    "# rsd: Maximum estimation error allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|approx_count_distinct(ORIGIN_COUNTRY_NAME)|\n",
      "+------------------------------------------+\n",
      "|                                       101|\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approxCountDistinct(\"ORIGIN_COUNTRY_NAME\", rsd=0.1)).show()\n",
    "# rsd: Maximum estimation error allowed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
