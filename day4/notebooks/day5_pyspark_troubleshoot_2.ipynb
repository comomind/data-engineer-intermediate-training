{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5일차 2교시 - Spark Skewness Problem\n",
    "> 특정 킷 값이 너무 많아 일부 Reduce 작업이 지연되어 전체 작업 시간에 영향을 주는 경우를 해결합니다\n",
    "\n",
    "### 목차\n",
    "* 1. Skewness 문제의 접근\n",
    "* 2. Skewness 편중의 인지\n",
    "* 3. Skewness 해결 전략\n",
    "* 4. References\n",
    "  * https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data Engineer Intermediate Day4\") \\\n",
    "    .config(\"spark.dataengineer.intermediate.day4\", \"troubleshoot-2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Skewness 문제의 접근\n",
    "#### 1.1. Skew 를 발생시키는 데이터를 반드시 사용해야만 하는가?\n",
    "> 대부분의 Skew 발생 대상 컬럼은 outer join 혹은 데이터 누락에 의한 Null 혹은 0 인 경우가 많은데 해당 데이터가 필요 없다면 제외합니다\n",
    "\n",
    "#### 1.2. Skew 를 발생 시키는 데이터를 별도의 파이프라인을 구성할 필요가 있는가?\n",
    "> Skew 발생 대상 컬럼에 대해 처리나 별도의 연산이 필요하다면, Skew 대상 데이터를 별도의 데이터프레임으로 구성하고 Union 을 통해 최종 결과를 생성하는 것이 병렬성 및 향후 유지보수 관점에도 유용할 수 있습니다\n",
    "\n",
    "#### 1.3. 조인 대상 테이블 가운데 Broadcast 해도 좋을 만큼 충분히 작은 테이블이 존재하는가?\n",
    "> 브로드캐스팅 대상 테이블을 큰 테이블로 전송하는 방식이므로 파티셔닝의 수를 변경할 수 없는 제약이 있어 큰 테이블이 사전에 잘 파티셔닝 혹은 evenly distributed 되어 있어야 더욱 좋은 효과를 발휘합니다\n",
    "> 브로드캐스팅 할 수 있는 크기가 최대 8gb 로 제한되어 있어 그 이상 전송할 수 없으며, 기본 값은 10mb 입니다.\n",
    "> 브로드캐스팅은 당연하게도 해당 브로드캐스팅 테이블은 드라이브 컨테이너 뿐만 아니라 드라이버의 메모리에 모두 올라올 수 있을 만큼 충분한 메모리가 필요합니다. 예를 들어 1gb 짜리 테이블이고 100개의 컨테이너가 뜬다고 가정하면 최소 총 100gb 이상의 추가적인 메모리 오버헤드가 발생합니다. 자칫 잘못하면 아래와 같은 메시지와 함께 어플리케이션이 종료됩니다.\n",
    "```java\n",
    "java.lang.OutOfMemoryError: Not enough memory to build and broadcast the table to all worker nodes.\n",
    "```\n",
    "> 브로드캐스팅은 해당 데이터 전체가 네트워크를 모두 점유하고 해당 컨테이너가 해당 테이블 전체를 복사해야 하므로 상당히 무거운 작업입니다. 그러한 부담을 감수할 만큼 충분히 가치가 있는 경우에만 적용할 수 있습니다. \n",
    "> 특히 브로드캐스팅 대상 테이블이 점점 커지는 경우라면 언젠가는 임계치를 초과하여 문제가 발생할 수 있으므로 데이터의 특성을 잘 고려해야만 합니다.\n",
    "\n",
    "#### 1.4 Skew 대상 컬럼을 특정할 수 있으며 브로드캐스팅이 어렵다면?\n",
    "> 치우친 데이터 컬럼에 대해 추가 랜덤 키(salt key)를 부여하고 조인 되는 대상 테이블에는 해당 salt key 최대 크기 만큼 뻥튀기 (explode) 하여 키 파티셔닝을 통한 병렬성을 늘릴 수 있도록 합니다 .\n",
    "* saltedJoin\n",
    "```python\n",
    "def saltedJoin(df: DataFrame, buildDf: DataFrame, joinExpression: Column, joinType: String, salt: Int): DataFrame = {\n",
    "    import org.apache.spark.sql.functions._\n",
    "    val tmpDf = buildDf.withColumn(“slt_range”, array(Range(0, salt).toList.map(lit): _*))\n",
    "    val tableDf = tmpDf.withColumn(“slt_ratio_s”, explode(tmpDf(“slt_range”))).drop(“slt_range”)\n",
    "    val streamDf = df.withColumn(“slt_ratio”, monotonically_increasing_id % salt)\n",
    "    val saltedExpr = streamDf(“slt_ratio”) === tableDf(“slt_ratio_s”) && joinExpression\n",
    "    streamDf.join(tableDf, saltedExpr, joinType).drop(“slt_ratio_s”).drop(“slt_ratio”)\n",
    "}\n",
    "```\n",
    "> 즉, 메인 테이블에 A 라는 키가 존재하고 조인 되는 테이블에도 A 가 있다면 메인 테이블에는 임의의 1~N 까지의 킷 값을 부여하고 조인되는 테이블의 레코드 수를 N배로 explode 하게 하여 동일한 값을 N배 레코드로 확장하여 조인할 수 있도록 하고, 마지막에 해당 salting key 컬럼을 제거합니다 \n",
    "```python\n",
    "val df = spark.read.parquet(“s3://...”)\n",
    "val geoDataDf = spark.read.parquet(“s3://...”)\n",
    "val userAgentDf = spark.read.parquet(“s3://...”)\n",
    "val ownerMetadataDf = spark.read.parquet(“s3://...”)\n",
    "df\n",
    " .saltedJoin(geoDataDf, exprGeo, “left”, 200)\n",
    " .saltedJoin(userAgentDf, exprUserAgent, “left”, 200)\n",
    " .saltedJoin(ownerMetadataDf, exprOwnerMetadata, “left”, 200)\n",
    " .write\n",
    " .parquet(“s3://...”)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = spark.range(1, 100)\n",
    "medium = spark.range(1, 1000)\n",
    "large = spark.range(1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Skewness 편중의 인지\n",
    ">  조인 연산에 있어서 파티션 단위 데이터 스큐를 해결하기 어려운 이유는 임의의 컬럼에 대해 조인 연산을 할 때에 해당 키를 기준으로 말아올렸을 때의 파티셔닝이 잘 분산되어 있다는 보장도 어렵고 사전에 예측도 어렵기 때문이다. 예를 들어 조인 조건이 A.name == B.name and A.model == B.model 일 경우 name 과 model 에 의해 생성되는 일치하는 파티셔닝 그룹이 어떤 그룹에 많은 데이터가 모일 지 예측하기 어렵고, 시간이 지남에 따라 혹은 특정 시기에 변화할 수도 있기 때문이다.\n",
    "\n",
    "* 파티션 단위 데이터 편중 현상 예측이 어려운 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Skewness 해결 전략\n",
    "\n",
    "#### 3.1 최대한 조인 대상 범위를 줄입니다\n",
    "> 전체 대상으로 조인을 하는 것 보다 최대한 필요한 컬럼만 지정하고, 구체적인 필터를 통해 조인 전에 데이터를 줄입니다\n",
    "\n",
    "#### 3.2 메모리에 올릴 수 있는 상대적으로 작은 테이블인지 확인\n",
    "> 조인대상 테이블 중에 충분히 작은 10~100mb 미만의 경우 broadcast 힌트를 통해 메모리에 올려 조인합니다\n",
    "\n",
    "#### 3.3 조인 키에 추가할 만한 데이터가 있는지 확인\n",
    "> 우리가 원하는 것은 가장 큰 파티션인 Ford Fiesta 을 작은 파티션들로 쪼개어 병렬처리가 가능하게 하기 위함\n",
    "0.1 리터 차이가 나는 필터를 적용하면 직관적이지만, 파티션이 \"model\", \"make\" 에 의해 결정나기 때문에 스큐현상을 피할 수 없습니다\n",
    "이를 회피하기 위해 exact matching 이 가능하도록 -1, 0, +1 의 3가지 경우를 explode 를 통해 생성해내어 조인을 수행합니다\n",
    "이 경우는 engine_size 가 0.1 단위로만 차이가 난다는 가정을 해야만 하지만, 비지니스 로직이 명확하다면 가장 좋은 성능을 냅니다\n",
    "\n",
    "#### 3.4 조인 대상 테이블에 랜덤한 수를 추가하여 병렬성을 추가하는 방법\n",
    "> 상대적으로 작은 테이블에 병렬수 만큼의 시퀀스를 추가해서 뻥튀기 합니다\n",
    "상대적으로 큰 테이블에는 파티션 별로 일정하게 증가하는 시퀀스를 추가하여 조인할 수 있도록 구성합니다\n",
    "\n",
    "#### 3.5 병렬 수준을 높이거나, 스큐가 발생하는 컬럼에 대해서만 병렬성을 추가하는 방법\n",
    "> 충분히 많은 타스크 수를 늘려서 병렬성을 높이는 방법\n",
    "정해진 컬럼에 대해서만 병렬성을 늘리고, 다른 컬럼에 대해서는 일정한 숫자를 추가합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration: string (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- engine_size: decimal(38,18) (nullable = true)\n",
      "\n",
      "10000 100000\n"
     ]
    }
   ],
   "source": [
    "s1 = spark.read.parquet(\"source/s1\")\n",
    "s2 = spark.read.parquet(\"source/s2\")\n",
    "s1.printSchema()\n",
    "c1 = s1.count()\n",
    "c2 = s2.count()\n",
    "print(c1, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------+--------------------+\n",
      "|registration|         make|  model|         engine_size|\n",
      "+------------+-------------+-------+--------------------+\n",
      "|     CPbYgbw|         FORD| FIESTA|1.300000000000000000|\n",
      "|     Q8GO2EU|         FORD| FIESTA|1.000000000000000000|\n",
      "|     qTQ7HxY|         FORD| FIESTA|1.200000000000000000|\n",
      "|     3FpMCC8|     VAUXHALL|  CORSA|1.500000000000000000|\n",
      "|     cirRCzK|       NISSAN|QASHQAI|1.300000000000000000|\n",
      "|     mK5LtWT|         FIAT|    500|1.100000000000000000|\n",
      "|     Cjxu9je|MERCEDED_BENZ|E CLASS|1.600000000000000000|\n",
      "|     j359V7w|       NISSAN|QASHQAI|1.100000000000000000|\n",
      "|     oaCQJN0|         FORD| FIESTA|1.300000000000000000|\n",
      "|     eGqJCKX|MERCEDED_BENZ|E CLASS|1.500000000000000000|\n",
      "+------------+-------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------------------+----------+\n",
      "|         make|  model|         engine_size|sale_price|\n",
      "+-------------+-------+--------------------+----------+\n",
      "|         FIAT|    500|1.100000000000000000|    1610.0|\n",
      "|          KIA|    RIO|1.800000000000000000|    1934.0|\n",
      "|       SUZUKI|  SWIFT|1.400000000000000000|     946.0|\n",
      "|       SUZUKI|  SWIFT|1.200000000000000000|    4799.0|\n",
      "|         FIAT|    500|1.400000000000000000|    5213.0|\n",
      "|       SUZUKI|  SWIFT|1.600000000000000000|    2529.0|\n",
      "|       NISSAN|QASHQAI|1.100000000000000000|    2120.0|\n",
      "|          KIA|    RIO|1.200000000000000000|    2122.0|\n",
      "|         FORD| FIESTA|1.300000000000000000|     862.0|\n",
      "|MERCEDED_BENZ|E CLASS|1.100000000000000000|    2456.0|\n",
      "+-------------+-------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[registration#6], functions=[avg(sale_price#17)])\n",
      "+- Exchange hashpartitioning(registration#6, 200)\n",
      "   +- *(2) HashAggregate(keys=[registration#6], functions=[partial_avg(sale_price#17)])\n",
      "      +- *(2) Project [registration#6, sale_price#17]\n",
      "         +- *(2) BroadcastHashJoin [make#7, model#8], [make#14, model#15], Inner, BuildLeft, (cast(CheckOverflow((promote_precision(cast(engine_size#16 as decimal(38,17))) - promote_precision(cast(engine_size#9 as decimal(38,17)))), DecimalType(38,17)) as double) <= 0.1)\n",
      "            :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true], input[2, string, true]))\n",
      "            :  +- *(1) Project [registration#6, make#7, model#8, engine_size#9]\n",
      "            :     +- *(1) Filter (isnotnull(make#7) && isnotnull(model#8))\n",
      "            :        +- *(1) FileScan parquet [registration#6,make#7,model#8,engine_size#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s1], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<registration:string,make:string,model:string,engine_size:decimal(38,18)>\n",
      "            +- *(2) Project [make#14, model#15, engine_size#16, sale_price#17]\n",
      "               +- *(2) Filter (isnotnull(make#14) && isnotnull(model#15))\n",
      "                  +- *(2) FileScan parquet [make#14,model#15,engine_size#16,sale_price#17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s2], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<make:string,model:string,engine_size:decimal(38,18),sale_price:double>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "cond = [s1.make == s2.make, s1.model == s2.model]\n",
    "s1.join(s2, cond).filter(s2.engine_size - s1.engine_size <= 0.1).groupBy(\"registration\").agg(avg(\"sale_price\")).explain()\n",
    "# .filter(abs(s2(\"engine_size\") - s1(\"engine_size\")) <= \"0.1\")\n",
    "# .groupBy(\"registration\").agg(avg(\"sale_price\").as(\"average_price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+\n",
      "|    make| model|count|\n",
      "+--------+------+-----+\n",
      "|    FORD|FIESTA| 5720|\n",
      "|    FIAT|   500|  574|\n",
      "|VAUXHALL| CORSA|  556|\n",
      "+--------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1.groupBy(\"make\", \"model\").count().sort(col(\"count\").desc()).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|   make| model|count|\n",
      "+-------+------+-----+\n",
      "|   FORD|FIESTA|55584|\n",
      "|   FIAT|   500| 5695|\n",
      "|HYUNDAI|   I20| 5646|\n",
      "+-------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s2.groupBy(\"make\", \"model\").count().sort(col(\"count\").desc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Skewness 전략 실습\n",
    "\n",
    "#### 4.1. 브로드캐스팅을 통한 조인연산\n",
    "> 데이터의 크기가 충분히 작아서 브로드캐스팅 되지만, 명시적으로 크기를 지정합니다, http://localhost:4040/stages/ 페이지에서 전체적으로 소요되는 시간을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[registration#6], functions=[avg(sale_price#17)])\n",
      "+- Exchange hashpartitioning(registration#6, 200)\n",
      "   +- *(2) HashAggregate(keys=[registration#6], functions=[partial_avg(sale_price#17)])\n",
      "      +- *(2) Project [registration#6, sale_price#17]\n",
      "         +- *(2) BroadcastHashJoin [make#7, model#8], [make#14, model#15], Inner, BuildLeft, (cast(CheckOverflow((promote_precision(cast(engine_size#16 as decimal(38,17))) - promote_precision(cast(engine_size#9 as decimal(38,17)))), DecimalType(38,17)) as double) <= 0.1)\n",
      "            :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true], input[2, string, true]))\n",
      "            :  +- *(1) Project [registration#6, make#7, model#8, engine_size#9]\n",
      "            :     +- *(1) Filter (isnotnull(make#7) && isnotnull(model#8))\n",
      "            :        +- *(1) FileScan parquet [registration#6,make#7,model#8,engine_size#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s1], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<registration:string,make:string,model:string,engine_size:decimal(38,18)>\n",
      "            +- *(2) Project [make#14, model#15, engine_size#16, sale_price#17]\n",
      "               +- *(2) Filter (isnotnull(make#14) && isnotnull(model#15))\n",
      "                  +- *(2) FileScan parquet [make#14,model#15,engine_size#16,sale_price#17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s2], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<make:string,model:string,engine_size:decimal(38,18),sale_price:double>\n",
      "+------------+------------------+\n",
      "|registration|   avg(sale_price)|\n",
      "+------------+------------------+\n",
      "|     ALcFxhR| 3022.568145376804|\n",
      "|     GXRKoPB| 2987.987181738367|\n",
      "|     FDicymS| 2979.525787965616|\n",
      "|     OxAQjdj| 2974.113934596959|\n",
      "|     JxDGxYo| 2980.301324503311|\n",
      "|     kKWnV2J|3008.4574344699604|\n",
      "|     ixFdKY8| 2999.611658456486|\n",
      "|     AOLIu7i|3011.1674161896976|\n",
      "|     60GrroE| 2999.611658456486|\n",
      "|     ZhdXcm3|2991.8207713688653|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 8.41 ms, sys: 8.82 ms, total: 17.2 ms\n",
      "Wall time: 40.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10*1024*1024)  # 10mb\n",
    "res = s1.join(s2, cond).filter(s2.engine_size - s1.engine_size <= 0.1).groupBy(\"registration\").agg(avg(\"sale_price\"))\n",
    "res.explain()\n",
    "res.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. 셔플링을 통한 조인\n",
    "> 브로드캐스팅 되지 않도록 임계치 값을 -1로 지정하여 브로드캐스팅 되지 않도록 설정하고, http://localhost:4040/stages/ 페이지에서 가장 시간이 오래걸린 작업을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[registration#6], functions=[avg(sale_price#17)])\n",
      "+- Exchange hashpartitioning(registration#6, 200)\n",
      "   +- *(5) HashAggregate(keys=[registration#6], functions=[partial_avg(sale_price#17)])\n",
      "      +- *(5) Project [registration#6, sale_price#17]\n",
      "         +- *(5) SortMergeJoin [make#7, model#8], [make#14, model#15], Inner, (cast(CheckOverflow((promote_precision(cast(engine_size#16 as decimal(38,17))) - promote_precision(cast(engine_size#9 as decimal(38,17)))), DecimalType(38,17)) as double) <= 0.1)\n",
      "            :- *(2) Sort [make#7 ASC NULLS FIRST, model#8 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(make#7, model#8, 200)\n",
      "            :     +- *(1) Project [registration#6, make#7, model#8, engine_size#9]\n",
      "            :        +- *(1) Filter (isnotnull(make#7) && isnotnull(model#8))\n",
      "            :           +- *(1) FileScan parquet [registration#6,make#7,model#8,engine_size#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s1], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<registration:string,make:string,model:string,engine_size:decimal(38,18)>\n",
      "            +- *(4) Sort [make#14 ASC NULLS FIRST, model#15 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(make#14, model#15, 200)\n",
      "                  +- *(3) Project [make#14, model#15, engine_size#16, sale_price#17]\n",
      "                     +- *(3) Filter (isnotnull(make#14) && isnotnull(model#15))\n",
      "                        +- *(3) FileScan parquet [make#14,model#15,engine_size#16,sale_price#17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s2], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<make:string,model:string,engine_size:decimal(38,18),sale_price:double>\n",
      "+------------+------------------+\n",
      "|registration|   avg(sale_price)|\n",
      "+------------+------------------+\n",
      "|     Q8GO2EU| 2977.881762228701|\n",
      "|     6yn3Vke|2993.3496869602764|\n",
      "|     FWPfoGh|2993.3496869602764|\n",
      "|     bUhDTXQ|2998.1522468868434|\n",
      "|     AXlbNWj|2995.7316161100016|\n",
      "|     VYvy491|2991.8207713688653|\n",
      "|     qgL3CRq| 2977.881762228701|\n",
      "|     O3vO6em|2986.3140002436944|\n",
      "|     s1zJoU3|2993.3496869602764|\n",
      "|     beenpZ6|2984.3086874154264|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 16.8 ms, sys: 7.54 ms, total: 24.3 ms\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "res = s1.join(s2, cond).filter(s2.engine_size - s1.engine_size <= 0.1).groupBy(\"registration\").agg(avg(\"sale_price\"))\n",
    "res.explain()\n",
    "res.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 솔팅 기법을 통한 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|new|\n",
      "+---+---+\n",
      "|  1|  0|\n",
      "|  1|  1|\n",
      "|  1|  2|\n",
      "|  1|  3|\n",
      "|  1|  4|\n",
      "|  2|  0|\n",
      "|  2|  1|\n",
      "|  2|  2|\n",
      "|  2|  3|\n",
      "|  2|  4|\n",
      "|  3|  0|\n",
      "|  3|  1|\n",
      "|  3|  2|\n",
      "|  3|  3|\n",
      "|  3|  4|\n",
      "|  4|  0|\n",
      "|  4|  1|\n",
      "|  4|  2|\n",
      "|  4|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers = spark.range(1, 5)\n",
    "numbers.withColumn(\"new\", explode(array([lit(x) for x in range(0,5)]))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[registration#6], functions=[avg(sale_price#17)])\n",
      "+- Exchange hashpartitioning(registration#6, 200)\n",
      "   +- *(5) HashAggregate(keys=[registration#6], functions=[partial_avg(sale_price#17)])\n",
      "      +- *(5) Project [registration#6, sale_price#17]\n",
      "         +- *(5) SortMergeJoin [make#7, model#8, cast(skew_key#256 as bigint)], [make#14, model#15, skew_key#262L], Inner, (cast(CheckOverflow((promote_precision(cast(engine_size#16 as decimal(38,17))) - promote_precision(cast(engine_size#9 as decimal(38,17)))), DecimalType(38,17)) as double) <= 0.1)\n",
      "            :- *(2) Sort [make#7 ASC NULLS FIRST, model#8 ASC NULLS FIRST, cast(skew_key#256 as bigint) ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(make#7, model#8, cast(skew_key#256 as bigint), 200)\n",
      "            :     +- Generate explode([0,1,2]), [registration#6, make#7, model#8, engine_size#9], false, [skew_key#256]\n",
      "            :        +- *(1) Project [registration#6, make#7, model#8, engine_size#9]\n",
      "            :           +- *(1) Filter (isnotnull(make#7) && isnotnull(model#8))\n",
      "            :              +- *(1) FileScan parquet [registration#6,make#7,model#8,engine_size#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s1], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<registration:string,make:string,model:string,engine_size:decimal(38,18)>\n",
      "            +- *(4) Sort [make#14 ASC NULLS FIRST, model#15 ASC NULLS FIRST, skew_key#262L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(make#14, model#15, skew_key#262L, 200)\n",
      "                  +- *(3) Filter ((isnotnull(model#15) && isnotnull(make#14)) && isnotnull(skew_key#262L))\n",
      "                     +- *(3) Project [make#14, model#15, engine_size#16, sale_price#17, (monotonically_increasing_id() % 3) AS skew_key#262L]\n",
      "                        +- *(3) FileScan parquet [make#14,model#15,engine_size#16,sale_price#17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<make:string,model:string,engine_size:decimal(38,18),sale_price:double>\n",
      "+------------+------------------+\n",
      "|registration|   avg(sale_price)|\n",
      "+------------+------------------+\n",
      "|     0AB64Pl|3037.4286995515695|\n",
      "|     GXRKoPB| 2987.987181738367|\n",
      "|     ALcFxhR| 3022.568145376804|\n",
      "|     JxDGxYo| 2980.301324503311|\n",
      "|     60GrroE| 2999.611658456486|\n",
      "|     AOLIu7i|3011.1674161896976|\n",
      "|     ixFdKY8| 2999.611658456486|\n",
      "|     kKWnV2J|3008.4574344699604|\n",
      "|     gx1Mzic| 2985.991675522494|\n",
      "|     1oc9DHa|2949.7984683595323|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 11.1 ms, sys: 6.64 ms, total: 17.8 ms\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "magic_number = 3\n",
    "t1 = s1.withColumn(\"skew_key\", explode(lit(array([lit(x) for x in range(0, magic_number)]))))\n",
    "t2 = s2.withColumn(\"skew_key\", monotonically_increasing_id() % magic_number)\n",
    "cond = [t1.make == t2.make, t1.model == t2.model, t1.skew_key == t2.skew_key]\n",
    "res = t1.join(t2, cond).filter(s2.engine_size - s1.engine_size <= 0.1).groupBy(\"registration\").agg(avg(\"sale_price\"))\n",
    "res.explain()\n",
    "res.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[registration#6], functions=[avg(sale_price#17)])\n",
      "+- Exchange hashpartitioning(registration#6, 200)\n",
      "   +- *(5) HashAggregate(keys=[registration#6], functions=[partial_avg(sale_price#17)])\n",
      "      +- *(5) Project [registration#6, sale_price#17]\n",
      "         +- *(5) SortMergeJoin [make#7, model#8, cast(skew_key#324 as bigint)], [make#14, model#15, skew_key#330L], Inner, (cast(CheckOverflow((promote_precision(cast(engine_size#16 as decimal(38,17))) - promote_precision(cast(engine_size#9 as decimal(38,17)))), DecimalType(38,17)) as double) <= 0.1)\n",
      "            :- *(2) Sort [make#7 ASC NULLS FIRST, model#8 ASC NULLS FIRST, cast(skew_key#324 as bigint) ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(make#7, model#8, cast(skew_key#324 as bigint), 200)\n",
      "            :     +- Generate explode([0,1,2,3]), [registration#6, make#7, model#8, engine_size#9], false, [skew_key#324]\n",
      "            :        +- *(1) Project [registration#6, make#7, model#8, engine_size#9]\n",
      "            :           +- *(1) Filter (isnotnull(make#7) && isnotnull(model#8))\n",
      "            :              +- *(1) FileScan parquet [registration#6,make#7,model#8,engine_size#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s1], PartitionFilters: [], PushedFilters: [IsNotNull(make), IsNotNull(model)], ReadSchema: struct<registration:string,make:string,model:string,engine_size:decimal(38,18)>\n",
      "            +- *(4) Sort [make#14 ASC NULLS FIRST, model#15 ASC NULLS FIRST, skew_key#330L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(make#14, model#15, skew_key#330L, 200)\n",
      "                  +- *(3) Filter ((isnotnull(model#15) && isnotnull(make#14)) && isnotnull(skew_key#330L))\n",
      "                     +- *(3) Project [make#14, model#15, engine_size#16, sale_price#17, (monotonically_increasing_id() % 4) AS skew_key#330L]\n",
      "                        +- *(3) FileScan parquet [make#14,model#15,engine_size#16,sale_price#17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/source/s2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<make:string,model:string,engine_size:decimal(38,18),sale_price:double>\n",
      "+------------+------------------+\n",
      "|registration|   avg(sale_price)|\n",
      "+------------+------------------+\n",
      "|     0AB64Pl|3037.4286995515695|\n",
      "|     GXRKoPB| 2987.987181738367|\n",
      "|     ALcFxhR| 3022.568145376804|\n",
      "|     JxDGxYo| 2980.301324503311|\n",
      "|     Q8GO2EU| 2977.881762228701|\n",
      "|     6yn3Vke|2993.3496869602764|\n",
      "|     FWPfoGh|2993.3496869602764|\n",
      "|     bUhDTXQ|2998.1522468868434|\n",
      "|     AXlbNWj|2995.7316161100016|\n",
      "|     VYvy491|2991.8207713688653|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 13.5 ms, sys: 5.7 ms, total: 19.3 ms\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "magic_number = 4\n",
    "t1 = s1.withColumn(\"skew_key\", explode(lit(array([lit(x) for x in range(0, magic_number)]))))\n",
    "t2 = s2.withColumn(\"skew_key\", monotonically_increasing_id() % magic_number)\n",
    "cond = [t1.make == t2.make, t1.model == t2.model, t1.skew_key == t2.skew_key]\n",
    "res = t1.join(t2, cond).filter(s2.engine_size - s1.engine_size <= 0.1).groupBy(\"registration\").agg(avg(\"sale_price\"))\n",
    "res.explain()\n",
    "res.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
